<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RabbitMQ笔记]]></title>
    <url>%2FJavaLearning%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2FRabbitMQ.html</url>
    <content type="text"><![CDATA[RabbitMQRabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现. AMQP: Advanved Message Queue, 高级消息队列协议. 它是应用层协议的一个开放标准, 为面向消息的中间件设计, 基于此协议的客户端与消息中间件可传递消息, 且不受产品, 开发语言条件的限制. RabbitMQ 的特点有: 可靠性(Reliability): 使用持久化, 传输确认, 发布确认等机制来保证可靠性. 灵活的路由(Flexible Routing): 在消息进入队列之前, 通过 Exchange 来路由消息的. 对于典型的路由功能, RabbitMQ 已经提供了一些内置的 Exchange 来实现. 针对更复杂的路由功能, 可以将多个 Exchange 绑定在一起, 也可以通过插件机制实现自己的 Exchange. 消息集群(Clustering): 多个 RabbitMQ 服务器可以组成一个集群, 形成一个逻辑 Broker. 高可用(Highly Available Queues): 队列可以在集群的机器上创建镜像节点, 使得在部分节点出问题的情况下队列仍然可用. 多种协议(Multi-protocol): RabbitMQ 支持多种消息队列协议, 如 STOMP, MQTT 等. 多语言客户端(Many Clients): RabbitMQ 几乎支持所有语言. 管理界面(Management UI): RabbitMQ 提供了一个易用的用户界面, 使得用户可以监控和管理消息 Broker 的诸多方面. 跟踪机制(Tracing): 如果消息异常, RabbitMQ 提供了消息跟踪机制. 插件机制(Plugin System): RabbitMQ 提供了许多插件, 来从许多方面进行扩展, 也可以编写自己的插件. 系统架构消息模型所有 MQ 产品从模型抽象上来说都是相似的过程: 消费者(Consumer)订阅某个队列(Queue), 生产者(Producer)创建消息, 然后发布到队列(Queue)中, Server(Broker)负责将队列中的消息发送给订阅该队列的消费者. Server 也叫 Broker, 它的角色就是维护从 Producer 到 Comsumer 的路线, 保证数据能够按照指定的方式进行传输. Publisher 数据发送方, 负责发送 Message 到 Server. Consumer 数据接收方, 负责从 Server 中订阅的 Queue 获取 Message. Message 消息, 一个 Message 由两个部分组成: Header 和 Body, Header 是由生产者添加的各种属性的集合, 包括 routing-key(路由键), priority(优先级), delivery-mode(消息是否需要持久化)等. Exchange 交换器, 根据消息的 routing-key 来将其路由给服务器中的队列. Queue 消息队列, 用来保存消息, 直到消息过期或被消费者获取. 它是消息的容器, 也是消息的终点. 一个消息可路由到一个或多个队列. Binding 绑定, 用于定义消息队列和交换器之间的关联. 一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则. Connection 网络连接. Channel 信道, 多路复用连接中的一条独立的双向数据流通道. 信道建立在真实的 TCP 连接内的虚拟连接, AMQP 命令不管是发布消息, 订阅队列还是接收消息, 都是通过信道发出. 因为对于操作系统来说, 建立和销毁 TCP 都是非常昂贵的开销, 所以引入了信道的概念, 以复用一条 TCP 连接. Virtual Host 虚拟主机, 每个 vhost 都拥有自己的队列, 交换器, 绑定和权限机制. 消息路由AMQP 中的消息的路由和 JMS 存在一些差别, AMQP 中增加了 Exchange 和 Binding 的角色. 生产者把消息发布到 Exchange 上, 消息最终到达队列并被消费者接收, 而 Binding 决定交换器的消息该发送到哪个队列. Exchange 类型Exchange 分发消息是根据类型的不同, 采用不同的分发策略. 目前共有四种类型: direct 消息中的路由键(routing-key)如果和 Binding 中的 binding key 完全匹配才将消息发到对应的队列中. fanout 每个发到 fanout 类型交换器的消息都会被分发到所有绑定的队列中区. fanout 交换器不处理路由键, 所以效率是最高的. topic topic 交换器通过模式匹配消息的路由键来路由消息, 它将路由键和绑定键的字符串用 . 分隔为多个单词, 同样也会识别两个通配符: # 匹配 0 个或多个单词, * 仅匹配一个单词. headers headers 交换器不依赖 routing key 与 binding key 的匹配规则来路由消息, 而是根据发送的消息内容中的 headers 属性进行匹配. 通过判断 headers 中的键值对与交换器与队列绑定时指定的键值对是否匹配来路由消息. 任务分配机制Round-robin 转发RabbitMQ 会将消息均衡地分发给每个消费者, 平均每个消费者将会获得相等数量的消息. 消息应答(Message Acknowledgement)消费者在处理完消息后告诉 MQ 消息已经处理完成, 之后 MQ 才将消息从队列移除. 如果消费者因为某些原因断开连接而没有发送应答, RabbitMQ 会认为该信息没有被完全地处理, 然后将会重新转发给别的消费者. 这里不存在 timeout, 即除非当前消费者断开连接, 否则消息不会被重新发送给其他消费者. 另外, publish message 是没有 ack 的. 消息持久化(Message durability)将消息持久化到硬盘, 下次 MQ 重启后不会丢失消息. 公平转发(Fair dispatch)如果有多个消费者同时订阅同一个 Queue 中的消息, Queue 中的消息会被平摊给多个消费者. 这时, 如果每个消息的处理时间不同, 就可能导致某些消费者一直忙碌, 而另一些消费者处于空闲状态. 我们可以通过设置 Prefetch count 来限制 Queue 每次发送给每个消费者的消息数, 比如 prefetchCount = 1, 则每个消费者每次只能接收到一条消息, 消费者只有在处理完(Ack)这条消息后才能再次获得一条新的消息. RPCMQ 本身是基于异步的消息处理, 生产者将消息发送到 RabbitMQ 后不会知道消费者是否处理成功. 如果生产者需要根据消息被处理的情况进行下一步处理, 这相当于 RPC(Remote Procedure Call, 远程过程调用). 在 RabbitMQ 中也支持 RPC. 实现的机制是: 客户端发送消息时, 在消息的属性(Message Properties, 在 AMQP 协议中定义了 14 种 properties, 这些属性会随着消息一起发送)中设置两个值 replyTo(一个 Queue 名称, 用于告诉服务器处理完成后将通知我的消息发送到这个 Queue 中)和 correlationId(此次请求的标识号, 服务器处理完成后需要将此属性返还, 客户端将根据这个 id 了解到辣条请求被成功执行了或执行失败). 服务端收到消息处理完后, 将生成一条应答消息到 replyTo 指定的 Queue, 同时带上 correlationId 属性. 客户端之前已订阅 replyTo 指定的 Queue, 从中收到服务器的应答消息后, 根据其中的 correlationId 属性分析哪条请求被执行了, 根据执行结果进行后续业务处理.]]></content>
      <categories>
        <category>中间件</category>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dev Note]]></title>
    <url>%2FJavaLearning%2FNotes%2FDevNote.html</url>
    <content type="text"><![CDATA[DBMySQL 8.x jdbc 连接参数1jdbc:mysql://&#123;host&#125;:&#123;port&#125;/&#123;schema&#125;?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 不设置时区会导致数据库无法连接, 报错: The server time zone value &#39;ÖÐ¹ú±ê×¼Ê±¼ä&#39; is unrecognized or represents more than one time zone. Spring BootSpring Boot profile 拆分通过 application-{profile} 的形式, 可实现 profile 的环境隔离和拆分. 环境隔离比如有 prod, dev 和 beta 三个环境. 可将环境特有的配置分别写在 application-prod.yml, application-dev.yml 和 application-beta.yml 中. 然后在主配置文件(通用)中通过 spring.profiles.active : ${env} 来切换当前环境, 达到环境隔离的效果. 拆分如果一个 profile 中配置过多, 想让文件更加简洁, 或按功能拆分, 比如配置了 redis, mybatis, 可以单独创建这两个的配置文件: application-devRedis.yml, application-devMybatis.yml. 然后在主配置文件中通过 spring.profiles.include: devRedis,devMybatis 来引入这两个配置. springBoot 序列化时忽略 null 字段.添加注解的方式无法生效, 需要在 application.yml 中通过 spring.jackson.default-property-inclusion: non_null 来指定. Swagger @ApiImplicitParam 和 @ApiParam 的使用@ApiImplicParam 用于描述接口方法参数对象的属性. @ApiParam 用于描述接口方法所需的参数对象. 例: 12345@ApiImplicitParams(&#123; @ApiImplicitParam(name = "people 的 name", value = "name"), @ApiImplicitParam(name = "people 的 age", value = "age) &#125;)public void updatePeople(@ApiParam People p, @ApiParam String type)&#123;&#125; 使用 Nginx 反向代理后获取不到真实客户端 IP起因由于配置了 Nginx 反向代理, 使用 proxy_pass 代理了客户端请求, 导致通过 HttpServletRequest.getRemoteAddr() 获取到的 IP 地址为 Nginx 服务器所在的地址. 解决方案Nginx 添加配置: 1234567location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_redirect off;&#125; SpringBoot 添加配置 12345server: use-forward-headers: true tomcat: remote-ip-header: X-Real-IP protocol-header: X-Forwarded-Proto]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Dev</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具踩坑记录]]></title>
    <url>%2FJavaLearning%2FNotes%2FSolutions.html</url>
    <content type="text"><![CDATA[CmderCmder 进入 Windows Linux Subsystem 后, 进入 vim 界面方向键无法使用.在 Startup -&gt; Tasks -&gt; bash::ubuntu -&gt; 启动参数中添加 %windir%\system32\bash.exe ~ -cur_console:p5. Win10电脑开机后, 亮度自动变为 50.(任务管理器) -&gt; 服务 -&gt; 显示增强服务(DisplayEnhancementService), 把该服务禁用即可.]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker介绍]]></title>
    <url>%2FJavaLearning%2FOps%2FDocker%E4%BB%8B%E7%BB%8D.html</url>
    <content type="text"><![CDATA[简介Docker 是基于 Go 语言实现的云开源项目. Docker 的主要目标是: “Build, Ship and Run Any App, Anywhere”, 也就是通过对应用组件的封装, 分发, 部署, 运行等生命周期的管理, 使用户的 APP(可以使一个 WEB 应用或数据库应用等)及其运行环境能够做到”一次封装, 到处运行”. Linux 容器技术(Linux Containers)的出现就解决了这样一个问题, Docker 就是在它的基础上发展过来的. 将应用运行在 Docker 容器上面, 而 Docker 容器在任何操作系统上都是一致的, 这就实现了跨平台, 跨服务器. 什么是容器容器就是将软件打包成标准化单元, 以用于开发, 交付和部署. 容器镜像是轻量的, 可执行的独立软件包. 包含了软件运行所需的所有内容: 代码, 运行时环境, 系统工具, 系统库和设置. 容器化软件在任何环境中都能够始终如一地运行. 容器赋予了软件独立性, 使其免受外在环境差异的影响, 从而有助于减少团队间在相同基础设施上运行不同软件时的冲突. 什么是 Docker Docker 是世界领先的软件容器平台. Docker 使用 Go 语言实现, 基于 Linux 内核的 cgroup, namespace, 以及 AUFS 类的 UnionsFS 等技术, 对进程进行封装隔离, 属于操作系统层面的虚拟化技术. Docker 能够自动执行重复性任务, 例如搭建和配置开发环境, 从而解放了开发人员以便他们专注在真正重要的事情上: 构建杰出的软件. 用户可以方便地创建和使用容器, 把自己的应用放入容器. 容器还可以进行版本管理, 复制, 分享和修改, 就像管理普通的代码一样. Docker 容器的特点 轻量 在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核; 它们能够迅速启动, 只需占用很少的计算和内存资源. 标准 Docker 容器基于开放式标准, 能够在所有主流 Linux 版本, Microsoft Windows 以及包括 VM 和云在内的任何基础设施上运行. 安全 Docker 赋予应用的隔离性不仅限于彼此, 还独立于底层的基础设施. Docker 默认提供最强的隔离, 因此应用出现问题, 也只是单个容器的问题, 而不会波及到整台机器. 为什么要使用 Docker 一致的运行环境 Docker 镜像提供了除内核外完整的运行时环境, 确保了应用运行环境一致性. 更快速的启动时间 可以做到秒级, 甚至毫秒级的启动时间. 大大节约了开发, 测试和部署的时间. 隔离性 避免公用的服务器, 资源容易受到其他用户的影响. 弹性伸缩, 快速扩展 善于处理集中爆发的服务器使用压力. 迁移方便 可以很轻易的将在一个平台上运行的应用, 迁移到另一个平台上, 而不用担心运行环境的变化所导致应用无法正常运行的情况. 持续交付和部署 使用 Docker 可以通过定制应用镜像来实现持续集成, 持续交付和部署. 容器与虚拟机容器与虚拟机不同, 传统虚拟机技术是虚拟出一套硬件后, 在其之上运行一个完整的操作系统, 然后在该系统上再运行所需应用进程; 而容器内的应用进程直接运行于宿主机的内核, 容器没有自己的内核, 而且也没有进行硬件虚拟. 因此容器要比传统虚拟机更为轻便. 特性 容器 虚拟机 部署速度 秒级 分钟级 存储大小 一般为MB 一般为GB 运行性能 几乎无额外性能开销 操作系统额外的CPU, 内存消耗 系统支持量 单机支持上千个容器 一般几十个 移植性 轻便, 灵活, 适应于 Linux 笨重, 与虚拟化技术耦合度高 硬件亲和性 面向软件开发者 面向硬件运维者 容器是一个应用层抽象, 用于将代码和依赖资源打包在一起. 多个容器可以在同一台机器上运行, 共享操作系统内核, 但各自的环境是隔离开来的. 虚拟机是一个物理硬件层的抽象, 用于将一台服务器变为多台服务器. 管理程序允许多个 VM 在一台机器上运行, 每个 VM 都包含一整套操作系统, 因此占用的空间更大, 启动也较慢. 虚拟机更擅长于彻底隔离整个运行环境, 例如: 云服务提供商通常采用虚拟机技术隔离不同的用户. 而 Docker 通常用于隔离不同的应用, 例如: 通过一个 Tomcat 镜像, 启动多个 Tomcat 容器, 每个容器中 Tomcat 的环境配置都是彼此隔离的. Docker 基本概念 镜像(Image)操作系统分为内核和用户空间. 对于 Linux 而言, 内核启动后, 会挂在 root 文件系统为其提供用户空间支持. 而 Docker 镜像 就相当于是一个 root 文件系统. 镜像是一个特殊的文件系统, 除了提供容器运行时所需的程序, 库, 资源, 配置等文件外, 还包含了一些为运行时准备的一些参数(如匿名卷, 环境变量, 用户等, 通过 DockerFile 配置). 镜像不包含任何动态数据, 其内容在构建之后不会被改变. Docker 设计时就充分利用了 Union FS 技术, 将其设计为分层存储架构. 镜像实际是由多层文件系统联合组成, 如下图蓝色部分所示: 镜像构建时, 会一层一层构建, 前一层是后一层的基础. 每一层构建完成就会形成一个镜像, 不会发生改变, 后一层上的任何改变只发生在自己其自身这一层. 比如: 删除前一层文件的操作, 实际不是真的删除前一层的文件, 而是仅在当前层标记为该文件已删除, 在容器最终运行的时候, 虽然不会看到这个文件, 但是实际上该文件会一直跟随镜像. 因此, 在构建镜像时, 需要额外小心, 每一层尽量只包含该层需要添加的东西, 任何额外的东西应该在该层构建结束前清理掉. 分层存储的特征还使得镜像的复用, 定制变得更为容易. 甚至可以用之前构建好的镜像作为基础层, 然后进一步添加新的层, 以定制自己所需的镜像. 容器(Container)镜像(Image)和容器(Container)的关系, 就像是面向对象设计中的类和实例一样, 镜像是静态的定义, 容器是镜像运行的实体. 容器可以执行创建, 启动, 停止, 删除和暂停等操作. 容器的实质是进程, 但与直接在宿主执行的进程不同, 容器进程运行于属于自己的独立的命名空间. 容器存储层的生存周期和容器一样, 容器消亡时, 容器存储层也随之消亡. 因此, 任何保存于容器存储层的信息都会随容器删除而丢失. 注意: 按照 Docker 最佳实践的要求, 容器不应该向其存储层写入任何数据, 容器存储层要保持无状态化. 所有的文件写入操作都应该使用数据卷(Volume), 或者绑定宿主目录, 在这些位置的读写会跳过容器存储层, 直接对宿主(网络存储)发生读写, 其性能会稳定性更高. 数据卷的生存周期独立于容器, 容器消亡, 数据卷不会消亡. 因此, 使用数据卷后, 容器可以随意删除, 重新运行, 数据不会丢失. 仓库(Repository)镜像构建完成后, 可以很容易地在当前宿主上运行. 但是如果要在其他服务器上使用这个镜像, 我们就需要一个集中存储, 分发镜像的服务. Docker Registry 就是这样的服务. 一个 Docker Registry 中可以包含多个仓库(Repository); 每个仓库可以包含多个标签(Tag); 每个标签对应一个镜像(Image). 所以说: 镜像仓库是 Docker 用来集中存放镜像文件的地方, 类似于代码仓库. 通常来讲, 一个仓库会包含同一个软件不同版本的镜像, 而标签就常用于对应该软件的各个版本. 通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式, 我们可以来指定该软件的具体版本的镜像. 如果不提供标签, 将以 latest 作为默认标签. Docker Registry 可分为公共和私有, 意味着我们可以自行搭建 Docker Registry. Docker 官方提供了 Docker Registry 的镜像, 可以直接使用作为私有 Registry 服务. 开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现, 足以支持 Docker 命令, 不影响使用, 但不包含图形界面, 以及镜像维护, 用户管理, 访问控制等高级功能. 最常使用的 Registry 公开服务是官方的 Docker Hub, 这也是 Docker 默认的 Registry, 但在国内访问比较慢. 国内常使用的有: 阿里云, 网易云. 数据卷(Volume) Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts: Volumes are easier to back up or migrate than bind mounts. You can manage volumes using Docker CLI commands or the Docker API. Volumes work on both Linux and Windows containers. Volumes can be more safely shared among multiple containers. Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality. New volumes can have their content pre-populated by a container. 为了持久化数据以及共享容器间的数据, Docker 提出了数据卷(Volume)的概念. 简单来说, 数据卷就是目录或者文件, 它可以绕过默认的 UnionFS(联合文件系统), 而以正常的文件或目录的形式存在于宿主机上. 使用通过 -v 方式添加命令 docker run -v [host-dir]:[container-dir]:[rw|wo|ro], 其中: host-dir: 表示宿主机上的目录, 如果不存在, Docker 会自动创建该目录. container-dir: 表示容器内部对应的目录, 如果该目录不存在, Docker 也会在容器内部创建该目录. rw|ro|wo: 控制数据卷的读写权限.(ro -&gt; read only, wo -&gt; write only) 如果不指定 host-dir, Docker 会自动创建一个目录, 如: /var/lib/docker/volumes/.../_data. 这种情况通常发生在通过 DockerFile 设置数据卷时. 通过 DockerFile 添加在 DockerFile 文件中通过 VOLUME [${container-dir1},${container-dir2},&quot;...&quot;] 的形式为镜像配置一个或多个数据卷. 这时会在宿主机中自动添加相对应的目录. 可以通过 docker inspect ${容器ID} 查看. 通过 –volumes-from 挂载很多时候, 我们想让多个容器共享一些数据, 这时我们可以创建一个数据卷容器. 命令 docker run --volumes-from ${父容器ID} 可以使新创建的容器共享父容器的数据卷. 容器之间配置信息的传递, 数据卷的生命周期一直持续到没有容器使用它位置. DockerFileDockerFile 是用来构建 Docker 镜像的构建文件, 是由一行行命令和参数构成的脚本. 保留字指令基础: 支持以 # 开头的注释. 每条保留字指令都约定为大写. 每条指令后面必须跟随至少一个参数. 执行按照从上到下顺序执行. 每条指令都会创建一个新的镜像层, 并对镜像进行提交. FROM指定 base image. 123FROM &lt;image&gt;:&lt;tag&gt;FROM scratch # 制作base imageFROM centos # 使用base images LABEL元数据 1234LABEL &lt;key&gt;=&lt;value&gt; ...LABEL maintainer = "vnaso@live.com"LABEL version = "1.0"LABEL description = "hello world" RUN在镜像的构建过程中执行特定的命令或安装软件, 并生成一个中间镜像. 12RUN &lt;command&gt; # shell 格式RUN ["excutable", "param1","param2"] # exec 格式 WORKDIR为接下来的 DockerFile 指令指定当前工作目录, 可多次使用, 如果使用的是相对路径, 则相对的是上一个工作目录. 类似于 shell 中的 cd 命令. 1WORKDIR /path/to/workdir ENV在构建的镜像中设置环境变量, 在后续的 DockerFile 指令中可以直接使用, 也可以固化在镜像里, 在容器运行时依然有效. 1234ENV &lt;key&gt; &lt;value&gt; # 无法再一行内设置多个变量ENY &lt;key&gt;=&lt;value&gt; # 可以设置多个环境变量 如果 &lt;value&gt; 中存在空格, 需要转义或用引号 "" 括起来ENV JAVA_HOME=/usr/java/jdk...\ CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib \PATH=$PATH:$GIT_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$JRE_HOME/bin: 可以在运行时指定环境变量, docker run --env &lt;key&gt;=&lt;value&gt;. 使用 ENV 可能会对后续的 DockerFile 指令造成影响, 如果只需要对一条指令设置环境变量, 可以使用: RUN &lt;key&gt;=&lt;value&gt; &lt;command&gt;. ADD &amp; COPYADD: 在构建镜像时, 复制上下文的文件到镜像内. 12ADD &lt;src&gt; ... &lt;dest&gt;ADD ["&lt;src&gt;",..."&lt;dest&gt;"] &lt;src&gt; 可以是文件, 目录, 也可以是文件 URL. 可以使用模糊匹配(wildcards, 类似 shell 的匹配), 可以指定多个 &lt;src&gt;, 必须是在上下文目录和子目录中, 无法添加形如 ../a.txt 这样的文件. 如果 &lt;src&gt; 是目录, 则复制的是目录下所有的内容, 但不包括该目录; 如果是压缩包, 则会以 tar -x 的方式解压后将内容复制到 &lt;dest&gt;. &lt;dest&gt; 可以是绝对路径, 也可以是相对 WORKDIR 目录的相对路径. 所有文件的 UID 和 GID 都是 0. COPY: 与 ADD 类似, 只是将上下文内的文件复制到镜像内, COPY 是在镜像内的复制, 格式与 ADD 一致. EXPOSE为构建的镜像设置监听端口, 使容器在运行时监听. 1EXPOSE &lt;port&gt; [&lt;port&gt; ...] EXPOSE 指令并不会让容器监听 host 的端口, 如果需要, 需要在 docker run 时使用 -p, -P 参数来发布容器端口到 host 某个端口上. CMD指定容器运行时的默认参数, 如果出现多次, 以最后一次为准. 123CMD ["executable", "param1", "param2"] # exec 格式CMD command param1 param2 # shell 格式CMD ["param1", "param2"] # 省略可自行文件的 exec 格式, 这种写法使 CMD 中的参数当做 ENTRYPOINT 的默认参数, 此时 ENTRYPOINT 也应该是 exec 格式. 与 RUN 指令的区别: RUN 在构建的时候执行, 并生成一个新的镜像, CMD 在容器运行的时候执行, 在构建时不进行任何操作. ENTRYPOINT指定镜像的执行程序, 只有最后一条 ENTRYPOINT 指令有效. 12ENTRYPOINT &lt;command&gt; &lt;param1&gt; &lt;param2&gt; # shell 格式, PID 不为 1, 也接收不到 Unix 信号. 需要使用 exex 或 gosu 命令处理.ENTRYPOINT ["&lt;executable&gt;","&lt;param1&gt;","&lt;param2&gt;"] # exec 格式 PID 为1 CMD 和 ENTRYPOINT 至少得使用一个, ENTRYPOINT 应该被当做 Docker 的可执行程序, CMD 应该被当做 ENTRYPOINT 的默认参数. docker run &lt;image&gt; &lt;arg1&gt; &lt;arg2&gt; ... 会把之后的参数传递给 ENTRYPOINT, 覆盖 CMD 指定的参数. 可以用 docker run --entrypoint 来重置默认的 ENTRYPOINT. ENTRYPOINT 和 CMD 的关系: No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME指定镜像内的目录为数据卷 12VOLUME ["/path/to/volume"]VOLUME /path/to/volume1 /path/to/volume2 在容器运行的时候, Docker 会把镜像中的数据卷内容复制到容器的数据卷中去. 如果在接下来的 DockerFile 指令中修改了数据卷中的内容, 则修改无效. ONBUILD向镜像中添加一个触发器(Trigger), 当以该镜像为 base image 再次构建新的镜像时, 会触发并执行其中的指令. 123ONBUILD [INSTRUCTION]# 在下一次以此镜像为base image的构建中，执行 ADD . /app/src, 将项目代目添加到新镜像中去ONBUILD ADD . /app/src 总结 DockerFile 构建过程解析 从基础镜像运行一个容器. 执行一条指令, 对容器做出修改. 执行类似 docker commit 的操作提交一个新的镜像层. docker 再基于刚提交的镜像运行一个新的容器. 执行 DockerFile 中的下一条指令直到所有指令执行完成. .dockerignore 文件构建镜像时, Docker 需要先准备 context, 将所有需要的文件收集到进程中. 默认的 context 包含 DockerFile 目录中的所有文件, 但是实际上并不需要 .git 目录, node_modules 目录等内容. .dockerignore 的作用和语法类似于 .gitignore, 可以忽略一些不需要的文件, 这样可以有效加快镜像构建时间, 同时减少 Docker 镜像的大小. 12.git/node_modules/ 常用指令docker 命令介绍1234567891011121314151617181920212223242526272829303132docker --help管理命令: container 管理容器 image 管理镜像 network 管理网络命令： attach 介入到一个正在运行的容器 build 根据 Dockerfile 构建一个镜像 commit 根据容器的更改创建一个新的镜像 cp 在本地文件系统与容器中复制 文件/文件夹 create 创建一个新容器 exec 在容器中执行一条命令 images 列出镜像 kill 杀死一个或多个正在运行的容器 logs 取得容器的日志 pause 暂停一个或多个容器的所有进程 ps 列出所有容器 pull 拉取一个镜像或仓库到 registry push 推送一个镜像或仓库到 registry rename 重命名一个容器 restart 重新启动一个或多个容器 rm 删除一个或多个容器 rmi 删除一个或多个镜像 run 在一个新的容器中执行一条命令 search 在 Docker Hub 中搜索镜像 start 启动一个或多个已经停止运行的容器 stats 显示一个容器的实时资源占用 stop 停止一个或多个正在运行的容器 tag 为镜像创建一个新的标签 top 显示一个容器内的所有进程 unpause 恢复一个或多个容器内所有被暂停的进程 docker 基本命令 查看系统内核 1uname -r docker 支持的 Linux 内核版本最低为 3.10. 启动 docker 1systemctl start docker 查看 docker 版本 1docker version 显示 docker 系统信息 1docker info 登录 docker 1docker login 操作 docker 镜像 测试 1docker run hello-world 检索 image 1docker search &lt;image-name&gt; 下载 image 1docker pull &lt;image-name&gt; 删除一个或多个镜像 12# 删除一个或多个镜像docker rmi &lt;image-name1&gt; &lt;image-name2&gt; 显示一个镜像的历史 1docker history &lt;image-name&gt; 容器相关 新建并启动容器 1docker run [OPTIONS] &lt;image-name&gt; [COMMAND] [ARG...] –name=”容器新名字”: 为容器指定一个名称;-d: 后台运行容器，并返回容器ID, 即启动守护式容器;-i: 以交互模式运行容器，通常与 -t 同时使用;-t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用;l-P: 随机端口映射;-p: 指定端口映射，有以下四种格式 1234ip:hostPort:containerPortip::containerPorthostPort:containerPortcontainerPort 创建并交互式进入容器 1docker run -i -t &lt;image-name&gt; /bin/bash 列出当前所有正在运行的容器 1docker ps [OPTIONS] -a: 列出当前所有正在运行的容器 + 历史上运行过的-l: 显示最近创建的容器.-n: 显示最近 n 个创建的容器.-q: 静默模式, 只显示容器编号.–no-trunc: 不截断输出. 退出容器 12exit # 容器停止退出Ctrl + P + Q # 容器不停止退出 启动容器 1docker start &lt;container-id/name&gt; 重启容器 1docker restart &lt;container-id/name&gt; 停止容器 1docker stop &lt;container-id/name&gt; 强制停止容器 1docker kill &lt;container-id/name&gt; 删除已停止的容器 1234docker rm &lt;container-id&gt;# 一次删除多个容器docker rm -f $(docker ps -a -q)docker ps -a -q | xargs docker rm 启动守护式容器 1docker run -d &lt;container-name&gt; 要想 Docker 容器后台运行, 就必须有一个前台进程. 如果容器运行的命令不是一直挂起的命令, 会自动退出. 所以最好让程序以前台进程的方式运行. 查看容器日志 1docker logs -f -t --tail &lt;container-id&gt; -t: 加入时间戳 -f: 跟随最新的日志打印 –tail n: 显示最后 n 条 查看容器内运行的进程 1docker top &lt;container-id&gt; 查看容器内部细节 1docker inspect &lt;container-id&gt; 进入正在运行的容器并以命令行交互 1234# 以交互式执行 /bin/bashdocker exec -it &lt;container-id&gt; /bin/bash# 重新进入docker attach &lt;container-id&gt; 区别: attach 直接接入容器启动命令的终端, 不会启动新的进程; exec 是在容器中打开新的终端, 并且可以启动新的进程. 从容器内拷贝文件到宿主机上 1docker cp &lt;container-id&gt;:&lt;container-dir&gt; &lt;host-dir&gt;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJVM.html</url>
    <content type="text"><![CDATA[简介JVM 组成架构 类加载器 类加载器主要用于定位类定义的二进制信息, 然后将这些信息解析并加载至虚拟机. 类加载器在方法区构造具有这个类的信息的数据结构后, 会在堆上创建一个 Class 对象作为访问这个数据结构的接口. 同时, 类加载器还会初始化类的静态数据, 也就是调用类的 &lt;clinit&gt;() 方法. 运行时数据区 堆: 线程共享. 存放对象及数组实例, 也就是运行期间 new 出来的对象. 方法区: 线程共享. 存放类型信息和运行时常量池. 程序计数器PC(Program Counter): 线程私有. 生命周期与线程相同, 是对 CPU 中 PC 的一种模拟, 如果正在执行 Java 方法, 存放下一条字节码指令的地址; 执行 Native 方法, 存放的值为空. Java 栈: 线程私有. 生命周期与线程相同, 栈中存放栈帧(用于进行方法调用和返回), 局部变量以及计算的中间结果. 本地方法栈: 用于支持本地方法调用. 执行引擎 以指令为单位读取 Java 字节码. 像 CPU 一样, 一条一条地执行机器指令. 每个字节码指令都由一个 1字节 的操作码和附加的操作数组成. 执行引擎取得一个操作码, 然后根据操作数来执行任务, 完成后就继续执行下一条操作码. 垃圾回收器 用于管理运行时数据区的分配和释放. Java 类的加载机制类的加载机制指的是将类的 .class 文件中的二进制数据读取到内存中, 将其存放在运行时数据区的方法区内, 然后在堆创建一个 java.lang.Class 对象, 用来封装类在方法区类的数据结构. 类的加载的最终结果是位于堆中的 Class 对象, 其封装了类在方法区内的数据结构, 并且对外提供了访问方法区类的数据结构的接口. 类加载器并不需要等到某个类被首次主动使用时再加载它, JVM 规范允许类加载器在预料某个类将要被使用时就预先加载它, 如果在预先加载的过程中遇到了 .class 文件缺失或存在错误, 类加载器必须在程序首次主动使用该类时才报告错误. 如果这个类一直没有被程序主动使用, 那么类加载器就不会报错. 类的生命周期 类的加载过程类的加载过程包括了: 加载, 验证, 准备, 解析, 初始化五个阶段. 在这五个阶段中, 加载, 验证, 准备, 初始化和卸载这五个阶段发生的顺序是确定的. 解析阶段可以在初始化之后再开始, 这是为了支持 Java 的运行时绑定(也称为 动态绑定或晚期绑定). 加载将 Java 字节码数据从不同的数据源读取到 JVM 中, 并映射为 JVM 认可的数据结构(Class 对象). 在加载阶段, 虚拟机需要完成以下三件事: 通过一个类的全限定类名来获取此类的二进制字节流. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构. 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象, 作为对方法区这些 数据的访问的入口. 加载阶段是可控性最强的阶段, 开发人员可以使用自定义自己的类加载器来完成类的加载. 加载阶段完成后, 虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中, 而且在堆中也创建一个 java.lang.Class 类的对象, 这样便可以通过该对象访问方法区中的这些数据. 连接连接就是将已读入到内存的类的二进制数据合并到虚拟机的运行时环境中, 即把原始的类定义信息平滑地转化入 JVM 运行中. 该阶段可分为三个步骤: 验证: 虚拟机安全的重要保障, JVm 需要核验字节信息是符合 Java 虚拟机规范的, 否则就被认为是 VerifyError, 确保被加载类的正确性. 验证阶段有可能触发更多 class 的加载. 准备: 为类的静态变量分配内存, 并将其初始化为默认值. 解析: 将常量池中的符号引用替换为直接引用. 验证验证是连接阶段的第一步, 这一阶段的目的是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求, 并且不回危害到虚拟机自身的安全. 验证阶段大致会完成 4 个阶段的检验动作. 文件格式验证: 验证字节流是否符合 Class 文件格式的规范. 例如: 是否以 0XCAFEBABE 开头, 主次版本号是否在当前虚拟机的处理范围之内, 常量池中的常量是否有不被支持的类型. 元数据验证: 对字节码描述的信息进行语义分析, 确定程序语义是合法的, 符合逻辑的. 字节码验证: 通过数据流和控制流分析, 确定程序语义是合法的, 符合逻辑的. 符号引用验证: 确保解析动作能正确执行. 验证阶段是非常重要的, 但不是必须的, 它对程序运行期没有影响, 如果所引用的类经过反复验证, 那么可以考虑采用 -Xverifynone 参数来关闭大部分的类验证措施, 以缩短虚拟机类加载的时间. 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段, 这些内存都将在方法区中分配. 对于该阶段有以下几点需要注意: 这时进行内存分配的仅包括类变量(static), 而不包括实例变量, 实例变量会在对象实例化时随着对象一块分配在堆中. 这里所设置的初始值通常情况下是数据类型默认的零值(如 int-&gt;0, long-&gt;0L, Object-&gt;null, boolean-&gt;false 等), 而不是在 Java 代码中被显式地赋予的值. 假设一个类变量的定义为: publib static int val = 3; 那么变量 val 在准备阶段过后的初始值为 0, 而不是 3. 因为这时候尚未开始执行任何 Java 方法, 而把 val 赋值为 3 的 public static 指令是在程序编译后, 存放于类构造器 &lt;clinit&gt;() 方法中的, 所以把 val 赋值为 3 的动作将在初始化阶段才会执行. 注意: 对于基本数据类型来说, 对于类变量(static)和全局变量, 如果不显式地对其赋值而直接使用, 系统会为其赋予默认的零值, 而对于局部变量来说, 在使用前必须显式地为其赋值, 否则编译时不通过. 对于引用数据类型来说, 如果没有对其进行显式地赋值而直接使用, 系统都会为其赋予默认的零值. 如果在数组初始化时没有对数组中的个元素赋值, 那么其中的元素将根据对应的数据类型而被赋予默认的零值. 如果类字段的字段属性表中存在 ConstantValue 属性, 即同时被 final 和 static 修饰, 那么在准备阶段变量就会被初始化为 ConstantValue 属性所指定的值. 假设一个类变量被定义为: public static final int val = 3; 编译时 Javac 将会为 val 生成 ConstanValue 属性, 在准备阶段虚拟机就会根据 ConstantValue 的设置将 val 赋值为 3. 可以理解为 static final 常量在编译器就将其结果放入了调用它的类的常量池中. 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程. 解析动作主要针对类或接口, 字段, 类方法, 接口方法, 方法类型, 方法句柄和调用点限定符 7 类符号引用进行. 符号引用就是一组符号来描述目标, 可以使任何字面量. 直接引用就是直接指向目标的指针, 相对偏移量或一个间接定位到目标的句柄. 初始化初始化为类的静态变量赋予正确的初始值. JVM 负责对类进行初始化, 主要对类变量进行初始化, 父类型的初始化逻辑优先于当前类型的逻辑. 在 Java 中对类变量进行初始值设定有两种方式: 在声明类变量时为其指定初始值. 使用静态代码块为类变量指定初始值. 两者的顺序取决于在代码中的先后顺序. 如: 12345678910111213private static String name = "Anny";static&#123; name = "Bob";&#125;// 结果// name = "Bob";// ----------------------------------static &#123; name = "Bob";&#125;private static String name = "Anny";// 结果// name = "Anny"; JVM 初始化步骤: 如果这个类还没有被加载和连接, 程序先加载并连接该类. 如果该类的直接父类还没有被初始化, 则先初始化其直接父类. 如果类中有初始化语句, 则系统依次执行这些初始化语句. 类初始化时机: 只有当对类的主动使用的时候才会导致类的初始化, 类的主动使用包括以下 5 中情况: 遇到 new, getstatic, putstatic 或 invokestatic 这 4 条字节码指令时没初始化则触发初始化. 使用场景: 使用 new 关键字实例化对象, 读取一个类的静态字段(被 static final 修饰, 已在编译器把结果放入常量池的静态字段除外), 调用一个类的静态方法. 使用 java.lang.reflect 包的方法对类进行反射调用的时候. 当初始化一个类的时候, 如果发现其父类还没有进行初始化, 则需先触发其父类的初始化. 当虚拟机启动时, 用户需指定一个要加载的主类(包含 main() 方法的那个类), 虚拟机会先初始化那个类. 当使用 JDK1.7 的动态语言支持时, 如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄, 并且这个方法句柄所对应的的类没有进行过初始化, 则需先触发其初始化. 除此以外, 所有引用类的方法都不会触发初始化, 叫做被动引用. 如: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class SuperClass &#123; static &#123; System.out.println("SuperClass init!"); &#125; public static int value = 1127;&#125;public class SubClass extends SuperClass &#123; static &#123; System.out.println("SubClass init!"); &#125;&#125;public class ConstClass &#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLOWORLD = "hello world!"&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); /** * output : SuperClass init! * * 通过子类引用父类的静态对象不会导致子类的初始化 * 只有直接定义这个字段的类才会被初始化 */ SuperClass[] sca = new SuperClass[10]; /** * output : * * 通过数组定义来引用类不会触发此类的初始化 * 虚拟机在运行时动态创建了一个数组类 */ System.out.println(ConstClass.HELLOWORLD); /** * output : * * 常量在编译阶段会存入调用类的常量池当中，本质上并没有直接引用到定义类常量的类， * 因此不会触发定义常量的类的初始化。 * “hello world” 在编译期常量传播优化时已经存储到 NotInitialization 常量池中了。 */ &#125;&#125; 在准备阶段, 变量都被赋予了初始值, 但是到了初始化阶段, 所有变量还要按照用户编写的代码进一步初始化. 也可以说, 初始化阶段是执行类构造器 &lt;clinit&gt;() 方法的过程. &lt;clinit&gt;() 方法是由编译器自动收集类中定义的所有类变量的赋值动作和静态语句块(static 语句块)中的语句合并生成的. 编译器收集的顺序是由语句在源文件中出现的顺序决定的. 静态语句块中只能访问到定义在静态语句块之前的变量; 定义在它之后的变量, 在前面的静态语句块中可以赋值, 但是不能访问. &lt;clinit&gt;() 方法与构造函数 &lt;init&gt;() 方法不同, 它不需要显式地调用父类构造器, 虚拟机会保证在子类的 &lt;clinit&gt;() 方法执行之前, 父类的 &lt;clinit&gt;() 已经执行完毕. 因此, 在虚拟机中第一个被执行的 &lt;clinit&gt;() 方法一定是 java.lang.Object 的. 如果一个类中既没有静态语句块, 也没有静态变量赋值动作, 那么编译器都不会为类生成 &lt;clinit&gt;() 方法. 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境中能正确的加锁, 同步. 如果多个线程初始化一个类, 那么只有一个线程回去执行 &lt;clinit&gt;() 方法, 其他线程都需要等待. 结束生命周期:在以下几种情况下, Java 虚拟机将结束生命周期: 执行了 System.exit()方法. 程序正常执行结束. 程序在执行过程中遇到了异常或错误而异常终止. 由于操作系统出现错误而导致 Java 虚拟机进程终止. 类加载器虚拟机设计团队把类加载阶段中的”通过一个类的全限定类名来获取此类的二进制字节流”这个步骤放到 Java 虚拟机外部去实现, 以便让应用程序自己决定如何去获取所需要的类. 实现这个动作的代码模块称为类加载器. 从 Java 虚拟机的角度来说, 只存在两种类加载器: 一种是启动类加载器(C++ 实现, 是虚拟机的一部分); 另一种就是其他所有的类加载器(Java 语言实现, 独立于虚拟机外部, 且全部继承自抽象类 java.lang.ClassLoader). 从开发人员的角度来看, 类加载器可以划分为以下 3 种: 注意: 这里的父类加载器并不是通过继承来实现的, 而是采用组合实现的. 启动类加载器(Bootstrap ClassLoader): 负责加载存放在 JAVA_HOME/JRE/lib 目录中的, 或被 -Xbootclasspath 参数指定的路径中的, 并且能被虚拟机识别的类库(如 rt.jar, 所有的 java. 开头的类均被 Bootstrap ClassLoader 加载). 启动类加载器时无法被 Java 程序直接引用的. 通过 ClassLoader.getParent() 方法获取会得到 null. 扩展类加载器(Extension ClassLoader): 这个加载器由 sun.misc.Launcher$ExtClassLoader 实现, 它负责加载 Java_HOME/JRE/lib/ext 目录中的, 或者被 java.ext.dirs 系统变量所指定的路径中的所有类库(如 javax. 开头的类). 开发者可以直接使用扩展类加载器. 应用程序类加载器(Application ClassLoader): 该类加载器由 sun.misc.Launcher$AppClassLoader 来实现, 它负责加载用户类路径(classpath)所指定的类. 开发者可以直接使用该类加载器, 如果应用程序中没有自定义过自己的类加载器, 一般情况下这个就是程序中默认的类加载器. 应用程序都是由这三种类加载器相互配合进行加载的, 如果有必要, 还可以加入自定义的类加载器. 因为 JVM 自带的 ClassLoader 只会从本地文件系统中加载标准的 Java Class 文件, 因此如果编写了自定义的 ClassLoader, 可以实现以下几点: 在执行非置信代码之前, 自动验证数字签名. 动态地创建符合用户特定需要的定制化构建类. 从特定的场所取得 Java Class, 如数据库和网络等. JVM 类加载机制 全盘负责: 当一个类加载器负责加载某个 Class 时, 该 Class 所依赖的和引用的其他 Class 也将由该类加载器负责载入, 除非显示使用另外一个类加载器来载入. 父类委托: 先让父类加载器试图加载该类, 只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类. 缓存机制: 缓存机制将会保证所有加载过的 Class 都会被缓存, 当程序中需要使用到某个 Class 时, 类加载器会先从缓存区寻找该 Class, 只有缓存区不存在, 系统才会读取该类对应的二进制数据, 并将其转换成 Class 对象, 存入缓存区. 这也就解释了为什么修改 Class 后必须重启 JVM, 程序的修改才会生效. 类的加载类加载有三种方式: 命令行启动应用时由 JVM 初始化加载. 通过 Class.forName() 方法动态加载. 1Class.forName("com.mysql.jdbc.Driver"); 通过 ClassLoader.loadClass() 方法动态加载. 12345678ClassLoader loader = TestClass.class.getClassLoader();loader.loadClass("TestClass");// -----------------------public class TestClass&#123; static&#123; System.out.println("TestClass init...") &#125;&#125; Class.forName() 和 ClassLoader.loadClass() 区别 Class.forName() 方法: 12public static Class&lt;?&gt; forName(String name, boolean initialize, ClassLoader loader) 参数的含义是: name: 要加载的 Class 的全限定类名. initialize: 是否初始化. loader: 指定的 ClassLoader. ClassLoader.loadClass() 方法: 12protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException 参数的含义是: name: 要加载的 Class 的全限定类名. resolve: 是否要进行连接. 两个方法的比较: Class.forName(className): 调用的是 Class.forName(className,true,classLoader), 意思是在加载类之后必须初始化. 也就是说, 在执行过此方法后, 目标对象的静态代码块已经被执行, 静态变量也已经被初始化. ClassLoader.loadClass(className): 调用的是 ClassLoader.loadClass(className,false), 意思是目标对象被装载后不进行连接, 也就意味着不会去执行该类静态代码块, 静态变量还是默认值. 在使用 newInstance() 创建实例时才连接. 双亲委派模型双亲委派模型的工作流程是: 如果一个类加载器收到了类加载的请求, 它首先不会去尝试加载这个类, 而是把请求委托给父加载器去完成. 依次向上. 因此, 所有的类加载请求最终都应该被传递到顶层的启动类加载器中, 只有当父加载器在它的搜索范围中没有找到所需的类时, 即无法完成该加载, 只加载器才会尝试自己去加载该类. 双亲委派机制: 当 AppClassLoader 加载一个 Class 时, 它首先不会自己尝试加载这个类, 而是把类加载请求委派给 ExtClassLoader 去完成. 当 ExtClassLoader 加载一个 Class 时, 它首先不会自己尝试加载这个类, 而是把类加载请求委派给 BootstrapClassLoader 去完成. 如果 BootstrapClassLoader 加载失败(例如在 $JAVA_HOME/jre/lib 中未查找到该 Class), 会让 ExtClassLoader 来尝试加载. 如果 ExtClassLoader 加载失败, 则会使用 AppClassLoader 来加载, 如果 AppClassLoader 也加载失败, 则会报异常 ClassNotFoundException. JVM 内存结构Java 虚拟机在执行 Java 程序的过程中会把它管理的内存区域划分为若干个不同的数据区域. 根据《Java虚拟机规范》的规定, Java 虚拟机所管理的内存包括的运行时数据区域分为: 方法区, 堆, 栈, 程序计数器, 本地方法栈. 其中堆是 JVM 中最大的一块, 可以进一步分为新生代和老年代, 新生代又可进一步分为Eden(伊甸区)和2个Survivor(幸存区)[^1]. 可参考下图进行理解: 栈(Java Stack)栈是线程私有的内存区域, 它的生命周期与线程相同. 栈描述的是 Java 方法执行的内存模型, 每个方法执行的同时都会创建一个栈帧(Stack Frame)用于存储局部变量表, 操作数栈, 动态链接和方法出口等信息. 每一个方法被调用直至执行完成的过程, 就对应着一个栈帧在栈中从入栈到出栈的过程. 局部变量表存放了编译期可知的各种基本数据类型(boolean, byte, char, short, int, float, long, double), 对象引用(reference 类型, 它不等同于对象本身, 可能是一个指向对象起始地址的指针, 也可能是指向一个代表对象的句柄或其他与此对象相关的位置, 见下图)和 returnAddress 类型(指向一条字节码指令的地址). 其中 64 位长度的 long 和 double 类型的数据会占用 2 个局部变量空间(Slot), 其余的数据类型只占用 1 个. 局部变量表所需的内存在编译期间完成分配, 当进入一个方法时, 这个方法需要在栈帧中分配多大的局部变量空间是完全确定的, 在方法运行期间不会改变局部变量表的大小. 在 Java 虚拟机规范中, 对这个区域规定了两种异常情况: 如果线程请求的栈深度大于虚拟机所允许的深度, 将抛出 StackOverflowError 异常. 如果栈可以动态扩展(当前大部分的 Java 虚拟机都可动态扩展, 只不过 Java 虚拟机规范中也允许固定长度的栈), 当扩展时无法申请到足够的内存时会抛出 OutOfMemoryError 异常. 本地方法栈(Native Method Stacks)本地方法栈与栈所发挥的作用是非常相似的, 它们之间的区别不过是: 栈为虚拟机执行 Java 方法(也就是字节码)服务, 而本地方法栈则是为虚拟机使用到的Native方法服务. 虚拟机规范中对本地方法栈的方法使用的语言, 使用方式与数据结构没有强制规定. 因此具体的虚拟机可以自由实现它, 甚至有的虚拟机(譬如 Sun HotSpot)直接就把本地方法栈和栈合二为一. 与栈一样, 本地方法栈也会抛出相同的异常. 程序计数器(Program Counter Register)程序计数器是一块较小的内存空间, 是线程私有的. 它的作用可以看作是当前线程所执行的字节码的行号指示器. 在虚拟机的概念模型中, 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行指令的字节码指令. 分支, 循环, 跳转, 异常处理, 线程恢复等基础功能都需要依赖这个计数器来完成. 由于 Java 虚拟机的多线程是通过线程轮流切换并分配处理器换行执行时间的方式来实现的, 在任何一个确定的时刻, 一个处理器(对于多核处理器来说是一个核)只会执行一条线程中的指令. 因此, 为了线程切换后能恢复到正确的执行位置, 每条线程都需要一个独立的程序计数器, 各条线程之间的计数器互不影响, 独立存储. 如果线程正在执行的是一个 Java 方法, 这个计数器记录的是正在执行的虚拟机字节码指令的地址; 如果正在执行的是 Native 方法, 这个计数器的值为空(Undefined). 此内存区域是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域. 堆(Heap)对于大多数应用来说, 堆是 Java 虚拟机所管理的内存中最大的一块. 堆是被所有线程共享的一块内存区域, 在虚拟机启动时创建. 此内存区域的唯一目的就是存放对象实例, 几乎所有的对象实例都在这里分配内存. 堆是GC(垃圾收集器)管理的主要区域, 如果从内存回收的角度来看, 由于现在收集器都是采用的分代收集算法, 所以堆中还可以细分为: 新生代和老年代, 再进一步细分有: EdenSpace(伊甸区), FromSpace(幸存区1)和ToSpace(幸存区2). 根据 Java 虚拟机规范的规定, 堆可以处于物理上不连续的内存空间中, 只要逻辑上连续即可. 在实现时, 既可以实现成固定大小的, 也可以是可扩展的, 不过当前主流的虚拟机都是按照可扩展来实现的(通过 -Xmx 和 -Xms 控制). 如果在堆中没有内存完成实例分配, 并且堆也无法再扩展时, 将会抛出 utOfMemoryError: Java heap space 异常. 方法区(Method Area)方法区与堆一样, 是各个线程共享的内存区域. 它用于存储已被虚拟机加载的类信息, 常量, 静态变量, 即时编译器编译后的代码等数据, 也就是用来存储类的描述信息-元数据(Metadata). 虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分, 但它却有一个别名叫做 Non-Heap(非堆), 目的应该是与堆区分. 相对而言, GC 对于这个区域的收集是很少出现的. 当方法区无法满足内存分配需求时, 将抛出 OutOfMemoryError 异常. 注意: 如果使用的是 Sun HotSpot 虚拟机, 在 JDK7 及之前版本, 我们也习惯称它为永久代(Permanent Generation), 更确切地来说, 应该是”HotSpot 使用永久代实现了方法区“. 而从 JDK8 开始, 永久代已经被彻底移除, 取而代之的是元空间(Metaspace), 它使用本地内存来存储类元数据信息. 也就是说, 只要本地内存足够, 它不会出现像永久代中 java.lang.OutOfMemoryError: PermGen space 这种错误. 同样的, 对永久代的设置参数 Permsize 和 MaxPermSize 也会失效. 默认情况下, Metaspace的大小可以动态调整, 或者使用新参数 MaxMetaspaceSize 来限制本地内存分配给类元数据的大小. 运行时常量池(Runtime Constant Pool)运行时常量池是方法区的一部分. Class 文件中除了有类的版本, 字段, 方法, 接口等描述信息外, 还有一项信息是常量池(Constant pool table), 用于存放编译期生成的各种字面量和符号引用, 这部分内容将在类加载后进入运行时常量池中存放. 运行时常量池相比较于 Class 文件中的常量池的另外一个特性是具备动态性, Java 语言并不要求常量一定只有编译器才产生, 也就是并非预置到 Class 文件中的常量池的内容才能进入方法区的运行时常量池, 运行期间也可能将新的常量放入池中. String.intern() 就是扩展常量池的一个方法. 它的作用是: 查找当前常量池中是否已有相同的字符串常量, 如果有就返回其引用, 如果没有就在常量池中添加对应的字符串, 并返回对应字符串常量的引用. 使用 new String(&quot;xxx&quot;) 是在堆中创建一个 String 对象实例, 所以两个 new String(&quot;hello&quot;) 使用 == 时返回 false. 而使用 new String(&quot;hello&quot;).intern() 则为 true. 直接内存(Direct Memory)直接内存并不是虚拟机运行时数据区的一部分, 也不是 Java 虚拟机规范中定义的内存区域. 但这部分内存也被频繁地使用, 而且也可能导致 OutOfMemoryError 异常出现. 在 JDK 中最直观的表现就是 NIO, 基于Channel(通道)与Buffer(缓冲区)的 I/O 方式, 它可以使用 Native 函数库直接分配堆外内存, 然后通过一个存储在堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作. 这样能在一些场景中显著提高性能, 因为避免了在堆和Native堆中来回复制数据. 垃圾回收(GC) 垃圾回收(Garbage Collection)简称GC. 在 JVM 中, 程序计数器, 栈, 本地方法栈都是随线程而生随线程而灭, 栈帧随着方法的进入和退出做入栈和出栈操作, 实现了自动的内存清理. 而堆和方法区则不同, 一个接口中的多个实现类需要的内存可能不一样, 一个方法中的多个分支需要的内存也可能不一样, 我们只有在程序处于运行期才知道哪些对象会创建, 这部分的内存的分配和回收都是动态的. 垃圾回收所关注的就是这部分内存. 对象存活判断判断对象是否存活一般有两种方式: 引用计数: 每个对象有一个引用计数属性, 新增一个引用时计数加 1; 引用释放时计数减 1. 计数为 0 时可以回收. 此方法比较简单, 但无法解决对象相互循环引用的问题. 从图中可以看出, 如果直接把 Obj1-reference 和 Obj2-reference 置为 null, 在堆当中的两块内存依然保持着互相引用无法回收. 可达性分析(Reachability Analysis): 从 GC Roots 开始向下搜索, 搜索所走过的路径称为引用链. 当一个对象到 GC Roots 没有任何引用链相连时, 则证明此对象是不可用的, 即不可达对象. 在 Java 中, GC Roots 包括: 栈(栈帧中的本地变量表)中引用的对象. 方法区中类静态属性引用的对象. 方法区中常量引用的对象. 本地方法栈中 JNI(即 Native 方法) 引用的对象. 引用从 JDK1.2 开始, 对象的引用被划分为 4 个级别, 从而使程序能够更加灵活地控制对象的生命周期. 这 4 种级别由高到低依次为: 强引用, 软引用, 弱引用和虚引用. 强引用(Strong Reference)强引用是最简单, 使用最普遍的引用. 如果一个对象具有强引用, 那么垃圾回收器就绝不会回收它. 当内存空间不足时, Java 虚拟机宁愿抛出 OutOfMemoryError 异常, 也不会通过回收具有强引用的对象来解决内存不足的问题. 如果强引用对象不使用时, 需要弱化从而使 GC 能够回收, 如下: 1234// 创建强引用对象Object strongReference = new Object();// 弱化方便 GC 回收strongReference = null; 如果在一个局部变量是强引用, 这个引用保存在栈中, 而真正的引用内容保存在堆中. 当局部变量所在的方法执行完成后, 就会退出方法栈, 则引用对象的引用数为 0, 这个对象会被回收. 软引用(SoftReference)如果一个对象只具有软引用, 则内存空间充足时, 垃圾回收器不会回收它. 如果内存空间不足时, 就会回收这些对象. 软引用可以用来实现内存敏感的高速缓存. 软引用可以和一个引用队列(ReferenceQueue)结合使用, 如果软引用所引用对象被垃圾回收, Java 虚拟机就会把这个软引用加入到与之关联的引用队列中. 12345678// 创建引用队列ReferenceQueue&lt;String&gt; refQueue = new ReferenceQueue&lt;&gt;();// 创建软引用对象String string = new String("soft");SoftReference&lt;String&gt; softReference = new SoftReference(string,refQueue);// 当软引用被垃圾回收后, 调用 poll() 方法可以获得该对象Reference&lt; ? extends String&gt; ref = refQueue.poll(); 弱引用(WeakReference)弱引用与软引用的区别在于: 只具有弱引用的对象拥有更短暂的生命周期. 一旦进行垃圾回收, 无论当前内存空间足够与否, 弱引用对象都会被回收. 12345// 创建弱引用对象String string = new String("weak");WeakReference&lt;String&gt; weakReference = new WeakReference&lt;&gt;(string);// 获取弱引用所引用的对象String res = weakReference.get(); 弱引用也可以和引用队列结合使用. 注意: 这里是用 new 来创建的字符串对象, 引用的对象在堆中, 所以会被 GC 回收. 但如果弱引用所引用的对象位于字符串常量池中时(如 String str = &quot;str&quot;), 因为常量池中的字符串不会被 GC 回收(有一个表来维护字符串), 这种情况下, get() 方法在 GC 后仍会获得字符串常量池中的对象. 虚引用(PhantomReference)虚引用与其他引用不同, 虚引用不会决定对象的声明周期. 虚引用在任何时候都可能被垃圾回收. 注意: 虚引用必须和引用队列结合使用, 当垃圾回收器准备回收一个对象时, 如果发现它还有虚引用, 就会在回收对象的内存之前, 把这个虚引用加入到与之关联的引用队列中. 1234567891011// 创建引用队列ReferenceQueue&lt;String&gt; refQueue = new ReferenceQueue&lt;&gt;();// 创建虚引用String str = new String("PhantomReference");PhantomReference&lt;String&gt; phantomReference(str,refQueue);System.out.println(phantomReference.get()); // 获取的结果为 null// 触发 GCSystem.gc();// 此时虚引用对象被加入到引用队列System.out.println(refQueue.poll().get()); // 获取到 "PhantomReference" 程序可以通过判断引用队列中是否已经加入了虚引用来了解被引用的对象是否将要进行垃圾回收. 如果程序发现某个虚引用已经被加入到引用队列, 那么就可以在所引用的对象的被回收之前采取最后的收尾工作. 比如资源清理和释放, 它比 finalize 更灵活, 可以基于虚引用做更安全可靠的对象关联的资源回收. 垃圾收集算法标记-清除算法(Mark-Sweep)标记-清除算法分为标记和清除两个阶段: 首先标记出所有需要回收的对象, 在标记完成后统一回清除回收掉所有被标记的对象. 之所以说它是最基础的收集算法, 是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的. 主要的缺点有两个: 一个是效率问题, 标记和清除过程的效率都不高; 另外一个是空间问题, 标记清除之后会产生大量的内存碎片, 内存碎片太多可能会导致当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾回收. 复制收集算法(Copying)复制收集算法将可用内存按容量划分为大小相等的两块, 每次只使用其中的一块. 当这一个块的内存用完时, 就将还存活着的对象复制到另一块内存上, 然后再把已使用的内存空间一次清理掉. 这样使得每次都是对其中的一块内存进行回收, 内存分配时也就不用考虑内存碎片等复杂情况, 只要移动堆顶指针, 按顺序分配内存即可, 实现简单, 运行高效. 这种算法的代价是将可用内存缩小为原来的一半, 持续复制长生存期的对象则导致效率降低. 标记-压缩算法(Mark-Compact)复制收集算法在对象存活率较高时就要频繁执行复制操作, 效率将会变低. 更关键的是, 如果不想浪费 50% 的空间, 就需要有额外的空间进行分配担保, 以应对被使用的内存中所有对象都 100% 存活的极端情况, 所以在老年代一般不能直接选用这种算法. 根据老年代存活期长的特点, 有人提出了另外一种标记-整理算法, 标记过程仍然与标记-清除算法相同, 但后续步骤不是直接对可回收对象进行整理, 而是让所有存活的对象都向一端移动, 然后直接清理掉端边界以外的内存. 分代收集算法(Generation Collection)分代回收算法的基本思想是: 根据垃圾回收对象的特性, 对不同阶段使用最合适的算法用于本阶段的垃圾回收. 它将内存区间根据对象的特点分成几块, 根据每块内存区间的特点, 使用不同的回收算法, 以提高垃圾回收的效率. 以 Hot Spot 虚拟机为例, 它将所有的新建对象都放入新生代的内存区域, 新生代的特点是对象会很快被回收. 因此, 在新生代就选择效率较高的复制算法. 当一个对象经过一次回收后依然存活, 它的年龄就加 1, 如果对象的年龄大于阈值(默认为 15, 通过 -XXMaxTenuringThreshold 参数设置, 最大为 15), 就会被放入老年代的内存空间. 在老年代中, 几乎所有的对象都是数次回收后依然得以幸存的, 因此, 可以认为这些对象在一段时期内, 甚至在应用程序的整个生命周期中, 将是常驻内存. 如果依然使用复制算法收集老年代, 将需要复制大量对象. 再加上老年代回收性价比要低于新生代, 因此这种做法也是不可取的. 根据分代的思想, 可以对老年代的回收使用与新生代不同的标记-压缩算法, 以提高垃圾回收效率. 垃圾收集器 如果说收集算法是内存回收的方法论, 垃圾收集器就是内存回收的具体实现. Serial 收集器Serial 收集器是最古老的以及效率最高的收集器, 只使用一个线程去回收, 但可能会产生较长的停顿. 新生代和老年代使用串行回收. 新生代使用复制算法, 老年代使用标记-压缩算法. 垃圾回收的过程中会Stop The World(服务暂停). 参数控制: -XX:+UseSerialGC: Serial 收集器 ParNew 收集器ParNew 收集器其实就是 Serial 收集器的多线程版本. 新生代并行, 老年代串行. 新生代使用复制算法, 老年代使用标记-压缩算法. 垃圾回收的过程中同样会Stop The World(服务暂停). 参数控制: -XX:+UseParNewGC: ParNew 收集器 -XX:ParallelGCThreads: 限制线程数量 Parallel Scavenge 收集器Parallel Scavenge 收集器类似 ParNew 收集器, 其更关注系统的吞吐量. 可以通过参数来打开自适应调节策略, 虚拟机会根据当前系统的运行情况收集性能监控信息, 动态调整这些参数以提供最合适的停顿时间或最大的吞吐量; 也可以通过参数控制 GC 的时间不大于多少毫秒或者比例. 新生代使用复制算法, 老年代使用标记-压缩算法. 新生代并行, 老年代串行. 参数控制: -XX:+UseParallelGC: 使用 Parallel 收集器 Serial Old 收集器 Paralle Old 收集器Parallel Old 收集器是Paralle Scavenge 收集器的老年代版本, 使用多线程和标记-压缩算法. 这个收集器是在 JDK1.6 才开始提供. 新生代使用复制算法, 老年代使用标记-压缩算法. 新生代和老年代并行. 参数控制: -XX:+UseParallelOldGc: 使用 Parallel 收集器 CMS 收集器CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器. 目前很大一部分的 Java 引用都集中在互联网站或 B/S 系统的服务端上, 这类引用尤其中是服务的响应速度, 希望系统停顿时间最短, 以给用户带来较好的体验. CMS GC 采用了标记-清除算法, 它的运作过程相对于前面几种收集器来说要更加复杂一些, 整个过程分为以下步骤: 初始标记(STW initial mark) 需要 Stop The World. 这个过程从垃圾回收的 GC Roots 开始, 只扫描能够和 GC Roots 直接关联的对象, 并作标记. 所以这个过程虽然暂停了整个 JVM, 但是很快就完成了. 并发标记(Concurrent marking) 这个阶段紧随初始标记阶段, 在初始标记的基础上继续向下追溯标记. 用一个闭包结构去记录可达对象, 但在这个阶段结束, 这个闭包结构并不能保证包含当前所有的可达对象. 因为用户线程可能会不断的更新引用域, 所以 GC 线程无法保证可达性分析的实时性, 所以这个算法里会跟踪记录这些发生引用更新的地方. 此阶段, 引用程序的线程和并发标记的线程并发执行, 所以用于不会感受到停顿. 并发预清理(Concurrent precleanig) 并发预清理阶段仍是并发的. 这个阶段, 虚拟机查找在执行并发标记阶段新进入老年代的对象(可能会有一些对象从新生代晋升到老年代, 或者有一些对象被分配到老年代). 通过重新扫描, 减少下一个阶段重新标记的工作, 因为下一个阶段会 STW. 重新标记(STW remark) 需要 Stop The World, 时间较初始标记阶段要长一些. 为了修正并发标记期间, 因用于程序继续运作而导致标记产生变动的那一部分对象的标记记录. 并发清除(Concurrent sweeping) 清理垃圾对象, 与应用程序线程并发执行. 并发重置(Concurrent reset) 这个阶段, 重置 CMS 收集器的数据结构, 等待下一次垃圾回收. 参数控制: -XX:+UseConcMarkSweepGC: 使用CMS收集器 -XX:+ UseCMSCompactAtFullCollection: Full GC 后, 进行一次碎片整理; 整理过程是独占的, 会引起停顿时间变长 -XX:+CMSFullGCsBeforeCompaction: 设置进行几次 Full GC 后, 进行一次碎片整理 -XX:ParallelCMSThreads: 设定CMS的线程数量(一般情况约等于可用CPU数量) G1 收集器G1 收集器是 JDK1.7 中正式投入使用的用于取代 CMS 的压缩回收器, 它虽然没有物理上隔断新生代与老年代, 但是仍然属于分代垃圾回收器. G1 GC 仍然会区分新生代与老年代, 新生代依然有 Eden区与 Survivor区. G1 GC 首先将堆分为大小相等的 Region, 避免全区域的垃圾收集, 然后追踪每个 Region 垃圾堆积的价值大小, 在后台维护一个优先列表, 根据允许的手机时间优先回收价值最大的 Region; 同时 G1 GC 采用 Remembered Set 来存放 Region 之间的对象引用以及其他回收器中的新生代与老年代之间的对象引用, 从而避免全堆扫描. G1 内存分布 一次 Young GC 随着 G1 GC的出现, Java 垃圾回收器通过引入 Region 概念, 从传统的连续堆内存布局设计, 逐步走向了物理上不连续但逻辑上依旧连续的内存块. 这样我们能够将某个 Region 动态地分配给 Eden, Survivor, 老年代, 大对象空间和空闲区间等任意一个. 每个 Region 都有一个关联的 Remembered Set(简称 RS), RS 的数据结构是 Hash 表, 里面的数据是 Card Table(堆中每 512byte 映射在 Card Table 1 byte). 简单地说 RS 里面存在的是 Region 中存活对象的指针. 当 Region 中数据发生变化时, 首先反映到 Card Table 中的一个或多个 Card 上, RS 通过扫描内部的 Card Table 得知 Region 中内存使用情况和存活对象. 在使用 Region 过程中, 如果 Region 被填满了, 分配内存的线程会重新选择一个新 Region, 空闲 Region 被组织到一个基于链表的组织结构(Linkedlist)中, 这样可以快速找到新的 Region. 总结 G1 GC 的特性如下: 并行性: G1 在回收期间, 可以有多个 GC 线程同时工作, 有效利用多核计算能力. 并发性: G1 拥有与应用程序交替执行的能力, 部分工作可以和应用程序同时执行, 因此, 一般来说, 不会再整个回收阶段发生阻塞应用程序的情况. 分代 GC: G1 依然是一个分代回收器, 但是和之前的各类回收器不同, 它同时兼顾年轻代和老年代. 对比其他回收器, 或者工作在年轻代, 或者工作在老年代. 空间整理: G1 在回收过程中, 会进行适当的对象移动, 不像 CMS 只是简单地标记清理对象. 在若干次 GC 后, CMS 必须进行一次碎片整理. 而 G1 butong , 它每次回收都会有效地复制对象, 减少空间碎片, 进而提升内部循环速度. 可预见性: 为了缩短停顿时间, G1 建立可预存停顿的模型, 这样在用户设置的停顿时间范围内, G1 会适当选择恰当的区域进行收集, 确保停顿时间不超过用户指定时间. G1 GC 的工作步骤如下所示: 初始标记: 标记一下 GC Roots 能直接关联的对象并修改 TAMS 值, 需要 STW 但耗时很短. 并发标记: 从 GC Roots 从堆中对象进行可达性分析寻找存活的对象, 耗时较长, 但可以与用户线程并发执行. 最终标记: 为了修正并发标记期间产生变动的那一部分标记记录, 这一期间的变化记录在 Remembered Set Log 里, 然后合并到 Remembered Set 里, 该阶段需要 STW 但是可并行执行. 筛选回收: 对各个 Region 回收价值排序, 根据用户期望的 GC 停顿时间制定回收计划来回收. 参数设置: --XX:+UseG1GC 使用 G1 收集器 -XX:MaxGCPauseMillis=n: 设置最大GC停顿时间(GC pause time)指标(target). 这是一个软性指标(soft goal), JVM 会尽量去达成这个目标. -XX:G1HeapRegionSize=n: 此参数指定每个 Region 的大小. 默认值将根据 heap size 算出最优解. 最小值为 1Mb, 最大值为 32Mb. -XX:ConcGCThreads=n: 并发垃圾收集器使用的线程数量. 默认值随 JVM 运行的平台不同而不同. 常用的收集器组合不同垃圾收集器适用的范围. 新生代GC策略 老年代GC策略 说明 1 Serial Serial Old Serial 和 Serial Old 都是单线程进行 GC. 特点就是 GC 时暂停所有应用线程. 2 Serial CMS+Serial Old CMS(Concurrent Mark Sweep)是并发 GC, 实现 GC 线程和应用线程并发工作, 不需要暂停所有应用线程. 另外, 当 CMS 进行 GC 失败时, 会自动使用 Serial Old 策略进行 GC. 3 ParNew CMS 使用 -XX:+UseParNewGC选项来开启. ParNew 是 Serial 的并行版本, 可以指定 GC 线程数, 默认 GC 线程数为 CPU 的数量. 可以使用 -XX:ParallelGCThreads选项指定 GC 的线程数. 如果指定了选项-XX:+UseConcMarkSweepGC` 选项, 则新生代默认使用ParNew GC 策略. 4 ParNew Serial Old 使用 -XX:+UseParNewGC 选项来开启. 新生代使用 ParNew GC 策略, 年老代默认使用 Serial Old 策略. 5 Parallel Scavenge Serial Old Parallel Scavenge 策略主要是关注一个可控的吞吐量: 应用程序运行时间 / (应用程序运行时间 + GC 时间), 可见这会使得 CPU 的利用率尽可能的高, 适用于后台持久运行的应用程序, 而不适用于交互较多的应用程序. 6 Parallel Scavenge Parallel Old Parallel Old 是 Serial Old 的并行版本. 7 G1 G1 -XX:+UnlockExperimentalVMOptions -XX:+UseG1GC 开启-XX:MaxGCPauseMillis =50 暂停时间目标-XX:GCPauseIntervalMillis =200 暂停间隔目标-XX:+G1YoungGenSize=512m 年轻代大小-XX:SurvivorRatio=6 幸存区比例 JVM 常用参数 -XX:PrintFlagsFinal: 显示所有可设置的参数及它们的值. -XX:PrintFlagsInitial: 显示在处理参数之前所有可设置的参数及它们的值, 然后直接退出程序. -Xms=-XX:InitialHeapSize: 初始堆大小, 默认物理内存的 1/64. 空余堆内存小于 40% 时, JVM 就会增大堆直到 -Xmx 的最大限制. -Xmx=-XX:MaxHeapSize: 最大堆大小, 默认物理内存的 1/4. 空余堆内存大于 70% 时, JVM 会减少堆直到 -Xms 的最小限制. -Xss: 每个线程的堆栈大小. -Xmn: 新生代大小. Sun 官方推荐配置为整个堆的 3/8. -XX:MetaspaceSize: 初始元空间大小, 该值越大触发 Metaspace GC 的时机就越晚. -XX:+PrintGCDetails: 输出垃圾回收详情. -XX:SurvivorRatio: Eden 区与 Survivor 区的大小比值. 如果设置为 4, 表示 EdenSpace:FromSpace:ToSpace = 4:1:1. -XX:NewRatio: 新生代与老年代的比值. 如果设置为 4, 表示 新生代:老年代 = 4:1. 如果 Xms=Xmx 且设置了 Xmn 的情况下, 此参数不生效. -XX:MaxTenuringThreshold: 垃圾最大年龄. 最大为 15. 如果设置为 0 的话, 则新生代对象不经过 Survivor 区, 直接进入老年代. 该参数只有在串行 GC 时才有效. -XX:+UseStringDeduplication: 在使用 G1 GC 时进行字符串排重, 将相同数据的字符串指向同一份数据. 参考 jvm-overview: http://www.ityouknow.com/java/2017/03/01/jvm-overview.html JVM总结–JVM体系结构: https://blog.csdn.net/samjustin1/article/details/52215274 重读 JVM: https://juejin.im/post/59ad4cd56fb9a02477075780 深入理解JVM之Java内存管理（基于JAVA8）: https://www.jianshu.com/p/e90b5121ff45 [^1]: 2个幸存区分别是FromSpace和ToSpace. 在 GC 时, 这两个内存区域的逻辑角色会交换.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令]]></title>
    <url>%2FJavaLearning%2FOps%2FGit%E5%91%BD%E4%BB%A4.html</url>
    <content type="text"><![CDATA[初始化初始化一个已经有内容了的目录如果本地已经有了内容, 但是还没有使用 Git 管理, 现在先在 Github 上创建了 Repository. 把项目同步到仓库的连招如下: 123456789101112131415161718192021222324252627# 1 初始化git init# 2 暂存并提交git add .git commit -m "init"# 3 设置远程仓库git remote add &lt;name&gt; &lt;url&gt;# 4 推送到远端git push# 此时发现 ! [rejected] master -&gt; master (fetch first) # 下面的黄色提示我们远程仓库有我们本地没有的工作(默认创建了README.txt), 推荐我们使用 git pull# 5 听话用 git pullgit pull# 此时没有高亮的提示了, 但还是有白字提醒我们当前分支没有跟任何远程分支建立关联# 6 听话建立关联 git branch --set-upstream-to=origin/&lt;branch&gt; master# 7 总该 push 了吧git push# 此时提示 ! [rejected] master -&gt; master (non-fast-forward)# 下面黄色提示我们: 本地分支落后于远程分支, 尝试先 git pull# 由于远程分支创建了 README.txt, 所以如果直接 git pull 会被拒绝, 提示 refusing to merge unrelated histories# 8 上干货git pull --allow-unrelated-histories# 9 接下来就是正常操作了git add .git commit -m "init remote"git push 仓库解除与远程仓库的关联首先获取远程的仓库名, 然后删除 1234# 获取仓库名git remote# 解除关联git remote remove &lt;name&gt; 关联远程仓库1git remote add &lt;name&gt; &lt;url&gt; 移除文件以及取消对文件的跟踪123456# 首先列出操作会取消跟踪的文件列表git rm -r -n --cached &lt;filename&gt;# 真正地取消跟踪git rm -r --cached &lt;filename&gt;# 提交git commit -m &lt;msg&gt; 分支管理合并分支并保留目标分支的提交记录如果合并没有冲突的分支, 想要保存下目标分支的提交记录, 可以禁止使用 Fast-Forward 模式. 1git merge --no-ff &lt;commit-id&gt; 撤销修改撤销修改工作区使用 HEAD 中的最新内容替换工作区中的文件 1234# 单个文件/文件夹git checkout --&lt;filename&gt;# 所有文件/文件夹git checkout . 暂存区使用 &lt;commit-id&gt; 中的内容更新暂存区中的文件 1234567# 单个文件/文件夹git reset &lt;commit-id&gt; &lt;filename&gt;# 所有文件/文件夹git reset &lt;commit-id&gt; .# git reset --mixed HEAD &lt;filename&gt; 的简写git reset filenamegit reset . 回到某个 commit会抹去某个 commit-id 后面的所有 commit. 工作区12# 回到某个 commit-id 版本, 不保留工作区修改.git reset --hard &lt;commit-id&gt; 暂存区12# 更新 HEAD 和 暂存区, 会保留工作区.git reset &lt;commit-id&gt; 查看查看某次 commit 的信息1git show &lt;commmit-id&gt; 修改修改某次 commit 的注释执行该命令后, 会出现交互界面让你修改最近一次尚未 push 的 commit 的注释. 1git commit --amend 知识点^ VS ~^: Git 将 ^ 解析为该引用的父提交. 但加上数字时, HEAD^n 代表 HEAD 的第 n 个父提交. 这个语法只适用于合并(merge)的提交, 因为提交合并会有多个父提交, 所以第一个父提交是你合并时所在的分支, 而第二个父提交是你说合并的分支. 多个 ^ 连用, 如 HEAD^^^ 表示 HEAD 的父提交的父提交的父提交. ~: Git 将 ~ 解析为该引用的第一个父提交. 第一个父提交 = 合并时所在分支的提交. 但加上数字时, 意义与 ^ 不同. HEAD~2 代表 HEAD 的第一个父提交的第一个父提交, 即”祖父提交”. 多个 ~ 连, 如 HEAD~~~ 表示 HEAD 的第一个父提交的第一个父提交的第一个父提交, 与加数字的作用相同. 总结^ 与 ~ 在不加数字时, 含义相同, 均指当前引用的 ^或 ~个数 的第一个父提交. ^n 表示当前引用的第 n 个父提交, 即 parent. ~n 表示当前引用的向前第 n 辈父提交, 即 ancestor. reset 流程初始状态: 移动 HEAD reset 做的第一件事是移动 HEAD 的指向. 这与改变 HEAD 自身不同(checkout 所做的); reset 移动 HEAD 指向的分支. 这意味着如果 HEAD 设置为 master 分支(例如, 你正在 master 分支上), 运行 git reset 9e5e64a 将会使 master 指向 9e5e64a. 它本质上是撤销了上一次 git commit 命令. 当你在运行 git commit 时，Git 会创建一个新的提交, 并移动 HEAD 所指向的分支来使其指向该提交. 当你将它 reset回 HEAD~(HEAD 的父结点)时, 其实就是把该分支移动回原来的位置, 而不会改变索引和工作目录. 现在你可以更新索引并再次运行 git commit 来完成 git commit --amend 所要做的事情了(见 修改最后一次提交). 更新索引(–mixed) reset 这时会用 HEAD 指向的当前快照的内容来更新索引(暂存区). 如果指定 --mixed 选项, reset 将会在这时停止. 这也是默认行为, 所以如果没有指定任何选项(在本例中只是 git reset HEAD~), 这就是命令将会停止的地方. 它依然会撤销一上次 commit，但还会 取消暂存 的所有东西. 于是, 我们回滚到了所有 git add 和 git commit 的命令执行之前. 更新工作目录(–hard) reset 这时会让工作目录看起来像索引. 如果使用 --hard 选项, 它将会执行这一步. 须注意, --hard 标记是 reset 命令唯一的危险用法, 它也是 Git 会真正地销毁数据的仅有的几个操作之一. 其他任何形式的 reset 调用都可以轻松撤消, 但是 --hard 选项不能，因为它强制覆盖了工作目录中的文件. 在这种特殊情况下，我们的 Git 数据库中的一个提交内还留有该文件的 v3 版本，我们可以通过 reflog 来找回它. 但是若该文件还未提交, Git 仍会覆盖它从而导致无法恢复. 总结 reset --soft: 执行步骤 1. 只回退 commit 的信息, 暂存区和工作区没有发生变化, 与回退之前保持一致. 如果要继续提交, 直接 git commit 即可. reset [--mixed]: 执行步骤 1,2, 默认. 将 HEAD 重置到另外一个 commit, 且更新索引(暂存区)和 HEAD 相同. 工作区不会被更改. reset --hard: 执行步骤 1,2,3. 彻底回到指定的 commit-id, 暂存区和工作区都会变为指定 commit-id 版本的内容. Fast-Forward 模式简单的说就是, 当你视图合并两个分支时, 如果顺着一个分支走下去能够到达另一个分支, 那么 Git 在合并两者的时候只会简单地将指针向前推进, 因为这种情况下的合并操作没有需要解决的冲突 – 这就叫做快进(Fast-Forward). 示例提前做的工作为: 12345master &gt; git commit -m "init"master &gt; git commit -m "a"master &gt; git checkout -b secondsecond &gt; git commit -m "b1"second &gt; git commit -m "b2" 现在处于 master 分支的 a commit, 想要合并 second 分支上的 b2 commit. 直接使用 merge second 后, 结果如下: 因为 b2 之于 a 只是领先了 2 个 commit, 即: master 是 second 的直接上游, 所以这种情况下 merge 会默认使用 Fast-Forward 模式, 只是将指针向前移动. 这种情况下, 想要看到 second 在被合并到 master 之前所做的提交就比较困难. 如果想要保留 second 的提交记录, 可以使用 merge --no-ff 参数来禁止 Fast-Forward 模式. 初始状态: 使用 merge --no-ff second 结果如下: master 分支和 second 分支成功地合并了, 并且 second 分支的提交记录也被保留了下来.]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表]]></title>
    <url>%2FJavaLearning%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[链表基础相关知识快慢指针回文问题链表环检测]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>链表</category>
      </categories>
      <tags>
        <tag>链表</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记]]></title>
    <url>%2FJavaLearning%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2FRedis.html</url>
    <content type="text"><![CDATA[Redis简介 Redis 是一个开源(BSD)许可的, 内存中的数据结构存储系统. 它可以用作数据库, 缓存和消息中间件. 它支持多种类型的数据结构, 如字符串(String), 散列(hashes), 列表(lists), 集合(sets), 有序集合(sorted sets)与范围查询, bitmaps, LUA 脚本, LRU 驱动事件, 事务和不同级别的磁盘持久化. 并通过 Redis 哨兵和自动分区提供高可用性. 特点 性能极高 纯内存访问. Redis 将所有数据放在内存中, 内存响应时间大约为 100ns. 单线程. 因为每次的访问时间短, 使用单线程避免了多线程上下文切换的开销. 非阻塞多路 I/O 复用机制. 多个连接的管理在同一进程中. 丰富的数据类型 支持 String, hash, list, set, sorted set 等数据结构的存储. 原子性 Redis 所有的操作命令都是原子性的. 多个操作也支持事务, 通过 MULTI 和 EXEC 指令. 丰富的特性 持久化(主从分区) 可以将内存中的数据保存在磁盘中, 重启的时候可以再次加载进行使用. 发布订阅模式 LUA 脚本支持 序列化支持 数据结构 在线熟悉 Redis 命令使用: http://try.redis.io/ 命令大全: http://www.redis.cn/commands.html Redis keysRedis keys 是二进制安全的, 这意味着可以用任何二进制序列作为 key 值. 从简单的字符串到一个 JPEG 文件的内容都可以, 空字符串也是有效的 key 值. 关于 key 的几条建议: 不建议使用太长的键值. 不仅消耗内存, 而且在数据中查找这类键值的计算成本很高. 键值也不宜太短, 最好保持一定的可读性. 最好坚持一种命名模式, 例如: object-type:id:field 是个不错的主意. 多个单词之间可以用 . 隔开, 如: user:001:home.addr keys [pattern]: 遍历所有 key. dbsize: 返回当前数据库里面的 keys 数量. exists: 检查 key 是否存在. del key: 删除指定的 key-value. expire key seconds: key 在 seconds 秒后过期. ttl key: 查看 key 剩余的存活时间. persist key: 去掉 key 的过期时间. type key: 返回 key 的类型. StringString 的 value 可以是 String 也可以是 数字. 一般做一些复杂 记数功能的缓存. 常用命令: SET key value [EX seconds] [PX milliseconds]: 设置一个 key 的 value 值, 可选择设置超时时间. 1234redis&gt; SET mykey &quot;Hello&quot;OKredis&gt; GET mykey&quot;Hello&quot; SETNX key value(SET if Not eXists): 设置一个 key 的 value 值, 返回 1 设置成功, 返回 0 设置失败. 123456redis&gt; SETNX mykey &quot;Hello&quot;(integer) 1redis&gt; SETNX mykey &quot;World&quot;(integer) 0redis&gt; GET mykey&quot;Hello&quot; SETEX key seconds value: 设置对应 key 的 value 值和超时时间. 123456redis&gt; SETNX mykey &quot;Hello&quot;(integer) 1redis&gt; SETNX mykey &quot;World&quot;(integer) 0redis&gt; GET mykey&quot;Hello&quot; MSET key value [key value …]: 设置多个 key-value 键值对. 123456redis&gt; MSET key1 &quot;Hello&quot; key2 &quot;World&quot;OKredis&gt; GET key1&quot;Hello&quot;redis&gt; GET key2&quot;World&quot; GET key: 返回 key 的 value, 如果不存在, 返回特殊值 nil. 如果 value 不是 String, 返回错误. 123456redis&gt; GET nonexisting(nil)redis&gt; SET mykey &quot;Hello&quot;OKredis&gt; GET mykey&quot;Hello&quot; GETSET key value: 设置 key 的 value 并返回之前旧的 value 值. 如果 key 之前不存在, 返回 nil. 123456redis&gt; INCR mycounter(integer) 1redis&gt; GETSET mycounter &quot;0&quot;&quot;1&quot;redis&gt; GET mycounter&quot;0&quot; APPEND key value: 如果 key 存在, 会把 value 追加到原来 value 值的末尾, 如果不存在, 会首先创建一个空字符串的 key, 再执行追加操作. 123456redis&gt; INCR mycounter(integer) 1redis&gt; GETSET mycounter &quot;0&quot;&quot;1&quot;redis&gt; GET mycounter&quot;0&quot; HashHash 是一个 String 类型的 Field 和 Value 的映射表, Hash 特别适合用来存储对象. HSET key field value: 设置 key 指定的哈希集中指定字段的值. 1234redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HGET myhash field1&quot;Hello&quot; HMSET key field value [field value …]: 设置 key 指定的哈希集中多个指定字段的值. 123456redis&gt; HMSET myhash field1 &quot;Hello&quot; field2 &quot;World&quot;OKredis&gt; HGET myhash field1&quot;Hello&quot;redis&gt; HGET myhash field2&quot;World&quot; HSETNX key field value: 只在 key 指定的哈希集中不存在指定字段时, 设置字段的值. 返回 1 表示成功赋值, 返回 0 表示存在该字段, 没有操作执行. 123456redis&gt; HSETNX myhash field &quot;Hello&quot;(integer) 1redis&gt; HSETNX myhash field &quot;World&quot;(integer) 0redis&gt; HGET myhash field&quot;Hello&quot; HVALS key: 返回 key 指定的哈希集中的所有字段的值. 1234567redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HVALS myhash1) &quot;Hello&quot;2) &quot;World&quot; HGET key field: 返回 key 指定的哈希集中该字段所关联的值. 123456redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HGET myhash field1&quot;foo&quot;redis&gt; HGET myhash field2(nil) HGETALL key: 返回 key 指定的哈希集中所有的字段和值. 每个字段名的下一个是它的值, 所以返回的长度是哈希集大小的两倍. 123456789redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HGETALL myhash1) &quot;field1&quot;2) &quot;Hello&quot;3) &quot;field2&quot;4) &quot;World&quot; HLEN key: 返回 key 指定的哈希集包含的字段的数量. 123456redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HLEN myhash(integer) 2 HEXISTS key field: 返回哈希集中对应的字段是否存在. 123456redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HEXISTS myhash field1(integer) 1redis&gt; HEXISTS myhash field2(integer) 0 HDEL key field [field …]: 从 key 指定的哈希集中移除指定的字段, 不存在的将被忽略. 123456redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HDEL myhash field1(integer) 1redis&gt; HDEL myhash field2(integer) 0 ListRedis List 基于 Linked List 实现, 是个双端链表. 可以非常快的在很大的列表上添加元素, 但如果需要快速访问集合元素, 建议使用(ZSet). List 可以做简单的消息队列的功能. LPOP key: 移除并返回 key 对应的列表的第一个元素. 1234567891011redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; LPOP mylist&quot;one&quot;redis&gt; LRANGE mylist 0 -11) &quot;two&quot;2) &quot;three&quot; LPUSH key value [value …]: 将所有指定的值插入到存于 key 的列表的头部. 如果 key 不存在, 会在操作前创建一个空列表. 1234567redis&gt; LPUSH mylist &quot;world&quot;(integer) 1redis&gt; LPUSH mylist &quot;hello&quot;(integer) 2redis&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;world&quot; LSET key index value: 设置 index 位置的元素值为 value. 1234567891011121314redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; LSET mylist 0 &quot;four&quot;OKredis&gt; LSET mylist -2 &quot;five&quot;OKredis&gt; LRANGE mylist 0 -11) &quot;four&quot;2) &quot;five&quot;3) &quot;three&quot; LRANGE key start stop: 返回存储在 key 的列表里指定范围的元素. 123456789101112131415161718redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; LRANGE mylist 0 01) &quot;one&quot;redis&gt; LRANGE mylist -3 21) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; LRANGE mylist -100 1001) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; LRANGE mylist 5 10(empty list or set) LINDEX key index: 返回存储在 key 里的索引为 index 的元素. 12345678910redis&gt; LPUSH mylist &quot;World&quot;(integer) 1redis&gt; LPUSH mylist &quot;Hello&quot;(integer) 2redis&gt; LINDEX mylist 0&quot;Hello&quot;redis&gt; LINDEX mylist -1&quot;World&quot;redis&gt; LINDEX mylist 3(nil) LLEN key: 返回存储在 key 里的 list 长度. 123456redis&gt; LPUSH mylist &quot;World&quot;(integer) 1redis&gt; LPUSH mylist &quot;Hello&quot;(integer) 2redis&gt; LLEN mylist(integer) 2 SetSet 是 String 类型的无序集合, 集合中的元素时唯一的. Set 通过哈希表实现, 所以添加, 删除, 查找的复杂度都是 O(1). SADD key member [member …]: 添加一个或多个指定的 member 元素到集合 key 中. 已经存在集合 key 中的元素将被忽略. 123456789redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 0redis&gt; SMEMBERS myset1) &quot;World&quot;2) &quot;Hello&quot; SCARD key: 返回集合存储的 key 的基数. 123456redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SCARD myset(integer) 2 SDIFF key [key …]: 返回一个集合与给定集合的差集元素. 123456789101112131415redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SDIFF key1 key21) &quot;a&quot;2) &quot;b&quot; SINTER key [key …]: 返回指定所有集合的成员的交集. 1234567891011121314redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SINTER key1 key21) &quot;c&quot; SMEMBERS key: 返回 key 集合所有的元素. 1234567redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SMEMBERS myset1) &quot;World&quot;2) &quot;Hello&quot; SUNION key [key …]: 返回给定的多个集合的并集中的所有成员. 123456789101112131415161718redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SUNION key1 key21) &quot;a&quot;2) &quot;b&quot;3) &quot;c&quot;4) &quot;d&quot;5) &quot;e&quot; SISMEMBER key member: 返回成员 member 是否是存储的集合 key 的成员. 123456redis&gt; SADD myset &quot;one&quot;(integer) 1redis&gt; SISMEMBER myset &quot;one&quot;(integer) 1redis&gt; SISMEMBER myset &quot;two&quot;(integer) 0 Sorted Set(ZSet)ZSet 和 Set 相比, 增加了一个权重参数 score, 使得集合中的元素能够按照 score 进行有序排列. ZADD key [NX|XX] [CH] [INCR] score member [score member …]: 将所有指定成员添加到键为 key 的有序集合中. 添加时可以指定多个 score/member 对. 123456789101112131415redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 1 &quot;uno&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;(integer) 2redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;1&quot;3) &quot;uno&quot;4) &quot;1&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;three&quot;8) &quot;3&quot; ZCARD key: 返回 key 的有序集元素个数. 123456redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZCARD myzset(integer) 2 ZCOUNT key min max: 返回有序集 key 中, score 值在 min 和 max 闭区间的成员. 12345678910redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZCOUNT myzset -inf +inf(integer) 3redis&gt; ZCOUNT myzset (1 3(integer) 2 ZSCORE key member: 返回有序集 key 中, 成员 member 的 score 值. 1234redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZSCORE myzset &quot;one&quot;&quot;1&quot; ZRANK key member: 返回有序集 key 中成员 member 的排名. 12345678910redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZRANK myzset &quot;three&quot;(integer) 2redis&gt; ZRANK myzset &quot;four&quot;(nil) ZRANGE key start stop [WITHSCORES]: 返回存储在有序集合 key 中指定返回的元素. 123456789101112131415161718&gt; zadd s 1 &quot;one&quot;(integer) 1&gt; zadd s 2 &quot;two&quot;(integer) 1&gt; zadd s 3 &quot;three&quot;(integer) 1&gt; zadd s 4 &quot;four&quot;(integer) 1&gt; ZRANGE s 0 -11) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;4) &quot;four&quot;&gt; ZRANGE s 0 1 WITHSCORES1) &quot;one&quot;2) 1.03) &quot;two&quot;4) 2.0 ZREM key member [member …]: 从 key 中存储的有序集中删除指定的元素成员. 12345678910111213redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZREM myzset &quot;two&quot;(integer) 1redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;1&quot;3) &quot;three&quot;4) &quot;3&quot; Redis 持久化RDB 方式: 保存某个时间点的全量数据快照Redis 默认的方式, 通过以指定的时间间隔执行数据集的时间点快照来将数据持久化到磁盘中. Redis 会单独创建(fork)一个子进程来进行持久化(BGSAVE), 会先将数据写入到一个临时文件中, 待持久化过程都结束了, 再用这个临时文件替换上次持久化的文件. 整个过程中, 主进程是不进行任何 IO 操作的, 这样确保了极高的性能. SAVE: 阻塞 Redis 服务器进程, 直到 RDB 文件被创建完毕. 使用命令: save. BGSAVE: Fork 出一个子进程来创建 RDB 文件, 不阻塞服务器进程. 使用命令: bgsave. Fork 的作用是复制一个与当前进程一样的进程. 新进程的所有数据(变量, 环境变量, 程序计数器等)数值都和原进程一致, 但是是一个全新的进程, 并作为原进程的子进程. 自动触发 RDB 持久化的方式: 根据 redis.conf 配置里的 SAVE m n 定时触发(用的是 BGSAVE). 主从复制时, 主节点自动触发. 执行 Debug Reload. 执行 Shutdown 且没有开启 AOF 持久化. AOF方式: 保存写状态Redis 默认是不使用该方式持久化的. AOF 方式的持久化, 是操作一次 Redis 数据库, 则将操作的记录(不包括查询操作)以 append 的形式追加保存到 AOF 持久化文件中, 以日志的形式来记录每个写操作. 创建 三种同步保存策略: appendfsync always: 同步持久化, 每次发生数据变更就会被立即记录到磁盘. 性能较差但数据完整性高. appendfsync everysec: 异步操作, 每秒记录. 折中的方案. appendfsync no: 从不同步. 恢复 正常恢复 开启 AOF 持久化后, 重新启动 Redis 时会加载当前目录下的 AOF 文件. 异常恢复 如果 Redis 非正常退出, 那么可能会导致写入 AOF 文件中的操作记录不完整. 这种情况下如果启动 Redis, 虽然不会报错, 但是 Redis 并未成功启动. 解决: 首先备份损坏的 AOF 文件. 使用 Redis 安装目录下的 Redis-check-aof --fix &lt;文件名&gt; 来修复文件. 重启 Redis. 日志重写解决 AOF 文件大小不断增大的问题: 调用 fork(), 创建一个子进程. 子进程把新的 AOF 写到一个临时文件里, 不依赖原来的 AOF 文件. 主进程持续将新的变动同时写到内存和原来的 AOF 中. 主进程获取子进程重写 AOF 的完成信号, 往新 AOF 同步增量变动. 使用新的 AOF 文件替换掉旧的 AOF 文件. 两种持久化方式对比RDB 优势 劣势 适合大规模的数据备份和恢复. 非常适用于灾难恢复(disaster recovery). 数据丢失的风险大. 虽然 Redis 允许设置不同的保存点来控制保存 RDB 文件的频率, 但仍可能会丢失最后一次的临时文件中的数据. RDB 是个非常紧凑的文件. 且可以分成多个版本存储. RDB 需要经常 fork 子进程来保存数据集到硬盘. 当数据集比较大的时候, fork 的过程是非常耗时的, 可能会导致 Redis 不能毫秒级地响应客户端请求. RDB 在保存 RDB文件时, 父进程唯一需要做的就是 fork 出一个子进程, 接下来的工作全部由子进程来做. 所以 RDB 持久化方式可以最大化 redis 性能. AOF 优势 劣势 比 RDB 更细粒度地保存策略. 相同数据集的数据, AOF 文件要远大于 RDB 文件, 恢复速度也慢于 RDB. AOF 文件只对日志文件作追加. AOF 运行效率要慢于 RDB, 每秒同步策略效率较好, 不同步效率与 RDB 相同. 可以在 AOF 文件体积过大时, 自动地在后台对 AOF 进行重写(比如 100条 incr 优化为 1条 incrby 100). AOF 文件有序地保存了对数据库执行的所有写入操作, 以 Redis 协议的格式保存. 文件可读性较好. 二者的选择: 如果关心数据, 但仍然可以承受数分钟以内的数据丢失, 可以只使用 RDB 持久. AOF 将 Redis 执行的每一条指令追加到磁盘中, 处理巨大的写入会降低 Redis 的性能. 数据库备份和灾难恢复: 定时生成的 RDB 快照(snapshot)非常便于进行数据库备份, 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快. 不使用 AOF, 仅靠 Master-Slave Replication 也可以实现高可用性. 且能减少大量 IO 的开销, 也减少了 rewrite 时带来的系统波动. 代价是: 如果 Master&amp;Slave 同时倒掉, 会丢失最后一次的数据. Redis 支持同时开启 RDB 和 AOF, 系统重启后, Redis 会优先使用 AOF 来恢复数据, 这样丢失的数据会最少. RDB-AOF 混合持久化方式redis4.0 之后采用 RDB-AOF混合持久化方式. 开启这个功能之后, AOF 重写产生的文件将会同时包含 RDB 格式的内容和 AOF 格式的内容, 其中 RDB 格式的内容用于记录已有的数据, 而 AOF 格式的内容则用于记录最近发生了变化的数据. 开启了混合持久化模式的 AOF 文件加载的流程如下: AOF 文件开头是 RDB 的格式, 先加载 RDB 内容再加载剩余的 AOF. AOF 文件开头不是 RDB 格式, 直接以 AOF 格式加载整个文件. Redis 事务Redis 事务支持一次执行多个命令, 本质是一组命令的集合. 一个事务中的所有命令都会序列化, 按顺序地串行化执行而不会被其他命令插入. 常用命令 DISCARD: 取消事务, 放弃事务块内所有的命令. EXEC: 执行所有事务块内的命令, 并取消 WATCH 命令对所有 key 的监视. MULTI: 标记一个事务块的开始. WATCH key [key …]: 监视一个或多个 key, 如果在事务执行之前 key 被其他命令所改动, 那么事务将被打断. UNWATCH: 取消 WATCH 命令对所有 key 的监视. 使用范式: 12345WATCH aKeyMULTI// options// set ...EXEC 例: 实现加一操作 123456WATCH aKeyval = GET aKeyval = val + 1MULTIset aKey $valEXEC 特性 如果事务块内的语法出错, 那么所有事务都不会被执行. 语法出错, 如: SET key 如果事务块内的指令结果出错, 那么出错的指令执行失败, 其他执行顺利执行. 指令结果出错, 如: SADD key 3(key 不能以数字开头) Redis 发布订阅Redis 的 SUBSCRIBE 命令可以让客户端订阅任意数量的频道, 每当有新信息发送到被订阅的频道时, 信息就会被发送给所有订阅该频道的客户端. 下图展示了频道 channel1, 以及订阅这个频道的三个客户端 - client1, client2, client5 之间的关系: 当新消息通过 PUBLISH 命令发送给频道 channel1 时, 这个消息就会发送给订阅它的三个客户端: 常用命令 PSUBSCRIBE pattern [pattern …]: 订阅一个或多个符合给定模式的频道. PUBSUB subcommand [argument [argument …]]: 查看订阅与发布系统状态. PUBLISH channel message: 将信息发送到指定的频道. PUNSUBSCRIBE [pattern [pattern …]]: 退订所有给定模式的频道. SUBSCRIBE channel [channel …]: 订阅给定的一个或多个频道的信息. UNSUBSCRIBE [channel [channel …]]: 退订给定的频道. 示例: 服务端 1234# 2 服务端发布消息PUBLISH channel2 hello# 4 服务端发布消息PUBLISH goodNews something 客户端 12345678910# 1 客户端订阅频道SUBSCRIBE channel1 channel2 channel3 *News# 3 客户端收到发布的消息1) &quot;message&quot; # 类型2) &quot;channel2&quot; # 频道3) &quot;hello&quot; # 内容# 5 客户端收到发布的消息1) &quot;message&quot; # 类型2) &quot;goodNews&quot; # 频道3) &quot;something&quot; # 内容 Redis 主从复制持久化保证了即使 Redis 服务器重启也不会丢失数据, 因为 Redis 服务重启后会将硬盘上持久化的数据恢复到内存中, 但是当 Redis 服务器的硬盘损坏了, 也可能导致数据丢失. 如果通过 Redis 的主从复制机制, 就可以避免这种单点故障, 如图: 主 Redis 中的数据有两个副本, 即 slave1 和 slave2. 即使一台 Redis 服务器宕机, 其他两台 Redis 服务器仍可以继续提供服务. 主 Redis 中的数据和从 Redis 上的数据保持实时同步, 当主 Redis 写入数据时, 通过主从复制机制会复制到两个从 Redis 服务上. 主从复制不会阻塞 master, 在同步数据时, master 可以继续处理 client 请求. 一个 Redis 可以同时是 master 和 slave. 如图: 主从关系建立关系: 从节点执行 slave of &lt;主机地址&gt; &lt;端口号&gt; . 在配置文件中加入 slave of &lt;主机地址&gt; &lt;端口号&gt;. 启动从节点时, 在 redis-server 后加入参数 --slaveof &lt;主机地址&gt; &lt;端口号&gt;. 解除关系: 从节点执行 slaveof no one. 查看关系: info replication 此外有几点需要注意: 从节点不能进行写入. 如果没有开启哨兵模式, 那么主节点下线后, 从节点会不断尝试重新连接. 主机重新上线后, 从机会连接上主节点. 关系保持不变. 除非在配置文件中写明, 否则从节点每次重新启动都要使用 slaveof 命令与主节点建立联系. 哨兵模式(sentinel)哨兵模式下, 会在后台监控主节点是否故障, 如果主节点故障下线, 那么会根据投票数自动将从节点转换为主节点. 使用 创建 sentinel.conf 文件, 文件名不能为其他. 添加配置: 12345## ip: 主机ip地址## port: 哨兵端口号## master-name: 可以自己命名的主节点名字(只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成).## quorum: 当这些 quorum 个数 sentinel 哨兵认为 master 主节点失联, 那么这时客观上认为主节点失联了.sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 启动哨兵: Redis-sentinel sentinel.conf 如果主节点在下线之后重新上线, 这时主节点会变成从节点. 更加详细配置请参考: https://juejin.im/post/5b7d226a6fb9a01a1e01ff64 复制原理主从复制过程大体可分为 3 个阶段: 建立连接阶段 该阶段的主要作用是在主从节点之间建立连接, 为数据同步做好准备. 保存主节点信息 slaveof 是异步命令, 从节点完成对主节点 ip 和 port 的保存后, 向发送 slaveof 命令的客户端直接返回 OK, 实际的复制操作在这之后才开始. 建立 socket 连接 从节点每秒 1 次调用定时复制函数 replicationCron(), 如果发现了有主节点可以连接, 便会根据主节点的 ip 和 port, 创建 socket 连接. 如果连接成功, 则: 从节点: 为该 socket 建立一个专门处理复制工作的文件处理器, 负责后续的复制工作. 如接收 RDB 文件, 接收命令传播等. 主节点: 接收到从节点的 socket 连接后(即 accept 后), 为该 socket 创建相应的客户端状态, 并将从节点看作是连接到主节点的一个客户端, 后面的步骤会以从节点向主节点发送命令请求的形式来进行. 发送 ping 命令 从节点成为主节点的客户端之后, 发送 ping 命令进行首次请求, 目的是: 检查 socket 连接是否可用, 以及主节点当前是否能够处理请求. 可能会出现三种情况: (1) 返回 pong: 说明 socket 连接正常, 且主节点可以处理请求, 复制过程继续. (2) 超时: 一定时间后从节点仍未收到主节点的回复, 说明 socket 连接不可用, 则从节点断开 socket 连接, 并重连. (3) 返回 pong 以外的结果: 如果主节点返回其他结果, 如正在处理超时运行的脚本, 说明主节点当前无法处理命令, 则从节点断开 socket 连接, 并重连. 身份验证 如果从节点中设置了 masterauth 选项, 则从节点需要向主节点进行身份验证; 没有设置该选项则不需要验证. 从节点通过向主节点发送 auth 命令来进行身份验证. 发送从节点端口信息 身份验证之后, 从节点会向主节点发送其监听的端口号. 主节点将该信息保存到该从节点对应的客户端的 slave_listening_port 字段中. 数据同步阶段 主节点之间的连接建立以后, 便可以开始进行数据同步, 该阶段可以理解为从节点数据的初始化. 具体执行的方式是: 从节点向主节点发送 psync 命令, 开始同步. 数据同步阶段是主从复制最核心的阶段, 根据主从节点当前状态的不同, 可以分为全量复制与增量复制. 需要注意的是, 在数据同步之前, 从节点是主节点的客户端, 主节点不是从节点的客户端. 而到了这一阶段及以后, 主从节点互为客户端. 原因在于: 在此之前, 从节点只需要响应从节点的请求即可, 不需要主动发请求, 而在数据同步阶段和后面的命令传播阶段, 主节点需要主动向从节点发送请求(如推送缓冲区的写命令), 才能完成复制. 命令传播阶段 数据同步阶段完成后, 主从节点进入命令传播阶段. 在这个阶段, 主节点将自己执行的写命令发送给从节点, 从节点接收命令并执行, 从而保证了主从节点数据的一致性. 在命令传播阶段, 除了发送写命令, 主从节点还维持着心跳机制: PING 和 REPLCONF ACK. 延迟与不一致问题 命令传播是异步的过程, 即主节点发送写命令后并不会等待从节点的回复, 因此实际上主从节点之间很难保持实时一致性, 延迟在所难免. 数据不一致的程度与主从节点之间的网络状况, 主节点写命令的执行频率, 以及主节点的 repl-disable-tcp-nodelay 配置等有关. repl-disable-tcp-nodelay no: 该配置作用于命令传播阶段, 控制主节点是否禁止与从节点的 TCP_NODELAY, 默认 no, 即不禁止 TCP_NODELAY. 当设置为 yes 时, TCP 会对包进行合并从而减少带宽, 但是发送的频率会降低, 从节点数据延迟增加, 一致性变差. 具体发送频率与 Linux 的内核配置有关, 默认配置为 40ms. 当设置为 no 时, TCP 会立即将主节点的数据发送给从节点, 带宽增加而延迟变小. 一般来说, 只有当应用对 Redis 数据不一致的容忍度较高, 且主从节点之间网络状况不好时, 才会设置为 yes. 全量复制和增量复制在 Redis2.8 以前, 从节点向主节点发送 sync 命令请求同步数据, 此时的同步方式是全量复制. 在 Redis2.8 以后, 从节点可以发送 psync 命令请求同步数据, 此时根据主从节点当前状态的不同, 同步方式可能是全量复制或增量复制. 全量复制用于初次复制或其他无法进行部分复制的情况, 将主节点中的所有数据都发送给从节点, 是一个重操作. 增量复制用于网络中断等情况后的复制, 只将中断期间主节点执行的写命令发送给从节点, 与全量复制相比更加高效. 需要注意的是: 如果网络中断时间过长, 导致主节点没有能够完整地保存中断期间执行的写命令, 则无法进行增量复制, 仍使用全量复制. 参考 面试中关于Redis的问题看这篇就够了 https://juejin.im/post/5ad6e4066fb9a028d82c4b66 搞懂Redis到底快在哪里 https://zhuanlan.zhihu.com/p/61816273 Redis从入门到实践 https://juejin.im/post/5a912b3f5188257a5c608729 redis的事务和watch https://www.jianshu.com/p/361cb9cd13d5]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java IO]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJava%20IO.html</url>
    <content type="text"><![CDATA[IO简介 IO(Input/Output), 在 Java 中可以分为 BIO, NIO, AIO 三个部分. BIO 是学习 Java 时最先接触到 IO 操作模块, 位于 java.io 包中. NIO 是在 JDK1.4 引入的非阻塞IO. AIO 是 NIO 的升级版, 也称为 NIO.2, 在 JDK1.7 中引入. BIO: 即传统的 java.io 包, 基于流模型实现, 是同步阻塞式 IO. 也就是说在操作输入输出流是, 在读写动作完成之前, 线程会一直阻塞在那, 它们之间的调用是可靠的线性顺序. 这种方式简单易用, 但容易造成系统资源的浪费, 虽然可以用线程池技术进行优化, 但当创建的线程数量过多时, 内存资源会被大量线程消耗殆尽, 且线程之间频繁切换也会造成资源浪费, 所以效率仍然不够高. NIO: 为解决 BIO 资源浪费问题而诞生的一种新的 IO 机制, 是同步非阻塞式 IO. 它提供了 Channel, Selector, Buffer 等核心组件, 用来构建多路复用的、同步非阻塞的 IO 程序, 同时提供了更接近操作系统底层高性能的数据操作方式. AIO: 是 NIO 的升级版 NIO.2, 是一种 异步非阻塞式 IO, 所以常被称为 AIO(Asynchronous IO). 它基于事件和回调机制实现, 也就是说应用操作之后会直接返回, 不会阻塞, 当后台处理完成后, 操作系统会通知相应的线程进行后续操作. 同步与非同步同步和异步关注的是消息通信机制 同步(Synchronous): 在发出一个调用时, 在没有得到结果之前, 该调用就不会返回. 一旦调用返回, 就得到返回值. 调用返回之前, 程序不会往下执行. 异步(Asynchronous): 调用在发出之后, 这个调用直接返回, 没有返回结果. 被调用者通过状态, 通知来通知调用者, 或者通过回调函数处理这个调用. 因为调用直接返回, 程序会继续执行. 阻塞与非阻塞阻塞和非阻塞关注的是程序在等待调用结果(消息, 返回值)时的状态. 阻塞(Blocking): 指调用的结果返回之前, 当前线程会被挂起. 调用线程只有在得到结果之后才会返回. 非阻塞(Nonblocking): 指在不能立刻得到结果之前, 当前线程不会被阻塞. BIO按流分类可分为输入流和输出流: 输入流: 把文件中的数据读取到内存中, 只能进行读操作. 输出流: 将内存中的数据写入到文件中, 只能进行写操作. 按数据分类可分为字节流和字符流: 字节流: 以字节为单位, 每次读写都是 8位 数据(1byte), 可以操作任何类型的数据. 字符流: 以字符为单位, 每次读写都是按照指定编码读取一个字符, 只能操作字符类型数据. 按功能分类可分为节点流和处理流: 节点流: 字节与数据源相连进行读写操作. 处理流: 通过调用节点流类, 从而更加灵活方便地进行读写操作. 类结构分类 字节流输入字节流 InputStream: ByteArrayInputStream, StringBufferInputStream, FileInputStream: 是三种基本的介质流, 分别从 Byte数组, StringBuffer 和本地文件中读取数据. PipeInputStream: 是从与其它线程共用的管道中读取数据. PipedInputStream 的一个实例要和 PipedOutputStream 的一个实例共同使用, 共同完成管道的读取写入操作, 主要用于线程操作. DataInputStream: 将基础数据类型读取出来. ObjectInputStream 和所有 FilterInputStream 的子类都是装饰流. ObjectInputStream 用于对象反序列化. 输出字节流 OutPutStream: ByteArrayOutputStream, FileOutputStream: 是两种介质流, 分别向 Byte数组 和本地文件中写入数据. PipedOutputStream: 向与其他线程共用的管道中读取数据. DataOutputStream: 将基础数据类型写入到文件中. ObjectOutputStream, 和所有 FilterOutputStream 的子类都是装饰流.] 字符流字符输入流 Reader: FileReader, CharReader, StringReader: 是三种基本的介质流, 分别从本地文件, char数组, String 中读取数据. PipedReader: 从与其他线程共用的管道中读取数据. BufferedReader: 具有缓冲功能, 避免频繁读写. InputStreamReader: 字节流通向字符流的桥梁, 使用指定的编码读取直接并解码为字符. 字符输出流 Writer: StringWriter: 向 String 中写入数据. CharArrayWriter: 此类实现一个可用作 Writer 的字符缓冲区, 以字符为单位操作数据. BufferedWriter: 增加缓冲功能, 避免频繁读写. PrintWriter 和 PrintStream: 将对象的格式表示打印到文本输出流. OutputStreamWriter: 字符流通向字节流的桥梁, 使用指定的编码将要写入的字符编码为字节. 操作方式分类 节点流对文件进行操作(节点流): FileInputStream (字节输入流) FileOutputStream (字节输出流) FileReader (字符输入流) FileWriter (字符输出流) 对管道进行操作(节点流): PipedInputStream (字节输入流) PipedOutStream (字节输出流) PipedReader (字符输出流) PipedWriter (字符输出流) PipedInputStream 的一个实例要和 PipedOutputStream 的一个实例共同使用, 共同完成管道的读取写入操作。主要用于线程操作. 字节/字符数组流(节点流): ByteArrayInputStream ByteArrayOutputStream CharArrayReader CharArrayWriter 处理流Buffered 缓冲流(处理流): 带缓冲区的处理流, 缓冲区的主要作用是: 一次性读取多个数据, 避免频繁读写磁盘, 提高数据访问效率. BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter 转化流(处理流): InputStreamReader: 把字节转化成字符 OutputStreamWriter: 把字节转化成字符 基本类型数据流(处理流): 用于操作基本数据类型值. 解决了输出数据类型转换的问题, 可以直接输出 float 或 long 类型, 提高效率. DataInputStream DataOutputStream 打印流(处理流): 一般是打印到控制台, 可进行控制打印的位置. PrintStream PrintWriter 对象流(处理流): 用于对象序列化. ObjectInputStream: 对象反序列化 ObjectOutputStream: 对象序列化 合并流(处理流): SequenceInputStream: 可以认为是一个工具类, 将两个或者多个输入流当成一个输入流依次读取. 分 类 字节输入流 字节输出流 字符输入流 字符输出流 抽象基类 InputStream OutputStream Reader Writer 访问文件 FileInputStream FileOutputStream FileReader FileWriter 访问数组 ByteArrayInputStream ByteArrayOutputStream CharArrayReader CharArrayWriter 访问管道 PipedInputStream PipedOutputStream PipedReader PipedWriter 访问字符串 StringReader StringWriter 缓冲流 BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter 转换流 InputStreamReader OutputStreamWriter 对象流 ObjectInputStream ObjectOutputStream 抽象基类 FilterInputStream FilterOutputStream FilterReader FilterWriter 打印流 PrintStream PrintWriter 推回输入流 PushbackInputStream PushbackReader 特殊流 DataInputStream DataOutputStream 注: 表中粗体字所标出的类代表节点流, 必须直接与指定的物理节点关联: 斜体字标出的类代表抽象基类，无法直接创建实. NIO简介NIO 是 JDK1.4 之后引入的一套 IO 接口, NIO 所有 IO 操作都是非阻塞的. N 可以理解为 Non-blocking 和 New. 核心组件Channel(通道)Java NIO 中所有的 IO 操作都基于 Channel 对象, 就像流操作都要基于 Stream 对象一样. A channel represents an open connection to an entity such as ahardware device, a file, a network socket, or a program component thatis capable of performing one or more distinct I/O operations, forexample reading or writing. 简单来说, 一个 Channel(通道) 代表和某一实体的连接, 这个实体可以是硬件设备, 文件, 网络套接字等. 即通道是 Java NIO 提供的一座桥梁, 用于我们的程序和操作系统底层 IO 服务进行交互. 通道是一种基本的抽象的描述, 执行不同的 IO 操作, 实现各不相同, 常见的实现有: FileChannel: 从文件中读写数据. SocketChannel: 能通过 TCP 读写网络中的数据. ServerSocketChannel: 可以监听新进来的 TCP 连接, 像 Web 服务器那样, 对每一个新进来的连接都会创建一个 SocketChannel. DatagramChannel: 能通过 UDP 读写网络中的数据. 通道与流的不同: 既可以从同道中人读取数据, 又可以写数据到通道. 但流的读写通常是单向的. 通道可以异步地读写. 通道中的数据总是要先读到一个 Buffer, 或者总是要从一个 Buffer 中写入. Channel 使用示例FileChannel1234567891011121314151617181920212223242526public class FileChannelTxt &#123; public static void main(String args[]) throws IOException &#123; //1.创建一个RandomAccessFile（随机访问文件）对象， RandomAccessFile raf=new RandomAccessFile("D:\\niodata.txt", "rw"); //通过RandomAccessFile对象的getChannel()方法。FileChannel是抽象类。 FileChannel inChannel=raf.getChannel(); //2.创建一个读数据缓冲区对象 ByteBuffer buf=ByteBuffer.allocate(48); //3.从通道中读取数据 int bytesRead = inChannel.read(buf); while (bytesRead != -1) &#123; System.out.println("Read " + bytesRead); //Buffer有两种模式，写模式和读模式。在写模式下调用flip()之后，Buffer从写模式变成读模式。 buf.flip(); //如果还有未读内容 while (buf.hasRemaining()) &#123; System.out.print((char) buf.get()); &#125; //清空缓存区 buf.clear(); bytesRead = inChannel.read(buf); &#125; //关闭RandomAccessFile（随机访问文件）对象 raf.close(); &#125;&#125; 开启 FileChannel. 由于 FileChannel 是抽象的, 所以需要通过 InputStream, OutputStream, RandomAccessFile 获取 FileChannel. 通过 FileChannel 读取/写入数据. 构造缓冲区 Buffer 来存放要读取或写入的数据, 然后通过方法 read() 和 write() 来读/写数据. 因为无法保证 write() 方法一次能向 FileChannel 中写入多少字节, 所以需要用 while 循环, 直到 Buffer 中已经没有尚未写入通道的字节. 关闭 FileChannel. FileChannel 中的方法: position(): 获取 FileChannel 的当前位置. 传入参数时可设置 position 的值. size(): 返回该实例所关联文件的大小. truncate(): 截取一个文件, 指定长度后面的部分将被删除. force(): 将通道里尚未写入磁盘的数据强制写到磁盘上, 处于性能方面考虑, 操作系统会将数据缓存在内存中, 所以无法保证写入到 FileChannel 里的数据一定会写到磁盘上, 要保证这一点, 需要调用 force() 方法. 例: channel.fore(true) SocketChannelSocketChannel 用于创建基于 TCP 协议的客户端对象, SocketChannel 中不存在 accept()方法, 通过 connect(), SocketChannel 对象可以连接到其他 TCP 服务器程序. SocketChannel 中的方法: connect(): 如果 SocketChannel 在非阻塞模式下, 此时调用 connect(), 该方法可能在连接建立之前就返回了. 为了确定连接是否建立, 可以调用 finishConnect() 的方法: 123456socketChannel.configureBlocking(false);socketChannel.connect(new InetSocketAddress("http://jenkov.com", 80));while(! socketChannel.finishConnect() )&#123; //wait, or do something else...&#125; write(): 非阻塞模式下, write() 方法在尚未写出任何内容时可能就返回了. 所以需要在循环中调用 write(). read(): 非阻塞模式下, read() 方法在尚未读取到任何数据时就可能返回了, 所以需要关注它的 int 返回值, 会告诉我们读取了多少字节. 客户端: 123456789101112131415161718192021222324252627public class WebClient &#123; public static void main(String[] args) throws IOException &#123; //1.通过SocketChannel的open()方法创建一个SocketChannel对象 SocketChannel socketChannel = SocketChannel.open(); //2.连接到远程服务器（连接此通道的socket） socketChannel.connect(new InetSocketAddress("127.0.0.1", 3333)); // 3.创建写数据缓存区对象 ByteBuffer writeBuffer = ByteBuffer.allocate(128); writeBuffer.put("hello WebServer this is from WebClient".getBytes()); writeBuffer.flip(); //把数据写给服务器 socketChannel.write(writeBuffer); //创建读数据缓存区对象 ByteBuffer readBuffer = ByteBuffer.allocate(128); socketChannel.read(readBuffer); //String 字符串常量，不可变；StringBuffer 字符串变量（线程安全），可变；StringBuilder 字符串变量（非线程安全），可变 StringBuilder stringBuffer=new StringBuilder(); //4.将Buffer从写模式变为可读模式 readBuffer.flip(); while (readBuffer.hasRemaining()) &#123; stringBuffer.append((char) readBuffer.get()); &#125; System.out.println("从服务端接收到的数据："+stringBuffer); socketChannel.close(); &#125;&#125; 通过 SocketChannel 连接到远程服务器. 创建Buffer 对象来存储读写的数据, 并通过 read() 和 write() 方法向服务端接收发送数据. 关闭 SocketChannel. ServerSocketChannelServerSocketChannel 允许我们监听 TCP 连接请求，accept() 方法返回一个新进来的连接的 SocketChannel. ServerSocketChannel 可以设置成非阻塞模式, 在非阻塞模式下, accept() 会立即返回, 如果还没有新进来的链接, 返回的将是 null. 因此, 需要检查返回的 SocketChannel 是否为 null. 服务器端: 1234567891011121314151617181920212223242526272829303132public class WebServer &#123; public static void main(String args[]) throws IOException &#123; try &#123; //1.通过ServerSocketChannel 的open()方法创建一个ServerSocketChannel对象，open方法的作用：打开套接字通道 ServerSocketChannel ssc = ServerSocketChannel.open(); //2.通过ServerSocketChannel绑定ip地址和port(端口号) ssc.socket().bind(new InetSocketAddress("127.0.0.1", 3333)); //通过ServerSocketChannel的accept()方法创建一个SocketChannel对象用户从客户端读/写数据 SocketChannel socketChannel = ssc.accept(); //3.创建写数据的缓存区对象 ByteBuffer writeBuffer = ByteBuffer.allocate(128); writeBuffer.put("hello WebClient this is from WebServer".getBytes()); writeBuffer.flip(); socketChannel.write(writeBuffer); //创建读数据的缓存区对象 ByteBuffer readBuffer = ByteBuffer.allocate(128); //读取缓存区数据 socketChannel.read(readBuffer); StringBuilder stringBuffer=new StringBuilder(); //4.将Buffer从写模式变为可读模式 readBuffer.flip(); while (readBuffer.hasRemaining()) &#123; stringBuffer.append((char) readBuffer.get()); &#125; System.out.println("从客户端接收到的数据："+stringBuffer); socketChannel.close(); ssc.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 通过 ServerSocketChannel.open() 创建一个 ServerSocketChannel 的实例. 通过 ServerSocketChannel 绑定 ip地址 和 端口号. 通过 ServerSocketChannel 的 accept() 方法获取一个 SocketChannel. accept() 方法会监听新进来的连接, 并在返回时包含一个新进来的连接的 SocketChannel, 所以在没有请求时会阻塞. 创建Buffer 对象来存储读写的数据, 并通过 read() 和 write() 方法从客户端接收发送数据. 关闭 SocketChannel 和 ServerSocketChannel(一般不关闭). 通道之间的数据传输在 Java NIO 中, 如果一个 Channel 是 FileChannel 类型的, 那么它可以直接把数据传输到另一个 Channel. transferFrom(): 把数据从通道源传输到 FileChannel. transferTo(): 把 FileChannel 数据传输到另一个 Channel. Buffer(缓冲区)Java NIO 中的 Buffer 用于和 NIO 通道进行交互. 缓冲区本质上是一块可以写入数据, 然后可以从中读取数据的内存. 数据是从通道读入缓冲区, 从缓冲区写入到通道中的. 这块内存被包装成 NIO Buffer 对象, 并提供了一组方法, 用来方便地访问这块内存. 在 Java NIO 中使用的核心缓冲区如下(覆盖了通过 IO 发送的基本数据类型: byte, char, short, int, long, float, double, long): ByteBuffer CharBuffer ShortBuffer IntBuffer FloatBuffer DoubleBuffer LongBuffer 基本用法使用 Buffer 读写数据一般遵循以下四个步骤; 写入数据到 Buffer. 调用 flip() 方法. 从 Buffer 中读取数据. 调用 clear() 或 compact() 方法. 当向 Buffer 写入数据时, Buffer 会记录写下了多少数据. 一旦要读取数据, 需要通过 flip() 方法将 Buffer 从写模式切换到读模式. 在读模式下, 可以读取之前写入到 Buffer 的所有数据. Buffer 对象有三个重要属性: position(指针位置), limit(有效数量), capacity(最大容量). Buffer 类似于一个数组, 只能从中读取索引从 position 到 limit 的数据. Buffer 在初始化后, position=0 ,limit=最大容量, capacity=最大容量. 在向其中写入数据后, position 等于当前指针的索引, limit 不变, capacity 不变. 如果此时读取 Buffer 中的数据, 只能读取当前指针的索引到有效数量的索引的无效数据. flip() 的作用就是将 position 赋值给 limit, 然后置 0, 这样就能获取到有效的数据, 起始索引就由 position 表示, 长度为 limit. 如图: 其中绿色部分表示使用 get() 方法会获得的数据. Buffer 的 capacity, position 和 limit缓冲区本质上是一块可以写入数据, 然后可以从中读取数据的内存. 这块内存被包装成 NIO Buffer 对象, 并提供了一组方法, 用来方便地访问该块内存. 它的三个属性为: capacity 作为一个内存块, Buffer 有一个固定的大小值 - capacity. 只能向里写 capacity 个 byte, long, char 等类型. 一旦 Buffer 满了, 需要将其清空(通过读数据或清除数据), 才能继续往里写数据. position 当写数据到 Buffer 中时, position 表示当前位置. 初始的 position 值为 0. 当一个 byte, long 等数据写到 Buffer 后, position 就会向前移动到下一个可插入数据的 Buffer 单元. position 最大可为 capacity - 1. 当读取数据时, 也是从某个特定位置读. 当前 Buffer 从写模式切换到读模式, position 会被置为 0. 当从 Buffer 的 position 处读取数据时(get()), position 向前移动到下一个可读的位置. limit 在写模式下, Buffer 的 limit 表示你最多能往 Buffer 里写多少数据. 写模式下, limit 等于 Buffer 的 capacity. 当切换 Buffer 到读模式时, limit 表示你最多能读到多少数据. 因此, 当切换 Buffer 到读模式时, limit 会被设置成写模式下的 position 值. 也就是说, 你能读到之前写入的所有数据(limit 被设置成已写数据的数量, 这个值在写模式下就是 position). Buffer 的获取与分配要想获得一个 Buffer 对象, 首先要进行分配. 每个 Buffer 类都有一个静态的 allocate() 方法, 该方法会返回一个参数值指定大小的该类的实例. 1234// 分配一个 48 字节 capacity 的 ByteBufferByteBuffer buf = ByteBuffer.allocate(48);// 分配一个 1024 字节的 CharBufferCharBuffer buf = CharBuffer.allocate(1024); 操作数据写数据: 从 Channel 中写到 Buffer. 1int bytesRead = inChannel.read(buf); 通过 Buffer 的 put() 方法写到 Buffer 里. 1buf.put(123); 读数据: 从 Buffer 读取数据到 Channel. 1int byteWritten = inChannel.write(buf); 使用 get 方法从 Buffer 中读取数据. 1byte aByte = buf.get(); 其他重要方法 flip(): 方法将 Buffer 从写模式切换到读模式. 调用方法会将 position 置 0, 并将 limit 设置成之前的 position 的值. 也就是说, position 现在用于标记读的位置, limit 表示之前写进了多少个 byte, char 等 – 现在能读取多少个 byte, char 等. rewind(): 将 position 置 0, 所以可以重读 Buffer 中的所有数据, limit 保持不变. clear(): 一旦读完 Buffer 中的数据, 需要让 Buffer 准备好再次被写入, 可以通过此方法完成. position 会被置 0, limit 被设置成 capacity 的值. 也就是说, Buffer 中的数据并未清除, 只是这些标记告诉我们可以从哪开始往 Buffer 中写数据. 会保留数据. compact(): 一旦读完 Buffer 中的数据, 需要让 Buffer 准备好再次被写入, 可以通过此方法完成. 将所有未读的数据拷贝到 Buffer 起始处, 然后将 position 设置为最后一个未读元素正后面. limit 设置为 capacity 的值. 会覆盖数据. mark(), reset(): 可以标记 Buffer 中的一个特定 position, 之后可以通过 Buffer.reset() 方法恢复到这个 position. 123buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark. equals(): 可以用来比较两个 Buffer. 只比较 Buffer 中的剩余元素(position 后的元素). 满足下列条件时, 返回真: 有相同的类型(byte, char 等). Buffer 中剩余的 byte, char 等的个数相等. Buffer 中所有的剩余的 byte, char 等都相同. compareTo(): 比较两个 Buffer 的剩余元素, 如果满足下列条件, 则认为前者小于后者: 第一个不相等的元素小于另一个 Buffer 中对应的元素. 所有元素都相等, 但第一个 Buffer 比另一个先耗尽.(前者 Buffer 的 capacity 小于 后者) Scatter 和 GatherChannel 提供了一种被称为 Scatter/Gather 的新功能, 也称为本地矢量 I/O. Scatter/Gather 是指在多个缓冲区上实现一个简单的 I/O 操作. 正确使用 Scatter / Gather可以明显提高性能. 大多数现代操作系统都支持本地矢量 I/O(native vectored I/O)操作. 当您在一个通道上请求一个Scatter/Gather 操作时, 该请求会被翻译为适当的本地调用来直接填充或抽取缓冲区, 减少或避免了缓冲区拷贝和系统调用. Scatter/Gather 应该使用直接的 ByteBuffers 以从本地 I/O 获取最大性能优势. Scatter/Gather 功能是通道(Channel)提供的 并不是Buffer. Scatter: 从一个 Channel 读取的信息分散到 N 个缓冲区中(Buufer). Gather: 将 N 个 Buffer 里面内容按照顺序发送到一个 Channel. Scattering ReadsScattering reads 是把数据从单个 Channel 读取到多个 Buffer 示例代码: 123456ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); read() 方法内部会负责把数据按顺序写进传入的 Buffer 数组内. 一个 Buffer 写满后, 会接着写到下一个 Buffer 中. Gathering WritesGathering Writes 是指数据从多个 Buffer 写入到一个 Channel. 示例代码: 12345678ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = &#123; header, body &#125;;channel.write(bufferArray); write() 方法会按照 Buffer 在数组中的顺序, 将数据依次写入到 Channel 中, 注意只有 position 到 limit 之间的数据才会被写入. Gathering Writes 能较好地处理动态消息. Selector(选择器)Selector 是 Java NIO 中能够检测一到多个 NIO 通道, 并能够知道通道是否为读写操作做好准备的组件. 这样, 一个线程就可以管理多个通道, 从而管理多个网络连接. 为什么要使用 Selector仅用单个线程来处理多个 Channel 的好处是: 只需要更少的线程来处理通道, 事实上, 可以只用一个线程处理所有的通道. 这样对于操作系统来说, 减少了上下文切换的开销, 也减少了创建多个线程带来的资源消耗(如内存). 因此, 使用的线程越少越好. 创建 Selector通过调用 Selector.open() 方法可以创建一个 Selector. 1Selector selector = Selector.open() 向 Selector 注册通道为了将 Channel 和 Selector 配合使用, 必须将 Channel 注册到 Selector 上. 通过 SelectableChannel.register() 方法来实现. 123// 设置通道非阻塞channel.configureBlocking(false);SelectionKey key = channel.register(selector,SelectionKey.OP_READ); 与 Selector 一起使用时, Channel 必须处于非阻塞模式下. 这意味着不能将 FileChannel 与 Selector 一起使用, 因为 FileChannel 不能切换到非阻塞模式, 而套接字通道可以. SelectableChannel 抽象类的 configureBlocking() 方法是由 AbstractSelectableChannel 抽象类实现的, SocketChannel, ServerSocketChannel, DatagramChannel 都是直接继承了 AbstractSelectableChannel 抽象类. register() 方法的第二个参数, 是一个 interest集合, 意思是在通过 Selector 监听 Channel 时, 对什么事件感兴趣. 可以监听的类型有: Connnect, Accept, Read, Write. 通道触发了一个事件也就意味着通道已为该事件准备就绪. 所以, 某个 Channel 成功连接到另一个服务器称之为”连接就绪”; 一个 ServerSocketChannel 准备好接收新进入的连接称之为”接收接续”; 一个通道有数据可读/可写称之为”读/写就绪”. 这四个事件用 SelectionKey 的常量表示为: SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果对不止一种事件感兴趣, 那么可以用”位或”操作符将常量连接起来 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; SelectionKey一个 SelectionKey 键表示了一个特定的通道对象和一个特定的选择器对象之间的注册关系. 有如下方法: attachment(): 返回 SelectionKey 的 attachment. channel(): 返回该 SelectionKey 对应的 Channel. selector(): 返回该 SelectionKey 对应的 Selector. interestOps(): 返回代表需要 Selector 监控的 IO 操作的 bit mask. readyOps(): 返回一个 bit mask, 代表相应 Channel 上可进行的 IO 操作. 当向 Selector 注册 Channel 后, register() 方法会返回一个 SelectionKey 对象, 这个对象包含了一些你感兴趣的属性: interest set ready set Channel Selector attachment 附加的对象(可选) interest set interest 集合是你所选择的感兴趣的事件集合, 可以通过 SelectionKey 读写 interest 集合, 如: 12345int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet) &amp; SelectionKey.OP_ACCEPT == SelectionKey.OP_ACCEPT;boolean isInterestedInConnect = (interestSet) &amp; SelectionKey.OP_CONNECT == SelectionKey.OP_CONNECT;boolean isInterestedInRead = (interestSet) &amp; SelectionKey.OP_READ == SelectionKey.OP_READ;boolean isInterestedInWrite = (interestSet) &amp; SelectionKey.OP_WRITE == SelectionKey.OP_WRITE; 用”位与”操作 interest 集合和给定的 SelectionKey 常量, 可以确定某个事件是否在 interest 集合中. ready set ready 集合是通道已经准备就绪的操作的集合, 在一次选择(Selection)之后, 你会首先访问这个 ready set, 如: 1int readySet = selectionKey.readyOps(); 可以用像检测 interest 集合那样, 来检测 Channel 中什么事件或操作已经就绪, 也可以使用以下方法, 它们都会返回一个布尔类型: selectionKey.isAcceptable() selectionKey.isConnectable() selectionkey.isReadable() selectionKey.isWritable() Channel + Selector 从 SelectionKey 访问 Channel 和 Selector 十分简单, 如: 12Channel channel = selectionKey.channel;Selector selector = selectionKey.selector(); 附加的对象 可以将一个对象或更多信息附着到 SelectionKey 上, 这样就能方便地识别某个给定的通道. 例如, 可以附加与通道一起使用的 Buffer, 或是包含聚集数据的某个对象: 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用 register() 方法向 Selector 注册 Channel 时附加对象: 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过 Selector 选择通道一旦向 Selector 注册了一个或多个通道, 就可以调用几个重载的 select() 方法, 这些方法返回对你所感兴趣的事件已经准备就绪的那些通道. 例如, 如果你对”读就绪”的通道感兴趣, select() 方法会返回读事件准备就绪的那些通道. 有以下方法可以使用: int select(): 阻塞到至少一个通道在你注册的事件上就绪. 返回值表示有多少通道已经就绪, 即, 自上次调用 select() 后, 有多少通道编程就绪状态. 如果调用 select() 方法, 因为有一个通道变成就绪状态, 返回了 1, 若再次调用 select() 方法, 如果另一个通道就绪了, 它会再次返回 1. 如果对第一个就绪的 Channel 没有任何操作, 现在就有两个就绪的通道, 但在每次 select() 方法调用之间, 只有一个通道就绪了. int select(long timeout): 在 select() 基础上增加超时等待. int selectNow(): 不会阻塞, 不管什么通道就绪, 都立刻返回(此方法执行非阻塞的选择操作, 如果指从前一次选择操作后, 没有通道变成可选择的, 则此方法直接返回 0). 一旦调用 select() 方法, 返回值不为 0 时, 则可以通过调用 Selector 的 selectKeys() 方法来访问已选择键集合(selected key set)中的就绪通道: 1Set selectedKeys = selector.selectedKeys(); 可以遍历这个已选择的键集合来访问就绪的通道: 123456789101112131415Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; 这个循环遍历已选择键集中的每个键, 并检测各个键对应的通道的就绪事件. 注意每次迭代末尾的 keyIterator.remove() 调用, Selector 不会自己从已选择键集中移除 SelectionKey 实例, 必须在处理完通道时自己移除. 下次该通道变成就绪时, Selector 会再次将其放入已选择键集中. 否则会重复. SelectionKey.channel() 方法返回的通道需要转型成你需要处理的类型, 如 ServerSocketChannel, SocketChannel 等. wakeUp() 某个线程调用 select() 方法后阻塞了, 即使没有通道已经就绪, 也有办法让其从 select() 方法返回. 只要让其他线程在第一个线程调用 select() 方法的那个对象上调用 wakeUp() 方法即可, 阻塞在 select() 方法上的线程会立即返回. 如果有其它线程调用了 wakeUp() 方法, 但当前 Selector 没有阻塞在 select() 方法上, 那么本次 wakeUp() 调用会在下一次select() 方法阻塞的时候生效. 参见: http://jm.taobao.org/2010/10/22/380/ close() 用完 Selector 后调用其 close() 方法会关闭该 Selector, 且使注册到该 Selector 上的所有 SelectionKey 实例无效, 通道本身并不会关闭. 模板代码服务器端 1234567891011121314151617181920212223242526272829303132ServerSocketChannel ssc = ServerSocketChannel.open();ssc.socket().bind(new InetSocketAddress("localhost", 8080));ssc.configureBlocking(false);Selector selector = Selector.open();ssc.register(selector, SelectionKey.OP_ACCEPT);while(true) &#123; int readyNum = selector.select(); if (readyNum == 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = selectedKeys.iterator(); while(it.hasNext()) &#123; SelectionKey key = it.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; it.remove(); &#125;&#125; 代码示例服务器端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class NioServer &#123;public static void main(String[] args) throws IOException &#123; // 创建一个selector Selector selector = Selector.open(); // 初始化TCP连接监听通道 ServerSocketChannel listenChannel = ServerSocketChannel.open(); listenChannel.bind(new InetSocketAddress("127.0.0.1",9999)); listenChannel.configureBlocking(false); // 注册到selector（监听其ACCEPT事件） listenChannel.register(selector, SelectionKey.OP_ACCEPT); // 创建缓冲区 ByteBuffer buffer = ByteBuffer.allocate(1024); while (true) &#123; selector.select(); //阻塞，直到有监听的事件发生 Iterator&lt;SelectionKey&gt; keyIter = selector.selectedKeys().iterator(); // 通过迭代器依次访问select出来的Channel事件 while (keyIter.hasNext()) &#123; SelectionKey key = keyIter.next(); if (key.isAcceptable()) &#123; // 有连接可以接受 SocketChannel channel = ((ServerSocketChannel) key.channel()).accept(); channel.configureBlocking(false); channel.register(selector, SelectionKey.OP_READ); System.out.println("与【" + channel.getRemoteAddress() + "】建立了连接！"); &#125; else if (key.isReadable()) &#123; // 有数据可以读取 buffer.clear(); // 读取到流末尾说明TCP连接已断开， // 因此需要关闭通道或者取消监听READ事件 // 否则会无限循环 if (((SocketChannel) key.channel()).read(buffer) == -1) &#123; key.channel().close(); continue; &#125; // 按字节遍历数据 buffer.flip(); while (buffer.hasRemaining()) &#123; byte b = buffer.get(); if (b == 0) &#123; // 客户端消息末尾的\0 System.out.println(); // 响应客户端 buffer.clear(); buffer.put("Hello, Client!\0".getBytes()); buffer.flip(); while (buffer.hasRemaining()) &#123; ((SocketChannel) key.channel()).write(buffer); &#125; &#125; else &#123; System.out.print((char) b); &#125; &#125; &#125; // 已经处理的事件一定要手动移除 keyIter.remove(); &#125; &#125;&#125;&#125; 客户端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Client &#123; public static void main(String[] args) throws Exception &#123; SocketChannel socketChannel = SocketChannel.open(); //设置为非阻塞 socketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress("127.0.0.1", 9999)); Selector selector = Selector.open(); // 注册连接服务器socket的动作 socketChannel.register(selector, SelectionKey.OP_CONNECT); ByteBuffer writeBuffer = ByteBuffer.allocate(32); ByteBuffer readBuffer = ByteBuffer.allocate(32); while (true) &#123; //选择一组键，其相应的通道已为 I/O 操作准备就绪。 //此方法执行处于阻塞模式的选择操作。 selector.select(); //返回此选择器的已选择键集。 Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); System.out.println("keys=" + keys.size()); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); // 判断此通道上是否正在进行连接操作。 if (key.isConnectable()) &#123; socketChannel.finishConnect(); socketChannel.register(selector, SelectionKey.OP_WRITE); System.out.println("server connected..."); break; &#125; else if (key.isWritable()) &#123; //写数据 System.out.print("please input message:"); String message = scanner.nextLine(); //ByteBuffer writeBuffer = ByteBuffer.wrap(message.getBytes()); writeBuffer.clear(); writeBuffer.put(message.getBytes()); //将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位 writeBuffer.flip(); socketChannel.write(writeBuffer); //注册写操作,每个chanel只能注册一个操作，最后注册的一个生效 //如果你对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来 //int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; //使用interest集合 socketChannel.register(selector, SelectionKey.OP_READ); socketChannel.register(selector, SelectionKey.OP_WRITE); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable())&#123;//读取数据 System.out.print("receive message:"); SocketChannel client = (SocketChannel) key.channel(); //将缓冲区清空以备下次读取 readBuffer.clear(); int num = client.read(readBuffer); System.out.println(new String(readBuffer.array(),0, num)); //注册读操作，下一次读取 socketChannel.register(selector, SelectionKey.OP_WRITE); &#125; keyIterator.remove(); &#125; &#125; &#125;&#125; AIOAIO 也就是 NIO.2. 它是异步非阻塞的 IO 模型. 异步 IO 是基于事件和回调机制实现的. 在 NIO 的基础上引入了新的异步通道的概念, 并提供了异步文件通道和异步套接字通道的实现. AIO 并没有采用 NIO 的多路复用器, 而是使用异步通道的概念. read() 和 write() 方法的返回类型都是 Future. 而 Future 模型都是异步的, 核心思想就是: 去主函数等待时间. 为什么 Netty 使用 NIO 而不使用 AIO?以下是作者原话: Not faster than NIO (epoll) on unix systems (which is true) There is no daragram suppport Unnecessary threading model (too much abstraction without usage) 参考 关于Java IO与NIO知识都在这里 https://juejin.im/post/5af79bcc51882542ad771546 Java核心（五）深入理解BIO、NIO、AIO https://juejin.im/post/5c048e0ff265da613a53c572 java IO体系的学习总结 https://blog.csdn.net/nightcurtis/article/details/51324105 一文让你彻底理解 Java NIO 核心组件 https://segmentfault.com/a/1190000017040893 并发编程网 NIO 系列教程 http://ifeve.com]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加密算法]]></title>
    <url>%2FJavaLearning%2FOps%2F%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[为了保证业务的安全, 就需要对权限和信息提供安全保障, 数字签名和信息加密是提供这类保障的核心技术. 不同业务需求会使用到不同的签名加密算法, 有时也会结合多种不同的签名加密算法实现目标. 数字签名数字签名, 即通过提供 可鉴别 的 数字信息 来验证 自身身份 的一种方式. 一套数字签名通常定义两种互补的运算, 一个用于 签名, 另一个用于 验证. 分别由 发送方 持有能够代表自己身份的 私钥, 由 接收方 持有与私钥对应的 公钥, 用于验证发送者发送的信息是否被篡改过. 发送方使用 摘要算法[^1] 提取出源文件的摘要后, 用私钥对摘要进行加密, 接收方接收到数据后, 用与发送方私钥对应的公钥对加密过的摘要进行解密, 并使用摘要算法获得文件内容的摘要, 最后将解密后的摘要和计算出的摘要进行比对, 如果一致则说明内容没有被篡改. 到此为止, 数字签名提供了较强的安全性保障, 但唯一的风险就是: 接收方无法确定自己使用的公钥就是发送发提供的[^2]. 数字证书可以完美解决这个问题. 数字证书(Certificate Authority)数字证书(CA)是一个由权威机构颁发的一种认可凭证. 这个机构用非对称加密产生一对密钥(公钥和私钥), 用私钥对公钥进行签名, 生成数字证书, 证书包含明文和密文两个部分, 明文中包含了: 签名证书的机构名, 颁发给谁, 有效期, 序列号等信息. 然后使用哈希算法对这些信息进行计算, 得到一个 hash 值, 再用私钥对这个 hash值进行加密, 这就得到了 CA 的数字签名. 将这两部分放入到数字证书的文件中, 就是一个数字证书. 接收方通过使用公钥解密密文来验证数字证书的正确性. 解密得到 hash 值后, 使用相同的哈希算法对明文数据进行计算, 将计算结果与解密得到的相比对, 如果一致则说明证书是正确的. 接收方就可以使用这个公钥对数据进行解密验证. 数字证书的公钥是公开的, 可以通过签发这个证书的机构获取其公钥. 即, 权威的证书颁发机构来保证公钥的正确性. 这些机构的证书被各个厂商设置为 可信任的根证书, 这些根证书可以去证明其他的证书. 如果一个根证书失效, 结果将是灾难性的. 签名最根本的用途就是 验证发送方的身份, 防止 中间人攻击, CSRF跨域身份伪造. 基于这一点, 在诸如设备认证, 用户认证, 第三方认证等认证体系中, 都会用到签名算法. 加密和解密数据加密是将原来为 明文 的数据内容按照某种 算法 进行处理, 使其成为 不可读 的数据, 通常称为 密文. 以达到 保护数据安全 的目的. 加密 的逆过程称为 解密. 即将经过处理的数据还原为原来数据的过程. 加密分为 对称加密 和 非对称加密 两类. 对称加密对称加密算法 是应用较早的加密算法, 在对称加密算法中, 使用的密钥只有 1 个. 发送方和接收方都使用这个密钥来对数据进行加密和解密. 这要求双方都必须知道 加密的密钥. 原理加密和解密数据使用同一个密钥, 适合对大量数据进行加密解密. 安全性关键是密钥的保存方式, 任何一方泄露密钥, 都会导致信息泄露. 代表算法DES 3DES Blowfish RC4 RC5 AES 非对称加密非对称加密算法 需要两个密钥, 一个称为 公开密钥(public key), 另一个称为 私有密钥(private key). 因为加密和解密的过程中使用的是两个不同的密钥, 所以这种算法称为非对称加密. 原理使用两个密钥: 公开密钥(public key), 私有密钥(private key). 使用其中一个密钥作为加密密钥后, 只能用相对应的另一个密钥才能解密. 密钥长度: 通常是 1024, 2048 位. 密钥长度增长一倍, 公钥操作所需时间增加约 4 倍. 私钥操作所需时间增加约 8 倍, 公钥生成时间增加约 16 倍. 加密的明文长度: 加密的明文长度不能超过 RSA 密钥的长度减去 11byte. 如密钥长度为 1024, 则明文长度不能超过 117byte(1024/8 - 11 = 117byte). 如果长度超过该值, 将会抛出异常. 加密后的密文长度为密钥长度. 如密钥长度为 1024bit, 最后生成的密文长度 固定为 1024bit. 安全性公钥可以被任何人知道, 但是私钥泄露就会导致信息泄露. 代表算法RSA 哈希散列算法哈希散列算法 不能称作加密算法, 只是将明文的数据通过计算得出对应的一个值, 使数据 不可读, 且无法通过值来逆推出明文 原理采用某种散列函数, 输入不同长度的明文, 得出固定长度的密文. 明文的微小变化都能引起密文的巨大变化. 安全性不能通过密文反推明文, 通常作为数据完整性校验. 应用场景生成信息摘要, 验证信息完整性. 不使用明文存储密码, 使用通过算法处理后的值来存储密码和验证密码. 代表算法MD5 SHA-1 SHA-2(SHA-224, SHA-256, SHA-384, SHA-512) 常见算法MD5MD5 用的是 哈希函数, 典型应用是对数据产生信息摘要, 以验证数据是否完整. MD5 是一种摘要算法, 对于任何的输入, 都会输出长度为 128bit 的值(通常用 16进制 表示为 32个字符). SHA-1SHA-1 是和 MD5 一样流行的 消息摘要算法. 然而 SHA-1 比 MD5 的安全性更强. 对于长度小于 2^64bit 的消息, SHA-1 会产生一个 160bit 的消息摘要. 基于 MD5, SHA-1 的消息摘要特性以及 不可逆, 可以被应用在检查文件完整性和数字签名等场景. HMACHMAC 是密钥相关的 哈希运算消息认证码(Hash-based Message Authentication Code). HMAC 运算利用 哈希算法(MD5, SHA-1等), 以 一个密钥 和 一个消息 为输入, 生成一个 消息摘要 作为输出. 发送方和接受方持有一个公共密钥, 一方发送 消息和数据摘要, 另一方使用 HMAC 用 接收的数据和公共密钥 作为输入, 将得到的数据摘要与接收的相比对, 如果匹配则说明数据在传输过程中未经篡改. AES/DES/3DES三者都是对称的块加密算法, 加解密的过程是 可逆的. 常用的有 AES128, AES192, AES256. DESDES 加密算法是一种分组密码[^3]. 每块数据的长度为 64bit, 使用密钥来自定义变换过程, 因此算法认为只有持有加密所用的密钥的用户才能解密密文. 密钥表面上是 64位 的, 然后只有其中的 56位 被实际用于算法, 其余 8位 可以用于奇偶校验, 并在算法中被丢弃. 因此 DES的有效密钥长度仅为56位. 对于 56位 长度的密钥来说, 如果使用穷举法来进行搜索, 运算次数为 2^56 次. 3DES是基于 DES 的对称算法, 对 一块数据 用 三个不同的密钥 进行 三次加密, 强度更高. AESAES 加密算法是密码学中的 高级加密标准. 该加密算法采用 对称分组密码体制, 密钥长度可以为 128位, 192位, 256位. 同样是将明文分块, 每块的长度只能是 128位, 每次加密一组数据, 直到加密完整个明文. 密钥的长度不同, 加密的轮数也不同. AES 本身就是为了取代 DES 的, AES 具有更好的安全性, 效率和灵活性. RSARSA 加密算法是目前最有影响力的 公钥加密算法, 并且被普遍认为是目前最优秀的公钥方案之一. RSA 是第一个能同时用于 加密 和 数字签名 的算法, 它能够抵抗到目前为止已知的所有密码攻击, 已被 ISO 推荐位公钥数据加密标准. RSA加密算法 基于一个十分简单的数论事实: 将两个大 素数 相乘十分容易, 但是想要对其进乘积进行因式分解却及其苦难, 因此可以将 乘积 公开作为 加密密钥. 对称加密算法 名称 密钥名称 运行速度 安全性 资源消耗 DES 56位 较快 低 中 3DES 112位或168位 慢 中 高 AES 128、192、256位 快 高 低 非对称加密算法 名称 成熟度 安全性 运算速度 资源消耗 RSA 高 高 中 中 ECC 高 高 慢 高 哈希散列算法 名称 安全性 速度 SHA-1 高 慢 MD5 中 快 参考 浅谈常见的七种加密算法及实现: https://juejin.im/post/5b48b0d7e51d4519962ea383 互联网安全之数字签名、数字证书与PKI系统: https://juejin.im/post/584b753b2f301e00572314dc 公钥私钥 [^1]:摘要算法指通过散列算法计算得到唯一的固定长度的哈希值, 无法逆向还原出源数据.[^2]:公钥和私钥都由发送方生成, 发送方通过把公钥提供给接收方, 使其能够对数据进行校验.[^3]:一种将固定长度的明文通过一系列复杂的操作变成同样长度的密文算法.]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 多线程]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B.html</url>
    <content type="text"><![CDATA[重要概念异步和同步同步和异步通常用来形容一次方法的调用, 同步方法调用一旦开始, 调用者必须在调用返回后才能继续后续的行为. 异步方法调用更像一个消息传递, 一旦开始, 方法调用就会立即返回, 调用者可以继续后续的工作. 并发和并行它们都可以表示两个或以上任务一起执行, 但并发偏重多个任务交替执行, 而并行是真正意义上的”同时执行”. 多线程在单核 CPU 中是交替执行(并发), 在多核 CPU 中, 因为每个 CPU 有自己的运算器, 所以在多个 CPU 中可以同时运行(并行). 高并发高并发互联网分布式系统架构设计中必须考虑的因素之一, 通常指设计保证系统能够同时并行处理大量请求. 高并发常用的一些指标有: 响应时间(Response Time), 吞吐量(Throughput), 每秒查询率(QPS), 并发用户数等. 临界区临界区用来表示一种公共资源或者说是共享数据, 可以被多个线程使用. 但是每一次, 只能有一个线程使用它, 一旦临界区资源被占用, 其他线程想要使用这个资源, 就必须等待. 在并行程序中, 临界区资源是保护的对象. 阻塞和非阻塞非阻塞指在不能立刻得到结果之前, 该函数不会阻塞当前进程, 而会立刻返回, 而阻塞与之相反. 多线程使用的常见三种方式 继承 Thread 类, 重写父类 run() 方法 12345678910111213public class thread1 extends Thread &#123; public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; System.out.println("我是线程"+this.getId()); &#125; &#125; public static void main(String[] args) &#123; thread1 th1 = new thread1(); thread1 th2 = new thread1(); th1.start(); th2.start(); &#125;&#125; 实现 Runnable 接口 12345678910111213141516171819202122public class thread2 implements Runnable &#123; public String ThreadName; public thread2(String tName)&#123; ThreadName = tName; &#125; public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; System.out.println(ThreadName); &#125; &#125; public static void main(String[] args) &#123; // 创建一个Runnable接口实现类的对象 thread2 th1 = new thread2("线程A:"); thread2 th2 = new thread2("线程B:"); // 将此对象作为形参传递给Thread类的构造器中，创建Thread类的对象，此对象即为一个线程 Thread myth1 = new Thread(th1); Thread myth2 = new Thread(th2); // 调用start()方法，启动线程并执行run()方法 myth1.start(); myth2.start(); &#125;&#125; 通过 Callable 和 Future 创建线程 123456789101112131415161718192021222324252627282930313233import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask; public class CallableThreadTest implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception&#123; int i = 0; for(;i&lt;100;i++)&#123; System.out.println(Thread.currentThread().getName()+" "+i); &#125; return i; &#125; public static void main(String[] args)&#123; CallableThreadTest ctt = new CallableThreadTest(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt); for(int i = 0;i &lt; 100;i++)&#123; System.out.println(Thread.currentThread().getName()+" 的循环变量i的值"+i); if(i==20)&#123; new Thread(ft,"有返回值的线程").start(); &#125; &#125; try&#123; System.out.println("子线程的返回值："+ft.get()); &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; catch (ExecutionException e)&#123; e.printStackTrace(); &#125; &#125;&#125; Runnable 和 Callable 的对比 Callable 规定重写 call(), Runnable 规定重写 run(). Callable 的任务执行后可返回值, Runnable 的任务没有返回值. call() 方法可以抛出异常, run() 方法不可以 运行 Callable 任务可以得到一个 Future 对象, 表示异步计算的结果, 提供了检查计算是否完成的方法, 以等待计算的完成, 并检查计算的结果. 通过 Future 对象还可以了解任务的执行情况, 可以取消任务的执行, 还可以获取执行结果. 线程生命周期 线程状态 新建状态: 新建线程对象, 没有调用 start() 之前. 就绪状态: 调用 start() 方法之后线程就进入就绪状态, 但是并不是说只要调用 start() 方法, 线程就马上变为当前线程, 在变为当前线程之前都是为就绪状态. 运行状态: 线程被设置为当前线程, 获得 CPU 后, 开始执行 run() 方法, 即线程进入运行状态. 等待状态: 处于等待状态的线程不会被分配 CPU, 它们要等待被显式地唤醒，否则会无限期等待. 超时等待: 处于超时等待状态的线程不会被分配 CPU, 它们无需显式唤醒, 在等待指定的时间后就自动唤醒. 阻塞状态: 处于运行状态的线程, 会因为系统对资源的调度而被中断进入阻塞状态. 死亡状态: 处于运行状态的线程, 当它主动或者被动结束后, 线程就处于死亡状态. 线程控制 join(): 等待. 阻塞调用此方法时所在的线程并释放锁, 让调用该方法的线程完成才继续执行. sleep(): 睡眠. 让当前的正在执行的线程暂停指定的时间, 并进入阻塞状态. yield(): 线程让步. 将线程从运行状态转换为就绪状态. 当某个线程调用 yield() 方法从运行状态转换到就绪状态后, CPU 会从就绪状态线程队列中只选择与该线程优先级相同, 或优先级更高的线程执行. setPriority(): 改变线程的优先级. 参数 priorityLevel 范围在 1-10 之间. 常用的静态常量值有: MAX_PRIORITY = 10, MIN_PRIORITY = 1, NORM_PRIORITY = 5. 具有较高优先级仅表示此线程具有更多的执行机会, 而并非会优先执行. 优先级还有继承性. setDaemon(true): 设置为守护线程. 守护进程主要为其他线程提供服务. JVM 中的垃圾回收线程, 但所有的前台线程都进入死亡状态时, 守护线程会自动死亡. 必须在 start() 之前执行, 否则会抛出 IllegalThreadStateEcxeption. 在守护线程中产生的新线程也是守护线程. 不是所有的任务都可以分配给守护线程来执行, 如: 读写操作或计算逻辑. interrupt(): 中断线程, 但不是真正意义上的中断, 而是给线程一个通知信号, 会改变这个线程内部维护的一个中断标识位, 而不会改变线程本身的状态(如阻塞, 终止等). 在线程因调用 sleep() 而处于 TIMED_WAITING 状态时, 调用 interrupt() 会抛出 InterruptedException, 从而使线程提前结束 TIMED_WAITING 状态. interrupted(): 检查当前线程是否中断, 并隐式将中断状态重置为 false. isInterrupted(): 判断线程是否被中断, 可以用来在 run() 中优雅地终止线程. 实例变量和线程安全 线程类中实例变量针对其他线程可以有共享和不共享之分 123456MyThread a = new MyThread("A");MyThread b = new MyThread("B");MyThread c = new MyThread("C");a.start();b.start();c.start(); 以上这种情况, 线程的变量不会共享 12345678SharedVariableThread mythread = new SharedVariableThread();// 下列线程都是通过mythread对象创建的Thread a = new Thread(mythread, "A");Thread b = new Thread(mythread, "B");Thread c = new Thread(mythread, "C");a.start();b.start();c.start(); 而这种情况下, a, b, c 线程共享 mythread 线程的变量 多线程分类 用户进程: 运行在前台, 执行具体的任务. 如: 程序的主线程, 连接网络的子线程. 守护线程: 运行在后台, 为其他前台线程服务. 如: 垃圾回收线程 特点: 一旦所有用户线程都结束运行, 守护形成会随 JVM 一起结束工作. 应用: 数据库连接池中的检测线程, JVM 虚拟机启动后的检测线程 synchronized 关键字 Java 并发编程领域中, synchronized 关键字一直都是元老级别的角色. 在 JDK1.6 之前, synchronized 的性能比 ReenTrantLock 差很多. 而在 JDK1.6 之后, JVM 团队对 synchronized 关键字做了很多优化, 主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁. synchronized 和 ReenTrantLock 的性能基本是持平了. synchronized 可以保证方法或者代码块在运行时, 同一时刻只有一个方法可以进入到临界区, 同时它还可以保证共享变量的内存可见性. synchronized 主要有一下三个作用: 保证互斥性, 保证可见性, 保证顺序性. 线程安全非线程安全 问题存在于 实例变量中, 如果是方法内部私有成员变量, 则不存在 非线程安全 问题. 如果连个线程同时操作对象中的实例变量, 则会出现 非线程安全, 解决办法就是在方法前加上 synchronized 关键字即可. 使用场景 注意: 类锁和对象锁之间不互斥. 即一个线程可以同时获得类锁和对象锁. synchronized 锁重入可重入锁的含义是: 自己可以再次获取自己内部的锁. 比如一个线程获得了某个对象的锁, 此时这个对象还没有释放, 当其再次想要获取这个对象的锁时, 还是可以获取的, 如果不可重入的话, 就会造成死锁. 可重入锁也支持在父子类继承的环境当中. 对象头 在 HotSpot 虚拟机中, 对象在内存中存储的布局可以分为 3 块区域: 对象头(Header), 实例数据(Instance Data) 和对其填充(Padding). HotSpot 虚拟机的对象头包括两部分信息, 第一部分用于存储对象自身的运行时数据, 如哈希码(HashCode), GC 分代年龄, 锁状态标志, 线程持有的锁, 偏向线程 ID, 偏向时间戳等, 这部分数据的长度在 32 位和 64 位的虚拟机中(未开启压缩指针)分别为 32bit 和 64bit, 官方称它为 Mark Word. 对象头的另外一部分是类型指针, 即对象指向它的类元数据的指针, 虚拟机通过这个指针来确定这个对象是哪个类的实例. 并不是所有的虚拟机实现都必须在对象数据上保留类型指针. 另外, 如果对象是一个 Java 数组, 那在对象头中还必须有一块用于记录数组长度的数据, 因为虚拟机可以通过普通 Java 对象的元数据信息确定 Java 对象的大小, 但是从数组的元数据中无法确定数组的大小. 实例数据部分是对象真正存储的有效信息, 也是在程序代码中所定义的各种类型的字段内容. 无论是从父类继承下来的, 还是在子类中定义的, 都需要记录起来. 对齐填充并不是必然存在的, 也没有特别的含义, 它仅仅起着占位符的作用. 是由于 HotSpot VM 的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍. synchronized 用的锁是存在 Java 对象头中的. 如果对象是数组类型, 则虚拟机用 3 个字宽(Word)存储对象头, 如果对象是非数组类型, 则用 2 个字宽存储对象头. 长度 内容 说明 32/64bit Mark Word 存储对象的 hashcode 或 锁信息 32/64bit Class Metadata Address 存储到对象类型数据的指针 32/32bit Array length 数组的长度(如果当前对象是数组) 32位虚拟机对象头的存储结构 64位虚拟机对象头的存储结构 对象监视器(monitor) 每个对象都有自己的监视器, 当这个对象由同步块或者这个对象的同步方法调用时, 执行方法的线程必须先获取该对象的监视器才能进入同步块和同步方法. 没有获取到监视器的进程将被阻塞在同步块和同步方法的入口处, 进入 BLOCKED 状态. 任意线程对 Object 的访问, 首先要获得 Object 的监视器, 如果获取失败, 该线程就要进入同步队列, 线程变为 BLOCKED, 当 Object 的监视器占有者释放后, 在同步队列中的线程就有机会重新获取该监视器. Monitor 是线程私有的数据结构, 每个线程有一个可用的 monitor record 列表, 同时还有一个全局的可用列表. 每一个被锁住的对象都会和一个 monitor 关联(对象头的 Mark Word 中的 Lock Word 指向 Monitor 的起始地址), 同时 Monitor 中有一个 owner 字段存放拥有该锁的线程的唯一标识, 标识该锁被这个线程占用, 结构如下: Owner: 初始时为 Null, 表示当前没有任何线程拥有该 Monitor Record, 当线程成功拥有该锁后, 保存线程唯一表示, 当锁被释放时, 又设置为 Null. EntryQ: 关联一个系统互斥锁, 阻塞所有视图锁住 Monitor Record 失败的线程. RcThis: 表示 BLOCKED 或 waiting 在该 Monitor Record 上的所有线程的个数. Nest: 用来实现重入锁的计数. HashCode: 保存从对象头拷贝过来的 HashCode 值 Cadidate: 用来避免不必要的阻塞或等待线程唤醒, 因为每一次只有一个线程能够成功拥有锁, 如果每次前一个释放锁的线程唤醒所有阻塞或等待的线程, 会引起不必要的上下文切换(从阻塞到就绪然后因为竞争锁失败又被阻塞)从而导致的性能下降. Candidate 只有两种可能的值: 0 表示没有需要唤醒的线程, 1 表示要唤醒的一个继任线程来竞争锁. 内存模型硬件内存架构 多 CPU: 一个现代计算机通常有两个或多个 CPU. 其中一些 CPU 还有多个核心. 这就意味着如果 Java 程序是多线程的, 那么程序中几个线程就可能并发执行. CPU 寄存器: 每个 CPU 都包含一些列的寄存器, 他们是 CPU 内内存的基础. CPU 在寄存器上执行的速度远大于在主存上执行的速度. 高速缓存 cache: 由于计算机的存储设备与处理器的运算速度之间有几个数量级的差距, 所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存与处理器之间的缓冲: 将运算需要使用到的数据复制到缓存中, 让运算能快速进行, 当运算结束后再从缓存同步回内存之中, 这样处理器就无需等待缓慢的内存读写了. 缓存可以分为一级缓存(L1), 二级缓存(L2), 三级缓存(L3), 当 CPU 要读取一个数据时, 首先从一级缓存中查找, 如果没有找到再从二级缓存中查找, 以此类推. CPU 的每一个核心都含有一套 L1(可能还有 L2) 缓存, 而所有核心共享 L3(或者和 L2) 缓存. 在某一时刻, 一个或者多个缓存行可能被读到缓存, 一个或者多个缓存行可能再被刷新回主存. 运作原理: 当一个 CPU 需要读取主存时, 它会将主存的部分读到 CPU 缓存中. 它甚至可能将缓存中的部分内容读到它的内部寄存器中, 然后再寄存器中执行操作. 当 CPU 需要将结果写回主存中去时, 它会将内部寄存器的值刷新到缓存中, 然后在某个时间点将值刷新回主存. 缓存一致性问题(CacheCoherence)在多处理器系统中, 每个处理器都有自己的高速缓存, 而它们又共享同一主存. 基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾, 但是也引入新的问题: 缓存一致性问题. 当多个处理器的运算任务都涉及同一块主内存区域时, 将可能导致各自的缓存数据不一致的情况, 为解决一致性问题, 需要各个处理器访问缓存时遵循一些协议, 在读写时根据协议来进行操作. 指令重排序问题巍峨了使得处理器内部的运算单元能尽量被充分利用, 处理器可能会对输入代码进行乱序执行优化, 处理器会在计算之后将乱序执行的结果重组, 保证该结果与顺序执行的结果是一致的, 但并不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致. 因此, 如果存在一个计算任务依赖另一个计算任务的中间结果, 那么其顺序性并不能靠代码的先后顺序来保证. 与处理器的乱序执行优化类似, Java 虚拟机的即时编译器中也有类似的指令重排序优化. 对于处理器来说, 一条 CPU 指令是按流水线技术来执行的, 可以被分为多个步骤. 为了使处理器内部的运算单元能够尽量的被充分利用, 处理器可能会对指令进行重排序, 例如 什么是内存模型前面的缓存一致性问题, 处理器优化的指令重排问题是硬件的不断升级导致的, 这些技术虽然提高了运行效率, 但是也导致了多线程模式下的并发问题. 为了保证并发编程中共享内存的正确性(可见性, 有序性, 原子性), 内存模型定义了共享内存系统中多线程程序读写操作行为的规则, 来规范对内存的读写操作, 从而保证指令执行的正确性. 这主要通过采用两种方式: 限制处理器优化和使用内存屏障. Java 内存模型和硬件内存架构之间的桥接Java 内存模型与硬件内存架构之间存在差异, 硬件内存架构没有区分线程栈和对. 对于硬件, 所有的线程和堆都分布在主内存中. 部分线程栈和堆可能有时候会出现在 CPU 缓存中和 CPU 内部的寄存器中. Java 内存模型(JMM)Java 内存模型本身是一种抽象的概念, 并不真实存在. 它描述的是一组规则或规范, 通过这组规范定义了程序中各个变量(包括实例字段, 静态字段和构成数组对象的元素)的访问方式. 屏蔽了各种硬件和操作系统的访问差异, 保证了 Java 程序在各种平台下对内存的访问都能保证效果一致. 由于 JVM 运行程序的实体是线程, 而每个线程创建时 JVM 都会为其创建一个工作内存, 用于存储线程私有的数据, 而 Java 内存模型中规定所有变量都存储在主内存, 主内存是共享内存区域, 所有线程都可以访问, 但线程对变量的操作(读取赋值等)必须在工作内存中进行, 首先要将变量从主内存中拷贝到自己的工作内存中, 然后对变量进行操作, 操作完成后再将变量写回主内存. 不能直接操作主内存中的变量, 工作内存中存储着主内存中的变量副本拷贝. 工作内存是每个线程的私有数据区域, 因此不同的线程间无法访问对方的工作内存, 线程间的通信必须通过主内存来完成, 如下图 主内存 主要存储的是 Java 对象实例, 所有线程创建的实例对象都存放在主内存中, 不管该对象是成员变量还是方法中的本地变量, 当然也包括了共享的类信息, 常量, 静态变量. 由于是共享数据区域, 多个线程对同一个变量进行访问可能会导致线程安全问题. 工作内存 主要存储当前方法的所有本地变量信息, 工作内存中也存储着主内存中的变量副本拷贝. 每个线程只能访问自己的工作内存, 即线程中的本地变量是对其他线程不可见的, 就算是两个线程执行的是同一段代码, 它们也会各自在自己的工作内存中创建属于当前线程的本地变量, 当然也包括了字节码行号指示器, 相关 Native 方法的信息. 根据虚拟机规范, 对于一个实例对象中的成员方法而言, 如果方法中包含本地变量是基本数据类型, 将直接存储在工作内存的栈帧结构中, 但倘若本地变量是引用类型, 那么该变量的引用会存储在工作内存的栈帧中, 而对象实例将存储在主内存中. 但对于实例对象的成员变量, 不管它是基本数据类型或者包装类型还是引用类型, 都会被存储到堆. 至于 static 变量以及类本身相关信息将会存储在主内存中. 需要注意的是, 在主内存中的实例对象可以被多线程共享, 如果多个线程同时调用了同一个对象的同一个方法, 那么这些线程会将要操作的数据拷贝一份到自己的工作内存中, 执行完成操作后才刷新到主内存中. 即, JMM 是一种规范, 目的是解决由于多线程通过共享内存进行通信时, 存储在工作内存中数据不一致, 编译器对代码指令重排序, 处理器会对代码乱序执行等带来的问题. 原子性原子性指的是一个操作是不可中断的, 即使在多线程环境下, 一个操作一旦开始就不会被其他线程影响. 对于 32 位系统来说, long 类型 和 double 类型的数据, 它们的读写并非是原子性的, 因为对于 32 位的虚拟机来说, 每次原子读写是 32 位的, 而 long 和 double 是 64 位的存储单元. 有序性有序性就是说 Java 内存模型中的指令重排不会影响单线程的执行顺序, 但是会影响多线程并发执行的正确性. 在多线程环境中, 一个线程中观察另一个线程, 所有操作都是无序的(指令重排现象和线程工作内存与主内存同步延迟现象). 可见性可见性指的是当一个线程修改了某个共享变量的值, 其他线程是否能够立刻感知这个值的改动. 由于线程对共享变量的操作都是线程从主存中拷贝到自己的工作内存进行操作然后才写回主存, 这就可能导致多个线程各自在自己的工作内存中修改了共享变量的值, 在没有写回主存时, 彼此之间的修改并不可见, 即造成了可见性问题. 另外由于指令重排和编译器优化也可能导致可见性问题. happen-beforehappen-before 关系, 是 Java 内存模型中保证多线程操作可见性的机制, 也是对早期语言规范中含糊的可见性概念的一个精确定义. 它的具体表现形式, 包括但远不止我们自觉中的 synchronized, volatile, lock 操作顺序等方面, 例如: 线程内执行的每个操作, 都保证 happen-before 后面的操作, 这就保证了基本的程序顺序规则, 这是开发者在书写程序时的基本约定. 对于 volatile 变量, 对它的写操作, 保证 happen-before 在随后对该变量的读取操作. 对于一个锁的解锁操作, 保证 happen-before 加锁操作. 对象构建完成, 保证 happen-before 于 finalizer 的开始动作. 类似线程内部操作的完成, 保证 happen-before 其它 Thread.join() 的线程等. happen-before 关系存在着传递性, a happen-before b, b happen-before c, 那么 a happen-before c. happen-before 不仅仅时对执行时间的保证, 也包括对内存读, 写操作顺序的保证, 但不能保证线程交互的可见性. volatile 关键字在 JDK1.2 之前, Java 的内存模型实现总是从主存(共享内存)中读取变量, 是不需要进行特别注意的. 而现在的 Java 内存模型下, 线程可以把变量保存到 本地内存(机器的寄存器)中, 而不是直接在主存中进行读写. 这可能导致一个线程在主存中修改了一个变量的值, 而另外一个线程继续使用它在寄存器中的变量值的拷贝, 造成数据不一致. JVM 在空闲时会尽力保证变量值的更新, 把主存中的数据读取到工作内存中. 所以没有加同步关键字的变量被修改后, 在工作内存中的值是不确定的. 相对于 synchronized 块的代码锁, volatile 提供了一个轻量级的针对共享变量的锁.当在多个线程间使用共享变量进行通信的时候, 需要考虑将共享变量用 volatile 修饰, 告知 JVM 这个变量是不稳定的, 每次都要到主存中进行读取, 避免数据不一致的问题出现. 可见性volatile 修饰的成员变量在每次被线程访问时, 都强迫从主存中重读改成员变量的值, 而且当成员变量发生变化时, 强迫线程将变化值回写到主存. 这样在任何时刻, 不同线程看到的成员变量都是同一个值, 保证了同步数据的可见性. 禁止重排序volatile 通过内存屏障来禁止指令重排序. volatile 重排序规则表: 当第二个操作是 volatile写 时, 不管第一个操作是什么, 都不能重排序. 这个规则确保 volatile写 之前的操作不会被编译器重排序到 volatile写 之后. 当第一个操作是 volatile读 时, 不管第二个操作是什么, 都不能重排序. 这个规则确保 volatile读 之后的操作不会被编译器重排序到 volatile读 之前. 当第一个操作是 volatile写, 第二个操作是 volatile读 时, 不能重排序. 原子性?volatile 无法同时保证内存可见性和原子性, 加锁机制(同步机制)既可以确保可见性, 又可以确保原子性. volatile 只能确保可见性. 原因是: 声明为 volatile 的简单变量如果当前值与该变量以前的值相关, 那么 volatile 关键字不起作用, 比如: count++, count = count + 1, 都不是原子操作. 适合使用 volatile 的情况: 对变量的写入操作不依赖变量的当前值, 或者能确保只有单个线程更新变量的值. 该变量没有包含在具有其他变量的不变式中. 如: 1234567public volatile boolean flag;while(!flag)&#123; // do something...&#125;public setFlag()&#123; flag = false;&#125; 不适用示例: 12345public volatile int count;// ...run() &#123; count++; // 不能保证原子性&#125; 只是需要解决变量在多个线程之间的可见性. 资源的同步性应交由 synchronized 关键字. 等待/通知(wait/notify)机制等待通知机制: 指一个线程 A 调用了对象 O 的 wait() 方法进入该对象的等待池中, 而另一个线程 B 调用了对象 O 的 notify()/notifyAll() 方法, 线程 A 收到通知后退出等待池而进入该对象的锁池中, 等待锁竞争. 相关方法 方法名 描述 notify() 随机唤醒等待池中的 一个线程, 并让该线程进入该对象的锁池, 等待锁竞争. notifyAll() 使该对象等待池中所有线程进入锁池. wait() 使调用该方法的对象所在的线程释放共享资源锁, 然后进入等待池. wait(long) 超时等待一段时间, 单位是毫秒. 等待时间结束后, 自动唤醒. wait(long,int) 更精确的等待时间控制, 精确到纳秒. 注意: notify() 不会释放锁, 在方法执行完 synchronize 代码块后才会释放. wait() 会立即释放锁. 只能在 同步方法或同步块 中调用 wait()/notify()/notifyAll() 方法. 方法调用之前必须先获得该对象的对象级别锁, 如果调用时没有持有适当的锁, 则抛出 IllegalMonitorStateException 异常 当线程处于 wait 状态时, 对线程对象调用 interrupt 方法会出现 InterruptedException 异常. sleep() 和 wait() 区别sleep() 方法是线程类(Thread)的静态方法, 让此线程暂停执行指定时间, 将执行机会让给其他线程, 但是监控状态依然保持, 到时后会自动恢复到就绪状态. 调用 sleep 不会释放对象锁. wait() 方法是 Object 类的方法, 对此对象调用 wait() 方法导致本线程放弃对象锁, 进入该对象的等待池, 只有针对此对象发出的 notify/notifAll 方法后, 线程才会进入对象锁池准备获得对象锁进入就绪状态. ThreadLocalJDK 中提供 ThreadLocal 类来解决多个线程使用同一个 public static 变量的问题. ThreadLocal 让每个线程绑定自己的值. 也就是说它让每个线程可以存储自己的数据, 存放在堆中. 重要: ThreadLocal 中的废弃项目的回收依赖于显式地触发, 否则就要等待到线程结束. 所以应用一定要自己负责 remove, 且不要和线程池配合, 因为 worker 线程往往是不会退出的. 堆内存中的对象可以被所有线程访问, 但是 ThreadLocal 通过一些技巧将可见性修改成了线程可见. 方法名 描述 get() 返回此线程局部变量的当前线程副本中的值 initialValue() 返回此线程局部变量的当前线程的初始值, 便于子类重写. 不重写为 Null. 在线程第一次调用 get 或 set 时执行. remove() 删除此线程局部变量的当前线程的值 set(T value) 将此线程局部变量的当前线程副本的值设置为指定的值 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test3 &#123; public static void main(String[] args) &#123; try &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(" 在Main线程中取值=" + Tools.tl.get()); Thread.sleep(100); &#125; Thread.sleep(5000); ThreadA a = new ThreadA(); a.start(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; static public class Tools &#123; public static ThreadLocalExt tl = new ThreadLocalExt(); &#125; static public class ThreadLocalExt extends ThreadLocal &#123; @Override protected Object initialValue() &#123; return new Date().getTime(); &#125; &#125; static public class ThreadA extends Thread &#123; @Override public void run() &#123; try &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println("在ThreadA线程中取值=" + Tools.tl.get()); Thread.sleep(100); &#125; &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 结果如图 可以看出: 不同线程从 ThreadLocal 中获取的值不相同. InheritableThreadLocalThreadLocal 类固然好, 但是子线程无法取到父线程 ThreadLocal 类的变量, 从而有了 InheritableThreadLocal 来解决这个问题. 修改上面代码中的 Tools 类和 ThreadLocalExt 类: 123456789static public class Tools &#123; public static InheritableThreadLocalExt tl = new InheritableThreadLocalExt(); &#125; static public class InheritableThreadLocalExt extends InheritableThreadLocal &#123; @Override protected Object initialValue() &#123; return new Date().getTime(); &#125; &#125; 结果如上图所示. 同时还可以重写 childValue(Object parentValue) 来获取父类的变量值, 定制子类的变量值. 只是需要注意: 如果子线程在取得值的同时, 主线程将 InheritableThreadLocal 中的值进行更改, 那么子线程取到的还是旧值. Lock 接口Lock 接口简介锁是用于多个线程控制对共享资源的访问的工具, 通常, 锁提供对共享资源的独占访问: 一次只能有一个线程可以获取锁, 并且对共享资源的所有访问要求首先获取锁. 但是一些锁可能允许并发访问共享资源, 如 ReadWriteLock 的读写锁. JDK1.5 之后, 并发包中新增了 Lock 接口以及相关实现类来实现锁功能. Lock 接口的实现类: ReentrantLock, ReentrantReadWriteLock.ReadLock, ReentrantReadWriteLock.WriteLock 简单使用123456789Lock lock=new ReentrantLock()；lock.lock();try&#123; // do something // 如果有return要写在try块中&#125;finally&#123; // 保证锁最终能被释放 lock.unlock();&#125; 特性和方法 synchronized Lock 存在层次 Java 关键字 接口 锁的释放 获取到锁的线程执行完同步代码, 释放锁线程执行发生异常, JVM 使线程释放锁 在 finally 中必须释放锁, 不然容易造成线程死锁 锁的获取 假设A线程获得锁, B线程等待. 如果A线程阻塞, B线程会一直等待 尝试非阻塞地获取锁[^1]能被中断地获取锁[^2]超时获取锁[^3] 锁状态 无法判断 可以判断有没有成功获取锁 锁类型 可重入 不可中断 非公平 可重入 可中断 公平/非公平 [^1]:当前线程尝试获取锁, 如果这一时刻没有被其他线程获取到, 则成功获取并持有锁.[^2]:获取到锁的线程能够响应中断, 当获取到锁的线程被中断时, 中断异常将会被抛出, 同时锁会被释放.[^3]:在指定的截止时间之前获取锁, 超过截止时间仍旧无法获取则返回. 基本方法 方法名 描述 void lock() 获得锁. 如果锁不可用, 则当前线程将被禁用以进行线程调度, 并处于休眠状态, 直到获取锁. void lockInterruptibly() 获取锁. 如果可用并立即返回. 如果锁不可用, 那么当前线程将被禁用以进行线程调度, 并且处于休眠状态. 在锁的获取中可以中断当前线程. Condition newCondition() 获取等待通知组件, 该组件和当前的锁绑定, 当前线程只有获得了锁, 才能调用该组件的 wait() 方法. 而调用后, 当前线程将释放锁. boolean tryLock() 只有在调用时才可以获得锁, 如果可用, 则获取锁, 并返回值 true. 如果不可用, 返回值 false. boolean tryLock(long time, TimeUnit unit) 超时获取锁, 当前线程在以下三种情况下会返回:1. 当前线程在超时时间内获得了锁.2. 当前线程在超时时间内被中断.3. 超时时间结束, 返回 false. void unlock() 释放锁. ReentrantLock构造方法: 方法名称 描述 ReentrantLock() 创建一个 ReentrantLock 的实例. ReentrantLock(boolean fair) 创建一个特定锁类型(公平锁/非公平锁)的 ReentrantLock 的实例. 默认是非公平锁. 成员方法: 方法名 描述 int getHoldCount() 查询当前线程保持此锁定的个数, 也就是调用lock() 方法的次数. protected Thread getOwner() 返回当前拥有此锁的线程, 如果不拥有, 则返回 null protected Collection getQueuedThreads() 返回包含可能正在等待获取此锁的线程的集合 int getQueueLength() 返回等待获取此锁的线程数的估计. protected Collection getWaitingThreads(Condition condition) 返回包含可能在与此锁相关联的给定条件下等待的线程的集合. int getWaitQueueLength(Condition condition) 返回与此锁相关联的给定条件等待的线程数的估计. boolean hasQueuedThread(Thread thread) 查询给定线程是否等待获取此锁. boolean hasQueuedThreads() 查询是否有线程正在等待获取此锁. boolean hasWaiters(Condition condition) 查询任何线程是否等待与此锁相关联的给定条件 boolean isFair() 如果此锁的公平设置为 true, 则返回 true. boolean isHeldByCurrentThread() 查询此锁是否由当前线程持有. boolean isLocked() 查询此锁是否由任何线程持有. ConditionCondition 是 Java 提供来实现等待/通知的类, Condition 类还提供比 wait/notify 更丰富的功能. Condition 对象是由 lock 对象所创建的, 但是同一个锁可以创建多个 Condition 的对象, 即创建多个对象监视器, 这样的好处是: 可以指定唤醒的线程. Condition 将 Object 监视器方法(wait, notify, notifyAll)分解成截然不同的对象, 以便通过将这些对象与任意 Lock 实现组合使用, 为每个对象提供多个 等待 set(wait-set). 其中, Lock 替代了 synchronized 方法和语句的使用, Condition 替代了 Object 监视器方法的使用. 要创建一个 Lock 的 Condition, 必须使用 newCondition() 方法. 成员方法: 方法名称 描述 void await() 相当于 Object 类的 wait 方法 boolean await(long time, TimeUnit unit) 相当于 Object 类的 wait(long timeout) 方法 signal() 相当于 Object 类的 notify 方法 signalAll() 相当于 Object 类的 notifyAll 方法 调用 signal() 和 signalAll() 方法, 会在执行完方法当前所在的 try 语句块后才释放锁. 注意: 必须在 condition.await() 方法调用之前调用 lock.lock() 代码获得同步监视器, 否则会报错. 公平锁与非公平锁Lock 锁分为: 公平锁和非公平锁. 公平锁: 表示线程获取锁的顺序是按照线程加锁的顺序来分配的. 即先来先得的FIFO. 非公平锁: 一种获取锁的强占机制, 是随机获取锁,. ReentrantReadWriteLock之前的 ReentrantLock(排他锁)具有完全互斥排他的效果, 即同一时刻只允许一个线程访问, 虽然保证了实例变量的线程安全性, 但是效率较低. ReentranReadWriteLock 读写锁能够解决这个问题. 读写锁维护了两个锁, 一个是读操作相关的锁, 也称为共享锁; 一个是写操作相关的锁, 也称为排他锁. 通过分离读锁和写锁, 其并发性比一般排他锁性能高. 多个读锁之间不互斥, 读锁与写锁互斥, 写锁与写锁互斥.(只要出现写操作的过程就是互斥的) 特性与方法 特性 说明 公平性选择 支持非公平(默认)和公平的锁获取方式, 吞吐量上来看还是非公平优于公平 重进入 该锁支持重进入, 以读写线程为例: 读线程在获取了读锁之后, 能够再次获取读锁. 而写线程在获取了写锁之后能够再次获取写锁也能够同时获取读锁 锁降级 遵循获取写锁, 获取读锁再释放写锁的次序, 写锁能够降级成为读锁 方法名 描述 ReentrantReadWriteLock() 创建一个 ReentrantReadWriteLock() 的实例 ReentrantReadWriteLock(boolean fair) 创建一个特定锁类型(公平锁/非公平锁)的ReentrantReadWriteLock 的实例 分别使用 lock.readLock().lock() 和 lock.writeLock().lock() 获取锁. 锁的状态JDK1.6 为了减少获得锁和释放锁所带来的性能消耗, 引入了”偏向锁”和”轻量级锁”. 所以锁一共有 4 种状态: 无锁状态, 偏向锁状态, 轻量级锁状态和重量级锁状态, 它会随着竞争情况逐渐升级. 锁可以升级但不能降级, 意味着偏向锁升级成轻量级锁后不能降级成偏向锁. 这种只升不降的策略, 目的是为了提高获得锁和释放锁的效率. 偏向锁在没有实际竞争的情况下, 还能针对部分场景继续优化. 如果不仅仅没有实际竞争, 自始至终, 使用锁的线程都只有一个, 那么, 维护轻量级锁都是浪费的. 偏向锁的目标是: 减少无竞争且只有一个线程使用锁的情况下, 使用轻量级锁产生的性能消耗. 轻量级锁每次申请, 释放锁都至少需要一次 CAS, 但偏向锁只有初始化时需要一次 CAS. “偏向”的意思是, 偏向锁假定将来只有第一个申请锁的线程会使用锁(不会有任何线程再来申请锁), 因此, 只需要在 Mark Word 中 CAS 记录 owner(本质上也是更新, 但初始值为空). 如果记录成功, 则偏向锁获取成功, 记录锁状态为偏向锁, 以后当前线程等于 owner 就可以零成本直接获得锁; 否则, 说明有其他线程竞争, 膨胀为轻量级锁. 偏向锁无法使用自旋锁优化, 因为一旦有其他线程申请锁, 就破坏了偏向锁的假定. 轻量级锁轻量级锁时有偏向锁升级来的, 偏向锁运行在一个线程进入同步块的情况下, 当第二个线程加入锁争用的时候, 偏向锁就会升级为轻量级锁. 轻量级锁是在没有多线程竞争的前提下, 减少传统的重量级锁使用产生的性能消耗. 轻量级锁所适应的场景是线程交替执行同步块的情况, 如果存在同一时间访问同一锁的情况, 就会导致轻量级锁升级为重量级锁. 使用轻量级锁时, 不需要申请互斥量, 仅仅将 Mark Word 中的部分直接 CAS 更新子项线程栈中的 Lock Record, 如果更新成功, 则轻量级锁获取成功, 记录锁的状态为轻量级锁; 否则, 说明已经有线程获得了轻量级锁, 目前发生了锁竞争, 接下来就膨胀为重量级锁. 重量级锁重量级锁在 JVM 中又叫对象监视器(Monitor), 至少包含一个竞争锁队列, 和一个信号阻塞队列(wait 队列), 前者负责做互斥, 后者用于做线程同步. 自旋锁自旋锁原理十分简单, 如果持有锁的线程能在很短时间释放锁资源, 那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态, 它们只需要等一等(自旋), 等待有锁的线程释放锁后即可立即获取锁. 这样就避免用户线程和内核的切换的消耗. 但是线程自旋需要消耗 CPU, 如果一直获取不到锁, 那线程也不能一直占用 CPU 自旋做无用功, 所以需要设定一个自旋等待的最大时间. 如果持有锁的线程执行的时间超过自旋等待的最大时间仍没有释放锁, 就会导致其他争用锁的线程在最大等待时间内获取不到锁, 这时争用线程会停止自旋进入阻塞状态. 自适应自旋锁自适应意味着自旋的时间不在固定, 而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定: 如果在同一个锁对象上, 自旋等待刚刚陈宫获得过锁, 并且持有锁的线程正在运行中, 那么 JVM 就会任务这次自旋很可能再次成功. 进而允许自旋等待时间持续相对更长的时间, 比如 100个循环. 相反, 如果对于某个锁, 自旋很少成功获得过, 那么在以后要获取这个锁时将可能减少自旋时间甚至直接略过自旋过程, 以避免浪费处理器资源. 偏向锁 轻量级锁 重量级锁适用于不同的并发场景 偏向锁: 无实际竞争, 且将来只有第一个申请锁的线程会使用锁. 轻量级锁: 无实际竞争, 多个线程交替使用锁, 允许短时间的锁竞争. 重量级锁: 有实际竞争, 且锁竞争时间长. 如果锁竞争时间段, 可以使用自旋锁进一步优化轻量级锁, 重量级锁的性能力, 减少线程切换. 如果锁竞争程度逐渐提高, 那么偏向锁逐步膨胀到重量级锁, 能够提高系统整体性能. 锁膨胀的过程: 只有一个线程进入临界区(偏向锁), 多个线程交替进入临界区(轻量级锁), 多线程同时进入临界区(重量级锁). 优缺点对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外消耗, 和执行非同步方法相比较仅存在纳秒级的差距 如果线程间存在锁竞争, 会带来额外的锁撤销的消耗 适用于只有一个线程访问同步块的场景 轻量级锁 竞争的线程不会阻塞, 提高了程序的响应速度 如果始终得不到锁竞争的线程, 使用自旋会消耗 CPU 追求响应时间同步块执行速度非常快 重量级锁 线程竞争不使用自旋, 不会消耗 CPU 线程阻塞, 响应时间缓慢 追求吞吐量同步块执行速度较慢 CASCAS(Compare and swap) 简介使用锁时, 线程获取锁是一种悲观锁策略, 即假设每一次执行临界区代码都会产生冲突, 所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁. synchronized 关键字的实现就是悲观锁. 而 CAS 操作(又称为无锁操作), 是一种乐观锁策略. 它假设所有线程访问共享资源的时候不会出现冲突, 既然不会出现冲突, 自然而然就不会阻塞其他线程的操作. 因此, 线程就不会出现阻塞停顿的状态. 如果出现冲突怎么办? 无锁操作是使用 CAS 又叫做比较交换来鉴别线程是否出现冲突, 在更新的时候去判断此期间有没有人更新数据, 如果因为冲突失败, 就重试. 适用于写比较少的情况下, 即冲突较少发生, 这样可以省去了锁的开销, 加大系统的吞吐量. 乐观锁的实现方式 CAS乐观锁的实现主要两个步骤: 冲突检测和数据更新. CAS 是乐观锁技术, 当多个线程尝试使用 CAS 同时更新同一个变量时, 只有其中一个线程能更新变量的值, 而其他线程都失败. 失败的线程并不会被挂起, 而是被告知这次竞争中失败, 可以再次尝试. CAS 操作中包含三个操作数: 需要读写的内存位置(V) 进行比较的预期原值(A) 拟写入的新值(B) 如果内存位置V的值与预期原值A相匹配, 那么处理器会自动将该位置值更新为新值B. 否则处理器不做任何操作. 无论哪种情况, 它都不会在 CAS 指令之前返回该位置的值.(在 CAS 的一些特殊情况下将仅返回 CAS 是否成功, 而不提取当前值)CAS 有效地说明了: 我认为位置V应包含值A, 如果包含, 则将B放到这个位置, 否则, 不要更改该位置, 只告诉我这个位置的值即可. CAS 的缺点 ABA 问题 如果内存地址 V 初次读取的值是 A, 并且在准备赋值的检查到它的值仍然为A, 那我们就能说它的值没有被其他线程改过了吗? 如果这段期间它的值曾经被改成了B, 后来又被改回为A, 那CAS操作就会误认为从来没有被改变过. ava 并发包为了解决这个问题, 提供了一个带有标记的原子引用类 AtomicStampedReference, 它可以通过控制变量值的版本来保证 CAS 的正确性. 因此, 在使用 CAS 前要考虑清楚 ABA 问题是否影响程序并发的正确性, 如果需要解决 ABA 问题, 改用传统的互斥同步可能比原子类更高效. 循环时间长开销大 自旋 CAS(不成功, 就一直循环执行, 直到成功)如果长时间不成功, 会给 CPU 带来非常大的执行开销. 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时, 我们可以使用循环 CAS 的方式保证原子操作, 但是对多个共享变量操作室, 循环 CAS 就无法保证操作的原子性, 这个时候可以用锁来保证原子性. 或者将多个共享变量存储在一个对象中. 上下文切换当前任务在执行完 CPU 时间片切换到另一个任务之前会保存自己的状态, 以便下次在切换回这个任务时, 可以再加载这个任务的状态. 任务从保存到再加载的过程就是一次上下文切换. 上下文切换通常是计算密集型的. 也就是说, 它需要相当可观的处理器时间, 在每秒几十上百次的切换中, 每次切换都需要纳秒量级的时间, 所以, 上下文切换对系统来说意味着消耗大量的 CPU 时间. 事实上, 可能是操作系统中时间消耗量最大的操作. Linux 相比于其他操作系统有很多优点, 其中一项就是: 其上下文切换和模式切换的时间消耗非常少 减少上下文切换上下文切换又分为 2 种: 让步式上下文切换 和 抢占式上下文切换. 前者是指: 执行线程主动释放 CPU, 与锁竞争严重程度成正比, 可通过减少锁竞争和使用 CAS 算法来避免 后者是指: 线程因分配的时间片用尽而被迫放弃 CPU, 或者被其他优先级更高的线程所强占, 一般由于线程数大于 CPU 可用核心数引起, 可通过适当减少线程数和使用协程来避免 总结: 减少锁的使用. 多个线程竞争锁时会引起上下文切换 使用 CAS 算法. 这种算法也是为了减少锁的使用. 减少线程的使用. 使用协程 线程池如果并发的线程数量很多, 并且每个线程都是执行一个时间很短的任务就结束了, 这样频繁创建线程就会大大降低系统的效率, 因为频繁创建线程和销毁线程需要时间. 线程池的产生和数据库的链接类似, 系统启动了一个线程的代价是比较高昂的, 如果在程序启动的时候就初始化一定数量的线程, 放入线程池中, 在需要使用时从线程池中取, 用完再放回池中, 这样能大大地提高程序性能. 再者, 线程池的一些初始化配置, 也可以有效的控制系统的并发数量, 防止因为消耗过多的内存. 线程池提供了一种限制和管理资源(包括执行一个任务). 每个线程池还维护一些基本信息, 例如已完成任务的数量. 使用线程池的好处 降低资源消耗. 通过重复利用已创建的线程降低线程创建和销毁造成的消耗. 提高响应速度. 当任务到达时, 任务可以不需要等到线程创建就能立即执行. 提高线程的可管理性. 线程时稀缺资源, 如果无线地创建, 不仅会消耗系统资源, 还会降低系统的稳定性, 使用线程池可以进行统一的分配, 调优和监控. Executor 框架Executor 框架主要有三大部分组成: 任务 执行任务需要实现 Runnable 或 Callable 接口. 这两个接口的实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行.= 任务的执行 包括任务执行机制的核心接口 Executor 以及继承自 Executor 接口的 ExecutorService 接口. ScheduledThreadPoolExecutor 和 ThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口. 异步计算的结果 Future 接口以及 Future 接口的实现类 FutureTask 类. 当我们把 Runnable 接口或 Callable 接口的实现类提交(调用 submit 方法)给 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 时, 会返回一个 FutureTask 对象. Executor 框架流程图 主线程首先创建实现 Runnable 或 Callable 接口的任务对象 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换 Executor.callable(Runnable task) 或 Executors.callable(Runnable task, Object result). 把创建完成的 Runnable 对象直接交给 ExecutorService 执行 ExecutorService.execute(Runnable command), ExecutorService.submit(Runnable task), ExecutorService.submit(Callable task). 执行 execute() 和 submit() 方法的区别: execute() 方法用于提交不需要返回值的任务, 所以无法判断任务是否被线程池执行成功. submit() 方法用于提交需要返回值的任务, 线程池会返回一个 Future 类型的对象, 通过这个 Future 对象可以判断任务是否执行成功. 并且可以通过 Future 的 get() 方法来获取返回值. get() 方法会阻塞当前线程直到任务完成, 而使用 get(long timeout, TimeUnit unit) 方法则会阻塞当前线程一段时候后立即返回, 无论任务是否执行完. 如果执行 ExecutorService.submit(), ExecutorService 将返回一个实现 Future 接口的对象 目前 JDK 中返回的是 FutureTask 对象. 由于 FutureTask 实现了 Runnable, 开发者也可以创建 FutureTask, 然后交给 ExecutorService 执行. 主线程可以执行 FutureTask.get() 方法来等待任务执行完成, 主线程也可以执行 FutureTask.cancel(boolean mayInterruptIfRunning) 来取消此任务的执行. ThreadPoolExecutorThreadPoolExecutor 是 Executor 框架的核心类. 重要属性: RejectedExecutionHandler: 当 ThreadPoolExecutor 已经关闭或饱和时(达到最大线程池大小且工作队列已满), execute() 方法将要调用 Handler. MaximumPoolSize: 最大线程池大小. Queue: 用来暂时保存任务的工作队列. corePoolSize: 核心线程池大小. 创建 ThreadPoolExecutor在《阿里巴巴Java开发手册》并发处理这一章节, 明确指出线程资源必须通过线程池提供, 不允许在应用中自行显示创建线程. 使用线程池的好处是在创建和销毁线程上所消耗的时间以及系统资源开销, 解决资源不足的问题. 如果不使用线程池, 有可能会造成系统创建大量同类线程而导致消耗完内存或者”过度切换”的问题. 同时强制线程池不允许使用 Executors 去创建, 而是通过 ThreadPoolExecutor 的方式, 这样的处理方式能让开发者更加明确线程池的运行规则, 规避资源耗尽的风险 Executors 返回线程池对象的弊端如下: FixedThreadPool 和 SingleThreadExector: 允许请求的队列长度为 Integer.MAX_VALUE, 可能堆积大量请求, 从而导致 OOM CachedThreadPool 和 ScheduledThreadPool: 允许创建的线程数量为 Integer.MAX_VALUE, 可能会创建大量线程, 从而导致 OOM 方式一: 通过构造方法实现 方式二: 通过 Executors 工具类创建, 常见的有 4 种: newCachedThreadPool: 大小不受限, 当线程释放时, 可重用该线程. 适用于执行很多的短期异步任务的小程序, 或者是负载较轻的服务器. newFixedThreadPool: 大小固定, 无可用线程时, 任务需等待, 直到有可用线程. 适用于为了满足资源管理需求, 而需要限制当前线程数量的应用场景. 适用于负载较重的服务器. newSingleThreadExecutor: 创建一个但线程, 任务会按顺序依次执行. 适用于需要单个后台线程执行周期任务, 同时保证顺序地执行各个任务的应用场景. newScheduledThreadPool: 创建一个定长线程池, 支持定时及周期性任务执行. 适用于需要多个后台执行周期任务, 同时为了满足资源管理需求而需要限制后台线程的数量的应用场景 参考资料 互联网校招面试必备——Java多线程: https://juejin.im/post/5ba133126fb9a05ce02a6f12 Java多线程学习: https://blog.csdn.net/qq_34337272/article/details/79640870 彻底理解synchronized: https://juejin.im/post/5ae6dc04f265da0ba351d3ff https://juejin.im/post/5ba133126fb9a05ce02a6f12#heading-11 Java内存模型（JMM）总结: https://zhuanlan.zhihu.com/p/29881777 全面理解Java内存模型(JMM)及volatile关键字: https://blog.csdn.net/javazejian/article/details/72772461]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合框架]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJava%E9%9B%86%E5%90%88.html</url>
    <content type="text"><![CDATA[集合框架Java 中的集合框架分为两大类: Collection 和 Map, 两者区别在于: Collection 是单列集合, Map 是双列集合 Collection 中只有 Set 要求元素唯一, Map 中键(key)需要唯一 Collection 的数据结构是针对元素的, Map 的数据结构是针对键的 CollectionCollection 主要分为 List 和 Set List: 存取有序, 有索引, 可以根据索引取值, 元素可以重复 Set: 存取无序, 元素不可以重复 List ArrayList: 底层数据结构是数组, 所以查询速度快, 增删速度慢. 非线程安全. LinkedList: 底层数据结构是链表, 所以查询慢, 增删快. 非线程安全. Vector(已过时): 底层数据结构是数组, 所以查询速度快, 增删速度慢. 线程安全. ArrayList 和 LinkedList 的异同 是否线程安全: ArrayList 和 LinkedList 都是不同步的, 非线程安全 底层数据结构: ArrayList 底层使用的是 Object 数组. LinkedList 底层使用的是双向链表数据结构. 插入和删除是否受元素位置的影响: ArrayList 采用数组存储, 所以插入和删除元素的时间复杂度受元素位置的影响. LinkedList 采用链表存储, 所以插入, 删除元素时间复杂度不受元素位置影响. 是否支持快速随机访问: LinkedList 不支持高效的随机元素访问 ArrayList 支持, 通过元素的索引快速获取元素对象 内存空间占用: ArrayList 的空间浪费主要体现在在 List 列表的结尾会预留一定的容量空间. LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间(因为要存放前前驱, 后继节点以及数据) RandomAccess接口: 12public interface RandomAccess&#123;&#125; RandomAccess 接口中什么都没有定义. 也就是说, 这个接口充当一个标识的作用, 标识实现这个接口的类具有随机访问功能. 在 binarySearch() 方法中, 它要判断传入的 List 是否是 RandomAccess 的实例. 如果是, 调用 indexedBinarySearch() 方法. 如果不是, 调用 iteratorBinarySearch() 方法. 1234567public static &lt;T&gt;int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key);&#125; ArrayList 实现了 RandomAccess 接口, 而 LinkedList 没有实现. 因为 ArrayList 底层数据结构是数组, 而 LinkedList 底层数据结构是双向链表. 它们实现随机访问的时间复杂度分别为 O(1) 和 O(n). List 遍历方式的选择 实现了 RandomAccess 接口的 List: 优先选择普通 for 循环, 其次 foreach. 未实现 RandomAccess 接口的 List: 优先选择 Iterator 遍历(foreach 遍历底层实现也是通过 Iterator), size 大的数据, 不要使用普通 for 循环. Set HashSet: 底层实际上是一个 HashMap 实例, 不保证迭代顺序, 非线程安全, 允许元素为 Null, 初始容量非常影响迭代性能. HashMap 中 key 的值就是 HashSet 的值, 而 value 是 final Object PRESENT = new Object() , 操作 HashSet 元素实际上就是操作 HashMap. LinkedHashSet: 迭代有序, 允许元素为 Null, 底层实际上是一个 HashMap + 双向链表实例(即 LinkedHashMap), 非线程安全, 初始容量不影响迭代性能. TreeSet: 可以实现排序功能(添加时排序), 底层实际上是 TreeMap 实例, 非线程安全. 保证 TreeSet 元素唯一性方法: 自定义对象实现 Comparable 接口, 重写 compareTo() 方法. 创建 TreeSet 时, 向构造器中传入比较器 Comparator 接口实现类对象, 实现 Comparator 接口重写 compare() 方法(用匿名类方式). 向 TreeSet 存入自定义对象, 如果自定义类没有实现 Comparable 接口, 或者没有传入 Comparator 比较器时, 会报 ClassCastException 异常. Map特点: 保存的是键值对, 键唯一, 值可以重复. HashMap: 底层数据结构为散列表, 无序, 允许为 Null, 非线程安全, 初始容量和装载因子对 HashMap 影响较大. 具体了解: HashMap LinkedHashMap:存取有序, 底层数据结构为散列表和双向链表, 允许为 Null, 非线程安全, 初始容量和装载因子对 HashMap 影响较大. TreeMap: 有序, 底层数据结构是红黑树, 非线程安全, 使用 Comparator 或 Comparable 来比较是否相等以及排序. 如果 Comparator 为 Null, 则使用 Key 作为比较器进行比较, 并且 key 必须实现 Comparable 接口. HashMap 和 HashTable 的区别 HashMap 是非线程安全的, HashTable 是线程安全的. HashTable 内部的方法基本都经过 synchronized 修饰. 因为线程安全的问题, HashMap 比 HashTable 效率高. HashMap 允许有 Null 存在, 而在 HashTable 中不允许 Null. 如果想要线程安全, 可以使用 ConcurrentHashMap. ConcurrentHashMap 对整个桶数组进行了分割分段(Segment), 然后在每一个分段上都用 lock 锁进行保护, 相对于 HashTable 的 synchronized 锁的粒度更精细, 并发性能更好. (JDK1.8 之后 ConcurrentHashMap 启用了一种全新的方式实现, 利用 CAS 算法). ConcurrentHashMap 不允许 Null. Queue PriorityQueue: 不允许 Null, 按元素大小进行重新排序, 底层数据结构是最小堆, 非线程安全. 在遍历时, 如果不需要删除元素, 以 peek 的方式遍历每个元素. Iterator() 中提供的迭代器并不保证以有序的方式遍历其中的元素. Deque: 是一个双端队列, 还可以当做栈使用. ArrayQueue: 底层数据结构是数组, 是循环队列, 通过 head 和 tail 两个游标来实现. 非线程安全, 集合的选用 需要根据键值来获取元素值: Map 需要排序: TreeMap 不需要排序: HashMap 保证线程安全: ConcurrentHashMap 只需要存放元素值: Collection 需要保证元素唯一: TreeSet 或 HashSet 不要要元素唯一: ArrayList 或 LinkedList 参考资料 Java集合总结: https://juejin.im/post/5ad40593f265da23750759ad Java集合入门和深入学习: https://juejin.im/post/5ad82dbef265da503825b240 Java集合（七） Queue详解: https://juejin.im/post/5a3763ed51882506a463b740]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2FJavaLearning%2FJava%2FFramework%2FSpring.html</url>
    <content type="text"><![CDATA[Spring IOCIOC 和 DI 简介 IOC(Inversion of Control)控制翻转, 包含了两个方面: 控制 和 反转 简单理解: 控制: 当前对象对内部成员的控制权 反转:控制权不由当前对象管理, 由其他类 / 第三方容器来管理 IOC 不够开门见山, 于是 Martin Fowler 提出了 DI(Dependency Injection)来替代 IOC. 即让调用类对某一个接口实现类的依赖关系由第三方(容器或协作类)注入, 以移除调用类对某一个接口实现类的依赖. 通过 DI, 对象的依赖关系将有系统中负责协调各对象的第三方组件在创建对象的时候进行设定, 对象无须自行创建或管理他们的依赖关系. 依赖关系将被自动注入到需要它们的对象当中去. 使用 IOC 的好处: 不用自己组装, 拿来就用 单例, 效率高, 不浪费空间 便于单元测试, 方便切换 mock 组件 便于进行 AOP 操作, 对于使用者是透明的 统一配置, 便于修改 原理IOC 容器其实就是一个大工厂, 用来管理我们所有的对象以及依赖关系 通过反射获取类的所有信息 通过配置文件或注解来描述类与类之间的关系 结合配置信息和反射来构建出对应的对象和依赖关系 Spring IoC 容器实现对象的创建和依赖: 根据 bean 配置信息在容器内创建 bean定义注册表. 根据注册表加载、实例化 bean, 建立 bean 与 bean 之间的依赖系. 将这些准备就绪的 bean 放到 Map 缓存池中, 等待应用程序调用. AOP AOP(Aspect Oriented Programming)面向切面编程: 通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术. 利用 AOP 可以对业务逻辑的各个部分进行隔离, 从而使业务逻辑各部分之间的耦合度降低, 提高程序的可用性, 同时提高了开发的效率. 术语 连接点(Join point) 连接点是程序执行过程中能够应用通知的所有点. 目前 Spring AOP 仅支持方法级. 通知(Advice) 通知是指拦截到切点之后要做的事情就是通知, 包含了需要用于多个应用对象的横切行为. 通知的 5 种类型: Before: 在方法被调用之前调用 After: 在方法完成后调用通知, 无论方法是否执行成功 After-returning: 在方法成功执行之后调用通知 After-throwing: 在方法抛出异常后调用通知 Aroud: 通知包含了被通知的方法, 在被通知的方法调用之前和调用之后执行定义的行为 切点(Pointcut) 切点定义了通知被应用的具体位置. 切点定义了那些连接点会得到通知. 切面(Aspect) 切面是通知和切点的结合. 通知和切点共同定义了切面的全部内容–它是什么, 在何时和在何处完成其功能. 引入(Introduction) 引入允许我们向现有的类添加新方法或属性. 织入(Weaving) 织入是把切面应用到目标对象并创建新的代理对象的过程. 切面在指定的连接点被织入到目标对象中. 在目标对象的生命周期里有多个点可以进行织入. 顾问(Advisor) 顾问是切面的一种, 能够将通知以更为复杂的方式织入到目标对象中, 是将通知包装为更复杂的切面的装配器. BeanSpring 容器(Bean 工厂)可简单分成两种: BeanFactory: 这是 Spring 中较原始的 Factory, 无法支持 Spring 的许多插件, 如: AOP, Web 应用等. ApplicationContext: 这是 BeanFactory 派生而来, 还继承了其他许多接口, 提供了更多的功能. 因此, 大多数场合都是使用 ApplicationContext. BeanFactory 类继承体系 ApplicationContext 类继承体系 其中在 ApplicationContext 子类中还有一个比较重要的: WebApplicationContext: 专门为 Web 应用服务 Spring 与 Web 应用的上下文融合: Bean 的生命周期BeanFactory 的生命周期: ApplicationContext 的生命周期 上图中 设置属性值 的下一步应是: 调用 BeanNameAware 的 setBeanName() 方法 分类对方法进行解析: Bean 自身的方法: 如调用 Bean 的构造方法实例化 Bean, 调用 setter 设置 Bean 的属性值以及通过 init-method 和 destroy-method 所指定的方法 Bean 级生命周期接口方法: 如 BeanNameAware, BeanFactory, InitializingBean 和 DisposableBean, 这些接口方法由 Bean 类直接实现. 容器级生命周期接口方法: 图中带 “★” 的步骤是由 InstantiationAwareBeanPostProcessor 和 BeanPostProcessor 这两个接口实现, 一般称它们的实现类为 后处理器. 后处理器接口一般不由 Bean 本身实现, 它们独立于 Bean, 实现类以容器附加装置的形式注册到 Spring 容器中并通过接口反射为 Spring 容器预先识别. 当 Spring 容器创建任何 Bean 的时候, 这些后处理器都会发生作用, 所以这些后处理器的影响是全局性的. 用户可以通过合理地编写后处理器, 让其仅对特定的 Bean 进行加工处理. ApplicationContext 与 BeanFactory 不同之处: ApplicationContext 会利用 Java 反射机制自动识别出配置文件中定义的 BeanPostProcessor, InstantiationAwareBeanPostProcessor 和 BeanFactoryPostProcessor 后置器, 并自动将它们注册到应用上下文中. 而 BeanFactory 需要在代码中通过手动调用 addBeanPostProcessor() 方法进行注册. ApplicationContext 在初始化应用上下文的时候就实例化所有单实例的 Bean, 而 BeanFactory 在初始化容器的时候并未实例化 Bean, 直到第一次访问某个 Bean 时才实例化目标 Bean. 整个步骤详细描述: Spring 找到配置文件中 Spring Bean 的定义, 再利用 Java Reflection API 对 Bean 进行实例化. Spring 将值和 Bean 的引用注入到 Bean 对应的属性中. 如果 Bean 实现了 BeanNameAware 接口, Spring 将 Bean 的 ID 传递给 setBeanName() 方法, 设置 Bean 的名字. 如果 Bean 实现了 BeanFactoryAware 接口, Spring 将调用 setBeanFactory() 方法, 将 BeanFactory 容器实例传入. 如果 Bean 实现了 ApplicationContextAware 接口, Spring 将调用 setApplicationContext() 方法, 将 Bean 所在的应用上下文的引用传入进来. 与上面的类似, 如果实现了其他 *Aware 接口, 就调用相应的方法. ApplicationContextAware: 获得 ApplicationContext 对象, 可以用来获取所有 Bean definition 的名字. BeanFactoryAware: 获得 BeanFactory 对象，可以用来检测 Bean 的作用域。 BeanNameAware: 获得 Bean 在配置文件中定义的名字. ResourceLoaderAware: 获得 ResourceLoader 对象, 可以获得 classpath 中某个文件. ServletContextAware: 在一个 MVC 应用中可以获取 ServletContext 对象, 可以读取 context 中的参数. ServletConfigAware: 在一个MVC应用中可以获取 ServletConfig 对象, 可以读取config中的参数. 如果 Bean 实现了 BeanPostProcessor 接口, Spring 将调用它们的 postProcessBeforeInitialization() 方法. 如果 Bean 实现了 InitializingBean 接口, Spring 将调用它们的 afterPropertiesSet() 方法. 类似地, 如果 Bean 使用 init-method 声明了初始化方法, 该方法也会被调用. 如果 Bean 实现了 BeanPostProcessor 接口, Spring 将调用它们的 postProcessAfterInitialization() 方法. 此时 Bean 已经准备就绪, 可以被应用程序使用了. 它们将一直驻留在应用上下文, 直到该应用上下文被销毁. 如果 Bean 实现了 DisposableBean 接口, Spring 将调用它的 destroy() 接口方法. 同样, 如果 Bean 使用 destroy-method 声明了销毁方法, 该方法也会被调用. Bean 的作用域Spring Framework 支持五种作用域, 分别阐述如下表: 作用域 描述 singleton 整个 Spring IoC容器内只有一个这样的 bean, 生成后一直保持到容器销毁 prototype 每次从容器获取该 bean都会得到一个新的实例, 会被如何保持, 会被保持多长时间, 都由使用者决定, 容器不再管理 application 整个 ServletContext 视为一个应用, 在这个应用内只有一个这样的 bean, 创建之后一直保持到该 ServletContext 销毁 session 整个 HTTP Session 对应只有一个这样的 bean, 创建之后一直保持到该 HTTP session 被销毁 request 整个 HTTP request请求处理过程对应只有一个这样的 bean, 创建之后一直保持到该 HTTP request 处理完被销毁 五种作用域中, 通过 scope = &quot;singleton&quot;(xml), @Scope(&quot;singleton&quot;)(annotation). 其中 request, session 和 global session 三种作用域仅在基于 web 的应用中使用. 只能用在基于 web 的 Spring ApplicationContext 环境. singleton - 唯一 bean 实例当一个 bean 的作用域为 singleton, 那么 Spring IoC 容器中只会存在一个共享的 bean 实例, 并且所有对 bean 的请求. 只要 id 与该 bean 定义相匹配, 则只会返回 bean 的同一实例. 可以指定 bean 节点的 lazy-init = &quot;true&quot; 来延迟初始化 bean. 这时候, 只有在第一次获取 bean 时才会初始化 bean, 即第一次请求该 bean 时才初始化. prototype - 每次请求都会创建一个新的实例当一个 bean 的作用域为 prototype, 表示一个 bean 定义对应多个对象实例. prototype 作用域的 bean 会导致每次对该 bean 请求时都会创建一个新的 bean 实例. **prototype 是原型类型, 它在创容器时并没有实例化, 而是当我们获取 bean 时才会去创建一个对象. request - 每一次 HTTP 请求都会产生一个新的 bean, 仅在当前 HTTP request 内有效每一次 HTTP 请求都会产生一个新的 bean, 同时该 bean 仅在当前 HTTP request 内有效, 当请求结束后, 该对象的生命周期即宣告结束. session - 一次 HTTP 请求都会产生一个新的 bean, 仅在当前 HTTP session 内有效每一次 HTTP 请求都会产生一个新的 bean, 同时该 bean 仅在当前 HTTP session 内有效. 当 HTTP session 最终被废弃的时候, 在该 HTTP session 作用域内的 bean 也会被废弃掉. globalSession作用域类似于标准的 HTTP session 作用域. 不过仅仅在基于 portlet 的 web 应用中才有意义. Portlet 规范定义了全局 Session 的概念, 它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享. 在global session 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内. 参考资料 Spring IOC知识点一网打尽: https://segmentfault.com/a/1190000014979704 JavaGuide: https://github.com/Snailclimb/JavaGuide Spring 实战 第4版]]></content>
      <categories>
        <category>Java</category>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC]]></title>
    <url>%2FJavaLearning%2FJava%2FFramework%2FSpringMVC.html</url>
    <content type="text"><![CDATA[SpringMVC 简介SpringMVC 框架是以请求为驱动, 围绕 Servlet 设计, 将请求发给控制器, 然后通过模型对象, 分析器来展示请求结果视图. 其中核心类是 DispatcherServlet, 它是一个 Servlet, 顶层是实现 Servlet 的接口. SpringMVC 使用XML 方式需要在 web.xml 中配置 DispatcherServlet, 并且需要配置 Spring 监听器 ContextLoaderListener. 12345678910111213141516171819&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;!-- 如果不设置init-param标签，则必须在/WEB-INF/下创建xxx-servlet.xml文件，其中xxx是servlet-name中配置的名称。 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/springmvc-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; SpringMVC 工作原理基本流程: 客户端发送请求 -&gt; 前端控制器 DispatcherServlet 接收客户端请求 -&gt; 找到处理器映射 HandlerMapping 解析请求对应的 Handler -&gt; HandlerAdapter 根据 Handler 来调用具体的处理器来处理请求, 并处理响应的业务逻辑. -&gt; 处理器返回一个模型视图 ModelAndView -&gt; 视图解析器进行解析 -&gt; 返回一个视图对象 -&gt; 前端控制器 DispatcherServlet 渲染数据 -&gt; 将得到的视图对象返回给用户 如图所示: 流程说明: 客户端发送请求, 请求交由 DispatcherServlet 处理. DispatcherServlet 根据请求信息调用 HandlerMapping, 解析请求对应的 Handler. 解析到对应的 Handler (也就是平常所说的 Controller 控制器)后, 开始有 HandlerAdapter 适配器处理. HandlerAdapter 会根据 Handler 来调用具体的处理器来处理请求, 并处理相应的处理逻辑. 处理器处理完业务后, 会返回一个 ModelAndView 对象, Model 是返回的数据对象, View 是个逻辑上的 View. ViewResolver 会根据逻辑 View 查找实际的 View. DispatcherServlet 把返回的 Model 传给 View(视图渲染). 把 View 返回给客户端 SpringMVC 重要组件说明前端控制器 DispatcherServlet作用: SpringMVC 的入口, 接收请求, 响应结果, 相当于转发器, 中央处理器. 有了 DispatcherServlet, 减少了其他组件之间的耦合度. 用户请求到达前端控制器, 它就相当于 MVC 模式中的 C, DispatcherServlet 是整个流程控制的中心. 由它来调用其他组件处理用户的请求, DispatcherServlet 的存在降低了组件之间的耦合度. 处理映射器 HandlerMapping作用: 根据请求的 URL 查找 Handler. HandlerMapping 负责根据用户请求寻找能够处理请求的 Handler(处理器 Controller). SrpingMVC 提供了不同的映射器实现不同的映射方式, 例如: 配置文件方式 - web.xml , 实现接口方式 - implements Controller, 注解方式 - @RequestMapping (常用). 处理适配器 HandlerAdapter作用: 按照特定的规则去执行 Handler. 通过 HandlerAdapter 对处理器进行执行, 这是适配器模式的应用, 通过扩展适配器, 可以对更多类型的处理器进行执行. 处理器 Handler (Controller)编写 Handler 时需要遵循 HandlerAdapter 的要求, 这样适配器才能正确执行 Handler Handler 是继 DispatcherServlet 前端控制器的后端控制器, 在 DispatcherServlet 的控制下, Handler对具体的用户请求进行处理. 由于 Handler 涉及到具体的业务请求, 所以一般情况下需要工程师根据业务需求开发 Handler 视图解析器 View Resolver作用: 进行视图解析, 根据逻辑视图名解析成真正的视图(View) View Resolver 负责将处理结果生成 View 视图, View Resolver 首先根据逻辑视图名解析成物理视图名, 即具体的页面地址, 再生成 View 视图对象, 最后对 View 进行渲染并将处理结果通过页面展示给用户. 视图 ViewView 是一个接口, 实现类支持不同的 View 类型. 如: JSP, Freemaker, pdf, excel… 注意: 处理器 Handler 以及视图层 View 都是需要我们自己手动开发的, 其他的一些组件比如: DispatcherServlet, HandlerMapping, HandlerAdapter 等等都是 DispatcherServlet 详细解析源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package org.springframework.web.servlet; @SuppressWarnings("serial")public class DispatcherServlet extends FrameworkServlet &#123; public static final String MULTIPART_RESOLVER_BEAN_NAME = "multipartResolver"; public static final String LOCALE_RESOLVER_BEAN_NAME = "localeResolver"; public static final String THEME_RESOLVER_BEAN_NAME = "themeResolver"; public static final String HANDLER_MAPPING_BEAN_NAME = "handlerMapping"; public static final String HANDLER_ADAPTER_BEAN_NAME = "handlerAdapter"; public static final String HANDLER_EXCEPTION_RESOLVER_BEAN_NAME = "handlerExceptionResolver"; public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = "viewNameTranslator"; public static final String VIEW_RESOLVER_BEAN_NAME = "viewResolver"; public static final String FLASH_MAP_MANAGER_BEAN_NAME = "flashMapManager"; public static final String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + ".CONTEXT"; public static final String LOCALE_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".LOCALE_RESOLVER"; public static final String THEME_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_RESOLVER"; public static final String THEME_SOURCE_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_SOURCE"; public static final String INPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".INPUT_FLASH_MAP"; public static final String OUTPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".OUTPUT_FLASH_MAP"; public static final String FLASH_MAP_MANAGER_ATTRIBUTE = DispatcherServlet.class.getName() + ".FLASH_MAP_MANAGER"; public static final String EXCEPTION_ATTRIBUTE = DispatcherServlet.class.getName() + ".EXCEPTION"; public static final String PAGE_NOT_FOUND_LOG_CATEGORY = "org.springframework.web.servlet.PageNotFound"; private static final String DEFAULT_STRATEGIES_PATH = "DispatcherServlet.properties"; protected static final Log pageNotFoundLogger = LogFactory.getLog(PAGE_NOT_FOUND_LOG_CATEGORY); private static final Properties defaultStrategies; static &#123; try &#123; ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); &#125; catch (IOException ex) &#123; throw new IllegalStateException("Could not load 'DispatcherServlet.properties': " + ex.getMessage()); &#125; &#125; /** Detect all HandlerMappings or just expect "handlerMapping" bean? */ private boolean detectAllHandlerMappings = true; /** Detect all HandlerAdapters or just expect "handlerAdapter" bean? */ private boolean detectAllHandlerAdapters = true; /** Detect all HandlerExceptionResolvers or just expect "handlerExceptionResolver" bean? */ private boolean detectAllHandlerExceptionResolvers = true; /** Detect all ViewResolvers or just expect "viewResolver" bean? */ private boolean detectAllViewResolvers = true; /** Throw a NoHandlerFoundException if no Handler was found to process this request? **/ private boolean throwExceptionIfNoHandlerFound = false; /** Perform cleanup of request attributes after include request? */ private boolean cleanupAfterInclude = true; /** MultipartResolver used by this servlet */ private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ private List&lt;HandlerAdapter&gt; handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ private RequestToViewNameTranslator viewNameTranslator; private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ private List&lt;ViewResolver&gt; viewResolvers; public DispatcherServlet() &#123; super(); &#125; public DispatcherServlet(WebApplicationContext webApplicationContext) &#123; super(webApplicationContext); &#125; @Override protected void onRefresh(ApplicationContext context) &#123; initStrategies(context); &#125; protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); &#125;&#125; DispatcherServlet 类中的属性 beans: HandlerMapping: 定义请求和处理程序对象之间的映射. 用于 Handlers 映射请求和一系列的对于拦截器的前处理和后处理, 大部分用 @Controlelr 注解. SimpleUrlHandlerMapping: 通过配置文件把 URL 映射到 Controller 类. DefaultAnnotationHandlerMapping: 通过注解把 URL 映射到 Controller 类. HandlerAdapter: 帮助 DispatcherServlet 处理映射请求的处理程序的适配器. AnnotationMethodHandlerAdapter: 通过注解把 URL 映射到 Controller 类的方法上. ViewResolver: 根据配置解析实际的 View 类型. UrlBaseViewResolver: 通过配置文件, 提供一种拼接 URL 的方式来解析视图. ThemeResolver: 解决 Web 应用程序可以使用的主题, 例如提供个性化布局. MultipartResolver: 解析多部分请求, 以支持从 HTML 表单上传文件. FlashMapManager: 存储并检索可用于将一个请求属性传递到另一个请求的 input 和 output 的 .FlashMap, 通常应用于重定向. 通过方法将会话中的数据发送为 flash attribute, flash 属性会一直携带这些数据直到下一次请求, 然后才会消失. 解决了重定向时会话数据丢失的问题. HandlerExceptionResolver: 异常处理的接口. SimpleMappingExceptionResolver: 通过配置文件进行异常处理. AnnotationMethodHandlerExceptionResolver: 通过注解进行异常处理. 在 Web MVC 框架中, 每个 DispatcherServlet 都拥有自己的 WebApplicationcontext, 它继承了 ApplicationContext. WebApplicationContext 包含了其上下文和 Servlet 实例之间共享的所有的基础框架 beans. 时序图 参考资料 JavaGuide: https://github.com/Snailclimb/JavaGuide]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap 和 ConcurrentHashMap]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FHashMap%20%E5%92%8C%20ConcurrentHashMap.html</url>
    <content type="text"><![CDATA[HashMap 简介HashMap 主要用来存放键值对. 基于哈希表的 Map 接口实现. JDK8 之前 HashMap 由 数据+链表 组成, 数组是 HashMap 的主体, 链表则是主要为了解决哈希冲突而存在的. JDK8 之后, 当链表长度大于阀值(默认为8)时, 将链表转化为红黑树, 以减少搜索时间. 底层数据结构分析JDK1.8 之前JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用, 也就是 链表散列. HashMap 通过 key 的 hashCode 经过 扰动函数 处理过后得到 hash 值, 然后通过 (n - 1) &amp; hash 判断当前元素存放的位置(n 为数组的长度). 如果当前位置存在元素, 就判断该元素与要存入的元素的 hash 值以及 key 是否相同, 如果相同, 就直接覆盖, 不相同, 就通过 拉链法 解决冲突. 扰动函数所谓扰动函数指的就是 HashMap 的 hash 方法. 使用 hash 方法也就是扰动函数, 是为了防止一些实现比较差的 hashCode() 方法造成的 哈希冲突, 也就是减少碰撞. JDK 1.7的 HashMap 的 hash 方法源码: 12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; JDK 1.8 HashMap 的 hash 方法源码: 1234567 static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 相比于 JDK1.8 的 hash 方法, 虽然原理不变, JDK 1.7 的 hash 方法的性能稍差一点, 因为扰动了 4 次. 拉链法“拉链法“: 将链表和数组相结合, 也就是说创建一个链表数组, 数组中每一格都是一个链表. 若遇到哈希冲突, 则将冲突的值加到链表中即可. JDK1.8 之后JDK1.8 在解决哈希冲突时有了较大的变化: 当链表长度大于阀值(默认为 8 )时, 将链表转化为红黑树, 以减少搜索时间. HashMap类: 12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v&gt;[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor;&#125; loadFactor 加载因子 loadFactor 加载因子用来控制数组存放数据的疏密程度, loadFactor 越趋近于 1, 那么数组中存放的数据(entry)也就越多, 也就越密, 链表的长度增加的可能性(碰撞的几率)就越高. 反之, loadFactor 越小(趋近于 0), 数组中存放的数据(entry)也就越少, 链表的长度增加的可能性(碰撞的几率)就越低. loadFactory 太大导致查找元素的效率低, 太小导致数组的利用效率低, 存放的数据会很分散. loadFactory 的默认值为 0.75f 是官方给出的一个比较好的临界值. threshold 该属性是衡量数组是否需要扩增的一个标准. threshold = capacity * loadFactor, 当 size &gt;= threshold 时, 就要考虑对数组的扩展. 给定的默认容量为 16, 加载因子为 0.75. Map 在使用过程中不断往里面存放数据, 当数量达到了 16 * 0.75 = 12 时, 就要将当前的数组扩容, 而扩容涉及到 rehash, 复制数据等操作, 十分消耗性能. Node 节点类: 123456789101112131415161718192021222324252627282930313233343536373839// 继承自 Map.Entry&lt;K,V&gt;static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; // 重写hashCode()方法 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 重写 equals() 方法 public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 源码分析构造方法 HashMap() HashMap(int) HashMap(int,float) HashMap(Map&lt;? extends K,? extends V&gt;) 123456789101112131415161718192021222324252627// 默认构造函数。public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; // 包含另一个“Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; putMapEntries 方法:123456789101112131415161718192021222324final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; put 方法HashMap 只提供了 put 用于添加元素，putVal 方法供 put 方法调用, 并没有提供给用户使用. putVal 方法添加元素: 如果定位到的数组位置没有元素, 直接插入. 如果定位到的数组位置有元素, 就和要插入的 key 比较. 相同: 直接覆盖. 不同: 判断 p 是否是一个新的树节点. 是, 就调用 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value) 将元素添加进入. 不是, 就遍历链表插入. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table未初始化或者长度为0, 进行扩容 // 第一次 resize 和后续的扩容有些不一样, 因为这次是数组从 null 初始化到默认的 16 或自定义的初始容量 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 确定元素存放在哪个桶中, 桶为空, 新生成结点放入桶中(此时, 这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等, key相等, 如果是, 取出这个节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等, 即key不相等; 判断是否为红黑树结点 else if (p instanceof TreeNode) // 调用红黑树的插值方法放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 到这里说明数组上该位置为链表结点 for (int binCount = 0; ; ++binCount) &#123; // 在链表最末插入结点(Java 7 是插入到链表最前面) if ((e = p.next) == null) &#123; // 到达尾部, 在尾部插入新结点 p.next = newNode(hash, key, value, null); // TREEIFY_THRESHOLD 为 8, 所以如果结点数量(算上这次加入的为 9)达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等, 即节点是否存在 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // e != null 表示在桶中有找到key值、hash值与插入元素相等的结点 // 下面的操作为进行 "值覆盖" 并返回旧值 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 由于插入新值导致 sieze 大于阈值, 则需要扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; get 方法12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 数组元素相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize 方法对 table 进行扩容, 会伴随着一次重新 hash 分配, 并且会遍历 hash 表中所有的元素, 十分耗时, 尽量避免. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了, 就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 对应使用 new HashMap(int initialCapacity) 初始化后, 第一次 put 的时候 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // 对应使用 new HashMap() 初始化后, 第一次 put 的时候 // signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) // 用新的数组大小初始化新的数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 如果是初始化, 到这里就结束了, 返回 newTab 即可. if (oldTab != null) &#123; // 数据迁移, 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 如果原数组位置上只有单个元素, 简单插入这个元素即可 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 红黑树节点迁移 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表迁移. 优化重 hash. // 参考: https://blog.csdn.net/u012961566/article/details/72963157 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; ConcurrentHashMap 简介ConcurrentHashMap 和 HashMap 思路差不多, 但是因为支持并发操作, 所以要复杂一些. Java8 以前, 整个 ConcurrentHashMap 由一个个 Segment 组成, Segment 意思是 “部分”或”一段”, 所以也称为分段锁. Segment 通过继承 ReentrantLock 来进行加锁, 所以每次需要加锁的操作锁住的是一个 Segment, 这样只要保证每个 Segment 是线程安全的, 也就实现了全局的线程安全. Java8 对 HashMap 进行了比较大的改动, 对于 ConcurrentHashMap, Java8 也引入了红黑树. 最难的在于扩容, 数据迁移操作不容易看懂. Java8 以前 Java8 以前的 ConcurrentHashMap 是由 Segment, HashEntry 组成, 和 HashMap 一样, 仍然是数组+链表. ConcurrentHashMap 采用了分段锁技术, 其中 Segment 继承于 ReentrantLock. 不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理, 理论上 ConcurrentHashMap 支持 ConcurrencyLevel(Segment 数组数量)的线程并发. 每当一个线程占用锁访问一个 Segment 时, 不会影响到其他 Segment. 源码解析核心成员变量123456789101112131415161718192021222324252627282930// table数组的默认长度，这个和HashMap是一样的static final int DEFAULT_INITIAL_CAPACITY = 16;// 加载因子 static final float DEFAULT_LOAD_FACTOR = 0.75f;// 并发级别 static final int DEFAULT_CONCURRENCY_LEVEL = 16;// 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 这里可以看到DEFAULT_INITIAL_CAPACITY、DEFAULT_LOAD_FACTOR、MAXIMUM_CAPACITY，都是和HashMap相应字段的值是相同的。// 段组的最小长度，这里最小值为2的原因是，如果小于2的话(即为1)，就没有锁分段的意义了，就和Hashtable一样了，不能两个线程同时并发存和取数据了。 static final int MIN_SEGMENT_TABLE_CAPACITY = 2;// 段组的最大长度 static final int MAX_SEGMENTS = 1 &lt;&lt; 16; static final int RETRIES_BEFORE_LOCK = 2;// 段掩码 final int segmentMask;// 段偏移量 final int segmentShift;// Segment 数组, 存放数据时首先需要定位到具体的 Segment 中. final Segment&lt;K,V&gt;[] segments; transient Set&lt;K&gt; keySet; transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; transient Collection&lt;V&gt; values; Segment12345678910111213141516 static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶 transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor; &#125; 初始化1234567891011121314151617181920212223242526272829303132333435363738394041424344public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; // 计算并行级别 ssize，因为要保持并行级别是 2 的 n 次方 while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; // 我们这里先不要那么烧脑，用默认值，concurrencyLevel 为 16，sshift 为 4 // 那么计算出 segmentShift 为 28，segmentMask 为 15，后面在定位 Segment 时的散列算法中会用到这两个值 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // initialCapacity 是设置整个 map 初始的大小， // 这里根据 initialCapacity 计算 Segment 数组中每个位置可以分到的大小 // 如 initialCapacity 为 64，那么每个 Segment 或称之为"槽"可以分到 4 个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组， // 并创建数组的第一个元素 segment[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入 segment[0] UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss;&#125; 如果使用 new ConcurrentHashMap() 初始化, 那么: Segment 数组长度为 16, 不可以扩容. segment[i] 的默认大小为 2, 负载因子是 0.75, 得出初始阈值为 1.5, 所以在插入第一个元素时不会触发扩容. 这里初始化了 segment[0], 其他位置还是 null. 当前 segmentShift 的值为 32 - 4 = 28, segmentMask 的值为 16 - 1 = 15. put 方法ConcurrentHashMap 的 put 方法 123456789101112131415161718public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 根据 hash 值找到 Segment 数组中的位置 j // hash 是 32 位，无符号右移 segmentShift(28) 位，剩下低 4 位， // 然后和 segmentMask(15) 做一次与操作，也就是说 j 是 hash 值的最后 4 位，也就是槽的数组下标(范围就在 0~size 之间). int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 刚刚说了，初始化的时候初始化了 segment[0]，但是其他位置还是 null， // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false);&#125; Segment 内部(数据+链表组成)的 put 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; // 在往该 segment 写入前，需要先获取该 segment 的独占锁 // 先看主流程，后面还会具体介绍这部分内容 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; // 这个是 segment 内部的数组 HashEntry&lt;K,V&gt;[] tab = table; // 再利用 hash 值，求应该放置的数组下标 int index = (tab.length - 1) &amp; hash; // first 是数组该位置处的链表的表头 HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 下面这串 for 循环虽然很长，不过也很好理解，想想该位置没有任何元素和已经存在一个链表这两种情况 for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; // 存在一个链表 K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; // 覆盖旧值 e.value = value; ++modCount; &#125; break; &#125; // 继续顺着链表走 e = e.next; &#125; else &#123; // 没有任何元素 // node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。 // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值，这个 segment 需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 扩容后面也会具体分析 else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， // 其实就是将新的节点设置成原链表的表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; // 解锁 unlock(); &#125; return oldValue;&#125; 初始化槽(Segment): ensureSegment ConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0], 对于其他槽来说, 在插入第一个值的时候初始化. 这里需要考虑并发, 因为可能会有很多个线程同时进来初始化同一个槽 segment[j], 不过只要有一个成功即可. 1234567891011121314151617181920212223242526272829private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg;&#125; 获取写入锁: scanAndLockForPut 在某个 segment 中 put 操作时, 首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value);, 也就是说先进行一次 tryLock(), 快速获取该 segment 的独占锁, 如果失败, 就进入到 scanAndLockForPut() 来获取锁. 123456789101112131415161718192021222324252627282930313233343536373839private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; &#125; // 重试次数如果超过 MAX_SCAN_RETRIES（单核1多核64），那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 该方法有两个出口, 一个是 tryLock() 成功, 循环终止, 另一个是重试次数超过 MAX_SCAN_RETRIES, 进入 lock() 阻塞等待获取独占锁. 总之就是在获取该 segment 的锁后退出, 如果需要的话, 顺便实例化一下 node. 扩容: rehash 扩容是针对 segment 数组某个位置内部的 HashEntry[] 进行扩容. put 的时候, 如果判断该值插入会导致该 segment 的元素个数超过阈值, 那么先进行扩容, 再插值. 该方法不需要考虑并发, 因为执行该方法时, 已经持有了该 segment 的独占锁. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 扩容为原来的 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 // 因为计算位置是 与运算, 扩容时 sizeMask(newCapacity-1) 的二进制多了一位1, 即 与运算 多计算一位, 结果或不变, 或增加原来的 oldCapacity. int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头结点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 // lastRun 节点之后的元素都会放入 idx + oldCapacity, 之前的有可能放入 idx 或 idx + oldCapacity, 在下一个循环体中处理 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 上面两个循环, 第一个循环去除也是可以工作的, 但是这个循环下来, 如果 lastRun 的后面有比较多的节点, 那么这次就是值得的. 因为我们只需要克隆 lastRun 前面的节点, 就省去了移动之后的节点的开销. 根据 Doug Lea 所说, 根据统计, 如果使用默认的阈值, 大约只有 1/6 的节点需要克隆. get 方法 计算 hash 值, 找到 segment 数组中的具体位置. segment 中也是一个数组, 根据 hash 找到数组(HashEntry[])中具体的位置. 找到所在的链表, 顺着链表查找即可. 1234567891011121314151617181920public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 1. hash 值 int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // 2. 根据 hash 找到对应的 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; // 3. 找到segment 内部数组相应位置的链表，遍历 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 并发问题分析get 过程中是没有加锁的, 需要考虑 get 的时候在同一个 segment 中发生了 put 或 remove 操作. put 操作的线程安全性 初始化 segment, 使用了 CAS 来初始化 Segment 中的数组(UNSAFE.compareAndSwapObject(ss, u, null, seg = s)). 添加节点到链表的操作是插入到表头的, 所以如果这个时候 get 操作在链表遍历的过程中已经到了中间, 时不会影响的. 如果 get 在 put 之后, 需要保证刚刚插入表头的节点被读取, 这个依赖于 setEntryAt() 方法中使用的 UNSAFE.putOrderedObject. 扩容. 扩容是创建了数组, 如果 get 先行, 那么就是在旧的 table 上做查询操作; 如果 put 先行, 那么 put 操作的可见性保证就是 table 使用了 volatile 关键字. remove 操作的线程安全性get 操作需要遍历链表, 但是 remove 操作会”破坏”链表. 如果 remove 破坏的节点 get 操作已经过去了, 那么不存在任何问题. 如果 remove 先行破坏了一个节点, 分两种情况考虑: 如果此节点是头结点, 那么需要将头结点的 next 设置为数组该位置的元素, table 虽然使用了 volatile 修饰, 但是 volatile 并不能提供数组内部操作的可见性保证, 所以源码中使用了 UNSAGE 来操作数组, 见 setEntryAt(). 如果要删除的节点不是头结点, 它会将删除节点的后继节点接到前驱节点中, 这里的并发保证是 next 属性是 volatile 的. Java8 以后 结构上和 Java8 的 HashMap 基本一样, 不过在保证线程安全性的实现比较复杂. 抛弃了原来的 Segment 分段锁, 而采用了 CAS + synchronized. 也将存放数据的 HashEntry 改为 Node, 但作用都是相同的. 其中 val 和 next 都用了 volatile 修饰来保证可见性. 源码解析初始化1234567891011// 无参构造函数里，什么都不干public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; 通过提供初始容量, 计算了 sizeCtl, sizeCtl = [(1.5 * initialCapacity + 1), 然后向上取最接近的 2 的 n 次方]. 如 initialCapacity 为 10, 那么得到的 sizeCtl 为 16; 如果 initialCapacity 为 11, 那么 sizeCtl 为 32. put 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果数组"空"，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else &#123; // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果发现了"相等"的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 到了链表的最末端，将这个新值放到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 红黑树 Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // binCount != 0 说明上面在做链表操作 if (binCount != 0) &#123; // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount &gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 对 table 长度加 1, 并检查是否需要扩容, 或者正在扩容. 如果需要扩容, 就调用扩容方法, 如果正在扩容, 就帮助其扩容. addCount(1L, binCount); return null;&#125; 初始化数组: initTable 主要就是初始化一个合适大小的数组, 然后设置 sizeCtl. 初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制. 1234567891011121314151617181920212223242526272829private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // 初始化的"功劳"被其他线程"抢去"了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 链表转红黑树: treeifyBintreeifyBin 不一定会进行红黑树转换, 也可能仅仅做数组扩容. 123456789101112131415161718192021222324252627282930313233private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // MIN_TREEIFY_CAPACITY 为 64 // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 后面我们再详细分析这个方法 tryPresize(n &lt;&lt; 1); // b 是头结点 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 加锁 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; // 下面就是遍历链表，建立一颗红黑树 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; // 将红黑树设置到数组相应位置中 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 扩容: tryPresize这个方法要完全看懂需要看之后的 transfer 方法. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) &#123; // c：size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 0.75 * n &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) &#123; // 我没看懂 rs 的真正含义是什么，不过也关系不大 // 根据容量 n 得到本次扩容唯一标识 - 摘自 https://www.cnblogs.com/stateis0/p/9062088.html // 参考: https://segmentfault.com/a/1190000016124883 int rs = resizeStamp(n); if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) // 如果 sc 的低 16 位不等于 标识符（校验异常 sizeCtl 变化了） // 如果 sc == 标识符 + 1 （扩容结束了，不再有线程进行扩容）（默认第一个线程设置 sc ==rs 左移 16 位 + 2，当第一个线程结束扩容了，就会将 sc 减一。这个时候，sc 就等于 rs + 1） // 如果 sc == 标识符 + 65535（帮助线程数已经达到最大） // 如果 nextTable == null（结束扩容了） // 如果 transferIndex &lt;= 0 (转移状态变化了) // 结束循环 break; // 2. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法 // 此时 nextTab 不为 null if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 1. 将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 我是没看懂这个值真正的意义是什么？不过可以计算出来的是，结果是一个比较大的负数 // 如果不在扩容，将 sc 更新：标识符左移 16 位 然后 + 2. 也就是变成一个负数。高 16 位是标识符，低 16 位初始是 2. // 调用 transfer 方法，此时 nextTab 参数为 null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 更新 sizeCtl 为负数后，开始扩容。 transfer(tab, null); &#125; &#125;&#125; 这个方法的核心在于 sizeCtl 值的操作, 首先将其设置为一个负数, 然后执行 transfer(tab, null), 在下一个循环将 sizeCtl 加 1, 并执行 transfer(tab, nt), 之后可能是继续 sizeCtl 加 1, 并执行 transfer(tab, nt). 所以, 可能的操作就是执行 1 次 transfer(tab, null) + n 次 transfer(tab, nt). 本次扩容第一次执行时, nt 为 null, 单线程扩容, 后续在扩容过程中有其他线程进入扩容方法时, nt 不为 null, 多线程协助其扩容. 数据迁移: transfer 将原来的 tab 数组元素迁移到新的 nextTab 数组中. 此方法支持多线程执行, 外围调用此方法的时候, 会保证第一个发起数据迁移的线程, nextTab 参数为 null. 原数组长度为 n, 所以我们有 n 个迁移任务, 让每个线程每次负责一个小任务是最简单的, 每做完一个任务再检测是否有其他没做完的任务, 帮助迁移就可以了. 而 Doug Lea 使用了一个 stride, 简单理解就是步长, 每个线程每次负责迁移其中的一部分, 如每次迁移 16 个小任务. 所以就需要一个全局的调度者来安排哪个线程执行哪几个任务, 这个就是属性 transferIndex 的作用. 第一个发起数据迁移的线程会将 transferIndex 指向原数组最后的位置, 然后从后往前的 stride 个任务属于第一个线程, 然后将 transferIndex 指向新的位置, 再往前的 stride 个任务属于第二个线程, 以此类推. 即将一个大的迁移任务分为了一个个子任务. transfer 这个方法并没有实现所有的迁移任务, 每次调用这个方法只实现了 transferIndex 往前 stride 个位置的迁移工作, 其他的需要外围的来控制. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // stride 在单核下直接等于 n，多核模式下为 (n&gt;&gt;&gt;3)/NCPU，最小值是 16 // stride 可以理解为”步长“，有 n 个位置是需要进行迁移的， // 将这 n 个任务分为多个任务包，每个任务包有 stride 个任务 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 // 前面我们说了，外围会保证第一个发起迁移的线程调用此方法时，参数 nextTab 为 null // 之后参与迁移的线程调用此方法时，nextTab 不会为 null if (nextTab == null) &#123; try &#123; // 容量翻倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; // nextTable 是 ConcurrentHashMap 中的属性 nextTable = nextTab; // transferIndex 也是 ConcurrentHashMap 的属性，用于控制迁移的位置 transferIndex = n; &#125; int nextn = nextTab.length; // ForwardingNode 翻译过来就是正在被迁移的 Node // 这个构造方法会生成一个Node，key、value 和 next 都为 null，关键是 hash 为 MOVED // 后面我们会看到，原数组中位置 i 处的节点完成迁移工作后， // 就会将位置 i 处设置为这个 ForwardingNode，用来告诉其他线程该位置已经处理过了 // 所以它其实相当于是一个标志。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // advance 指的是做完了一个位置的迁移工作，可以准备做下一个位置的了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab /* * 下面这个 for 循环，最难理解的在前面，而要看懂它们，应该先看懂后面的，然后再倒回来看 * */ // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 每一次自旋前的预处理，主要是定位本轮 transfer 处理的桶区间 // advance 为 true 表示可以进行下一个位置的迁移了 // 简单理解结局：i == transferIndex - 1, bound == transferIndex-stride while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 将 transferIndex 值赋给 nextIndex // 这里 transferIndex 一旦小于等于 0，说明原数组的所有位置都有相应的线程去处理了 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // 看括号中的代码，nextBound 是这次迁移任务的边界，注意，是从后往前 bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // 当前是处理最后一个 transfer 任务的线程, 或出现扩容冲突. if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; // 所有的迁移操作已经完成 nextTable = null; // 将新的 nextTab 赋值给 table 属性，完成迁移 table = nextTab; // 重新计算 sizeCtl：n 是原数组长度，所以 sizeCtl 得出的值将是新数组长度的 0.75 倍 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // 之前我们说过，sizeCtl 在迁移前会设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 // 然后，每有一个线程参与迁移就会将 sizeCtl 加 1， // 这里使用 CAS 操作对 sizeCtl 进行减 1，代表做完了属于自己的任务 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 到这里，说明 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT， // 也就是说，所有的迁移任务都做完了，也就会进入到上面的 if(finishing)&#123;&#125; 分支了 finishing = advance = true; // 最后完成数据迁移的线程重新检查一次旧 table 中的所有桶, 看是否都被正确迁移到新 table了 // 正常情况下, 重新检查时, 旧 table 所有桶都应该是 ForwardingNode; // 特殊情况下, 比如扩容冲突(多个线程申请到同一个 transfer 任务), 此时当前线程领取的任务会作废, 那么最后检查时还要处理因为作废而没有被迁移的桶, 把他们正确迁移到新 table 中. i = n; // recheck before commit &#125; &#125; // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 该位置处是一个 ForwardingNode，代表该位置已经迁移过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 对数组该位置处的结点加锁，开始处理数组该位置处的迁移工作 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // 头结点的 hash 大于 0，说明是链表的 Node 节点 if (fh &gt;= 0) &#123; /** * 下面的过程会将旧桶中的链表分成两部分：ln链和hn链 * ln链会插入到新table的槽i中，hn链会插入到新table的槽i+n中 */ // 找到原链表中的 lastRun，然后 lastRun 及其之后的节点是一起进行迁移的 // lastRun 之前的节点需要进行克隆，然后分到两个链表中. 类似于 Java7 中 ConcurrentHashMap 的迁移. int runBit = fh &amp; n; // 由于 n 是 2 的幂次, 所以 runBit = 要么是 0(原位置), 要么高位是 1(原位置 + 原容量). Node&lt;K,V&gt; lastRun = f; // lastRun 指向最后一个相邻 runBit 不同的节点. for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 其中的一个链表放在新数组的位置 i setTabAt(nextTab, i, ln); // 另一个链表放在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; else if (f instanceof TreeBin) &#123; // 红黑树的迁移 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 如果一分为二后，节点数少于 8，那么将红黑树转换回链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; // 将 ln 放置在新数组的位置 i setTabAt(nextTab, i, ln); // 将 hn 放置在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; &#125; &#125; &#125; &#125;&#125; get 方法 计算 hash 值. 根据 hash 值找到数组对应位置: (n - 1) &amp; h. 根据该位置处节点性质进行相应查找 如果该位置为 null, 那么直接返回 null. 如果该位置处的节点刚好就是我们需要的, 返回该节点的值即可. 如果该位置节点的 hash 值小于 0, 说明正在扩容, 或者是红黑树. 如果以上 3 条都不满足, 那就是链表, 遍历比对即可. 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 判断头结点是否就是我们需要的节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果头结点的 hash 小于 0，说明 正在扩容，或者该位置是红黑树 else if (eh &lt; 0) // 参考 ForwardingNode.find(int h, Object k) 和 TreeBin.find(int h, Object k) return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 参考资料 JavaGuide: https://github.com/Snailclimb/JavaGuide Java7/8 中的 HashMap 和 ConcurrentHashMap 全解析: http://www.importnew.com/28263.html HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你！: https://juejin.im/post/5b551e8df265da0f84562403#heading-0]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HashMap</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaEE]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJavaEE.html</url>
    <content type="text"><![CDATA[Servlet 在 Java Web 程序中, Servlet 主要负责接收用户请求 HttpServletRequest , 在 doGet() , doPost() 中做相应的处理, 并将回应 HttpServletResponse 返回给用户. Servlet 可以设置初始化参数, 供 Servlet 内部使用. 一个 Servlet 类中只会有有一个实例, 在它初始化时调用 init() 方法, 销毁时掉哦用 destroy() 方法. Servlet 需要在 web.xml 中配置. 一个 Servlet 可以设置多个 URL 访问. Servlet 非线程安全, 多线程并发的读写会导致数据不同步的问题. 解决的办法是尽量不要定义 name 属性, 而是把 name 变量分别定义在 doGet() 和 doPost() 方法中. 注意: 多线程的并发读写 Servlet 类属性会导致数据不同步. 但是如果只是并发地读取属性而不写入, 则不存在数据不同步的问题. 因此 Servlet 里的只读属性最好定义为 final 类型 Servlet 和 CGI 区别CGI 的不足: 需要为每个请求启动一个 CGI 程序的系统进程. 如果请求频繁, 会带来很大开销. 需要为每个请求加载和运行一个 CGI 程序, 带来很大开销. 需要重复编写处理网络协议的代码以及编码, 这些工作都是非常耗时的. Servlet 的优点: 只需要启动一个操作系统进程以及加载一个 JVM , 大大降低了系统的开销. 如果多个请求需要做同样处理的时候, 这时只需要加载一个类, 大大降低了开销. 所有动态加载的类可以实现对网络协议以及请求解码的共享, 大大降低了工作量 Servlet 能直接和 Web 服务器交互, 而普通的 CGI 程序不能. Servlet 还能在各个程序之间共享数据, 使数据库连接池之类的功能很容易实现 一个基于 Java Web 的应用通常包含一个或多个 Servlet 类. Servlet 不能够自行创建并执行, 它是在 Servlet 容器中运行的, 容器将用户的请求传递给 Servlet 程序并将 Servlet 的响应回传给用户. Servlet 接口中的方法及生命周期Servlet 接口中定义了 5 个方法, 其中前三个方法与 Servlet 生命周期相关: void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destroy() String getServletInfo() ServletConfig getServletConfig() 生命周期Web 容器加载 Servlet 并将其实例化后, Servlet 生命周期开始. 容器运行其 init() 方法进行 Servlet 的初始化. 请求到达时调用 Servlet 的 service() 方法, service() 方法根据需要调用与请求对应的 doGet() 或 doPost() 方法. 当服务器关闭或项目被卸载时, 服务器会将 Servlet 实例销毁. 此时会调用 Servlet 的 destroy() 方法. init 和 destroy 方法只会执行一次, service 方法会在客户端每次请求 Servlet 时执行. Servlet 中有时会用到一些需要初始化与销毁的资源. 因此可以把初始化资源的代码放入 init 方法中, 销毁资源的代码放入 destroy 中. 转发(Forward)和重定向的区别(Redirect)的区别转发是服务器行为, 重定向是客户端行为 转发(Forward) 通过 RequestDispatcher 对象的 forward (HttpServletRequest request, HttpServletResponse response) 方法实现的. RequestDispatcher 可以通过 HttpServletRequest 的 getRequestDispatcher() 方法获得. 如: request.getRequestDispatcher(&quot;login_sucess.jsp&quot;).forward(request, response); 重定向(Redirect) 利用服务器返回的状态码实现的. 客户端浏览器请求服务器时, 服务器返回一个状态码. 服务器通过 HttpServletResponse 的 setStatus(int status) 方法设置状态码. 如果服务器返回 301 或者 302 , 则浏览器会到新的网址重新请求该资源 从地址栏显示说 forward 是服务器请求资源, 服务器直接访问目标地址的 URL , 把那个 URL 的响应内容读取过来, 然后把这些内容再发给浏览器. 浏览器端不知道服务器发送的内容来自哪里, 所以地址栏还是原来的地址 redirect 是服务端根据逻辑, 发送一个状态码, 告诉浏览器重新去请求哪个地址. 所以地址栏显示的是新的 URL 从数据共享说 forward: 转发的页面和转发到的页面可以共享 request 中的数据 redirect: 不能共享数据 从运用方面说 forward: 一般用于用户登录的时候, 根据角色转发到相应的模块 redirect: 一般用于用户注销登录返回主页面和跳转到其他页面等 从效率说 forward: 高 redirect: 低 实现会话跟踪使用Cookie向客户端发送 Cookie 123Cookie c = new Cookie("name","value"); // 创建Cookiec.setMaxAge(60*60*24); // 设置最大时效，此处设置的最大时效为一天response.addCookie(c); // 把Cookie放入到HTTP响应中 从客户端读取 Cookie 123456789101112String name ="name"; Cookie[]cookies =request.getCookies(); if(cookies !=null)&#123; for(int i= 0;i&lt;cookies.length;i++)&#123; Cookie cookie =cookies[i]; if(name.equals(cookis.getName())) //something is here. //you can get the value cookie.getValue(); &#125; &#125; 优点: 数据可以持久保存, 不需要服务器资源. 基于文本的 key-value 缺点: 大小受限制, 用户可以禁用 Cookie 功能. 保存在本地, 有安全风险 URL 重写在 URL 中添加用户会话的信息作为请求的参数, 或者将唯一的会话 ID 添加到 URL 结尾以标识一个会话. 优点: 在 Cookie 被禁用时依然可以使用 缺点: 必须对网站的 URL 进行编码, 所有页面必须动态生成. 不能用预先记录下来的 URL 进行访问. 隐藏的表单1&lt;input type = "hidden" name = "session" value = "..." /&gt; 优点: Cookie 被禁用时可以使用 缺点: 所有页面必须是表单提交之后的结果 HttpSession最强大功能最多的会话跟踪技术. 当一个用户访问某个网站时会自动创建 HttpSession 分配一个 JSESSIONID. 每个用户可以访问他自己的 HttpSession . 可以通过 HttpServletRequest 对象的 getSession() 方法获得 HttpSession , 通过 HttpSession 对象的 getAttribute() 方法, 同时传入 key 就可以获取保存在 HttpSession 中的对象. HttpSession 保存在服务器的内存中, 因此不要将过大的对象存放在里面. 虽然目前的 Servlet 容器可以在内存即将满时将 HttpSession 中的对象移动到其他存储设备中, 但也会影响性能. 添加到 HttpSession 中的值可以是任意 Java 对象. 这个对象最好实现了 Serializable 接口, 这样 Servlet 容器在必要的时候可以将其序列化到文件中, 否则在序列化时就会出现异常. 使用 和Cookie搭配使用 第一次创建 Session 的时候, 服务端会在 HTTP 协议中告诉客户端需要在 Cookie 里记录一个 Session ID . 以后在每次 HTTP 请求时, 客户端都会发送 Cookie 信息到服务端, Cookie 中就携带着 Session ID 来帮助服务端辨识当前请求的用户身份. 如果客户端禁用了 Cookie 功能, 就会使用下面的方式. 和URL重写技术搭配使用 在每次 HTTP 交互, URL 后面附带加上一个如 sid=xxxx 这样的参数, 服务端根据此来识别用户 Cookie 和 Session 的主要区别在于: Cookie 保存在客户端, Session 保存在服务端 Get 和 Post 请求区别 Get Post 用来请求从服务器上获得的资源. 用于幂等的场景 操作服务器资源. 用户不幂等的场景 将表单中数据按照 name = value 的形式, 加上 ? 拼接到 URL 后, 各个变量之间用 &amp; 连接. 将表单中的数据放在 HTTP 协议的请求头或消息体中 传输的数据收到浏览器对 URL 长度的限制(最大长度为 2048 字符) 可以通过 request body 技术传输大量数据, 上传文件通常使用 post 方式 参数会显示在地址栏上 数据不会显示在地址栏 使用 MIME类型 application/x-www-form-urlencode 的 URL 编码文本的格式传递参数 支持更多的编码类型且不对数据类型做限制 Servlet url-patternServlet 对 url 进行匹配的规则分为四类, 优先级从高到低分别为: 精确匹配: /user/login | /index.html. 请求的 url 与 url-pattern 完全一致, 则匹配成功, 否则进入下一级匹配. 路径匹配: /* | /user/*. 请求的 url 满足 url-pattern 的格式, 如 /user/login 会被匹配到 /user/* 的 Servlet, 匹配失败进入下一级匹配. 扩展名匹配: *.jpg | *.js. 请求的 url 满足 url-pattern 的格式, 如 cat.jpg 会被匹配到 *.jpg 的 Servlet, 匹配失败进入缺省匹配. 缺省匹配: /. 以上几个 url-pattern 都匹配失败就会进入缺省匹配的 Servlet, 缺省匹配的默认 Servlet 是 Tomcat 自身提供的 default Servlet. 参考资料 JavaGuide: https://github.com/Snailclimb/JavaGuide]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础笔记]]></title>
    <url>%2FJavaLearning%2FJava%2FBasis%2FJava%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[JVM JDK 和 JRE javac :包含于JDK中的将.java文件编译为字节码(.class)文件的编译器. JVM可以理解的代码就叫做字节码. 即扩展名为.class的文件. JSP 会被转换为 Java servlet. 并被 JDK 用 javac 编译器编译. 字符型常量和字符串常量 形式上:字符型常量是单引号引起的一个字符.如&#39;a&#39;. 字符串常量是双引号引起的若干个字符如&quot;Hello world&quot;. 含义上:字符型常量可以当做一个整形值(16位 Unicode 字符 0~65535)参与表达式运算. 字符串常量代表一个地址值(引用字符串常量池中的对象. 不会被垃圾回收) 重载(Overload)和重写(Override) 重载 重写 发生在同一个类中 发生在父子类中 方法名必须相同参数类型、个数、顺序、方法返回值和权限修饰符可以不同 方法名和参数列表必须相同. 权限修饰符范围大于等于父类抛出异常的范围小于等于父类. 返回值返回小于等于父类 封装 继承 多态封装封装把一个对象的属性私有化. 同时提供一些想让外界访问的属性的方法. 继承继承是使用已存在的类的定义作为基础建立新类的技术. 新类的定义中可以增加新的成员变量和成员方法. 且同时拥有父类声明的可以被继承的变量和方法. 多态多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定. 只有在程序运行期间才能确定. 实现多态的方法:继承和接口 String 及 String.intern()前提知识 new String(&quot;abc&quot;) 的分析: &quot;abc&quot; 会在字符串常量池中查找是否有 abc这个字符串, 如果有, 返回其引用, 如果没有, 将 abc 添加到字符串常量池, 再返回其引用. new String 会在堆内存中创建一个 &quot;abc&quot; 的字符串对象, 返回堆中该对象的引用. 无论常量池中是否存在 &quot;abc&quot; 字符串. new String(&quot;abc&quot;) 会创建两个对象, 一个是在常量池中通过 &quot;abc&quot; 创建的, 一个是在堆中创建的 String 对象. String.intern() 的分析: 12String str = new String("abc");str.intern(); 以上面代码为例: - **在 JDK 6 以前**: 如果*字符串常量池*中存在 `&quot;abc&quot;`, 则返回其在常量池中的引用, 否则将 `&quot;abc&quot;` 拷贝到常量池中, 并返回常量池中的应用. - **从 JDK 7 开始**: 如果*字符串常量池*中存在 `&quot;abc&quot;`, 仍返回其在常量池中的引用, **但不存在时**, 不再将其拷贝到常量池中, 而是直接在常量池中生成一个对在堆中的原字符串的引用. 此例中即 `new String(&quot;abc&quot;)` 创建的对象的引用. 使用 Final String VALUE = &quot;abc&quot;; 时, 代码中所有使用 VALUE 变量的地方在编译时都会被直接替换成 &quot;abc&quot;. 例如: 123Final String a = "1"; Final String b = "2"; String c = a + b; // 编译后变为: String c = "1" + "2"; 字符串引用进行 + 运算时, 会创建 StringBuilder/StringBuffer.append(), 之后再转换成 String, 这种情况下会 new 一个 String 对象. 例如: 12345String str1 = "a";String str2 = "b";String str12 = str1 + str2;String ab = "a" + "b";str12 == ab; // false 案例案例一 123456789public static void main(String[] args) &#123; String str1 = "string"; String str2 = new String("string"); String str3 = str2.intern(); System.out.println(str1==str2);// false System.out.println(str1==str3);// true System.out.println(str2==str3);// false&#125; 对应 #1. 案例二 123456789101112131415public static void main(String[] args) &#123; String baseStr = "baseStr"; final String baseFinalStr = "baseStr"; String str1 = "baseStr01"; String str2 = "baseStr"+"01"; String str3 = baseStr + "01"; String str4 = baseFinalStr+"01"; String str5 = new String("baseStr01").intern(); System.out.println(str1 == str2);// true System.out.println(str1 == str3);// false System.out.println(str1 == str4);// true System.out.println(str1 == str5);// true &#125; 对应 #2 #3 #4 案例三 1234567public static void main(String[] args) &#123; String str2 = new String("str")+new String("01"); str2.intern(); String str1 = "str01"; System.out.println(str2==str1);// true &#125; 对应 #2. 案例四 123456public static void main(String[] args) &#123; String str1 = "str01"; String str2 = new String("str")+new String("01"); str2.intern(); System.out.println(str2 == str1);// false &#125; 对应 #2. StringBuffer 和 StringBuilder可变性StringString类中使用private final char value[]来保存字符串. 所以String对象是不可变的 StringBuilder 与 StringBuffer两者都继承自AbstractStringBuilder类. 该类同样使用char[] value来保存字符串. 但没有用final修饰. 所以这两种对象是可变的 线程安全性StringString中的对象是不可变的. 在字符串常量池中只有一个. 线程安全. StringBuffer 和 StringBuilderAbstractStringBuilder是 StringBuilder 与 StringBuffer的父类. 定义了一系列字符串的基本操作. 如expandCapacity, append, insert, indexOf等公共方法. StringBufferStringBuffer 对方法加了同步锁或者对调用的方法加了同步锁. 所以是线程安全的. StringBuilderStringBuilder 没有加锁. 所以是非线程安全的. 性能 String 类型每次被改变的时候. 都会在字符串常量池中生成一个新的 String 对象. 然后将指针指向新的 String对象 StringBuffer每次都会对对象本身进行操作. 不会生成新的对象并改变引用. StringBuilder 相比 StringBuilder 性能仅提升10%~15%左右. 但非线程安全. 使用 操作少量数据 =&gt; String 拼接字符串时 =&gt; StringBuilder StringBuffer 单线程操作字符串缓冲区下大量数据 =&gt; StringBuilder 多线程操作字符串缓冲区下大量数据 =&gt; StringBuffer 自动装箱与拆箱装箱将基本类型用他们对应的引用类型包装起来.如:int-&gt;Integer 自动装箱时编译器调用valueOf将原始类型值转换成对象 拆箱将包装类型转换为基本数据类型.如:Integer-&gt;int 同时自动拆箱时，编译器通过调用类似intValue(). doubleValue()这类的方法将对象转换成原始类型值. 发生场景 进行 = 赋值操作（装箱或拆箱） 进行+，-，*，/混合运算 （拆箱） 进行&gt;. &lt;. ==比较运算（拆箱） 调用equals进行比较（装箱） ArrayList. HashMap等集合类 添加基础类型数据时（装箱） 注意事项生成无用对象增加GC压力因为自动装箱会隐式地创建对象.如果在一个循环体中. 会创建无用的中间对象. 这样会增加GC压力. 拉低程序的性能.所以在写循环时一定要注意代码. 避免引入不必要的自动装箱操作. 对象相等比较==可以用于基本类型进行比较. 也可以用于对象进行比较. 当用于对象与对象之间比较时. 比较的不是对象代表的值. 而是检查两个对象是否是同一对象.这个比较过程中没有自动装箱发生.进行对象值比较不应该使用==，而应该使用对象对应的equals方法. 1234567891011121314151617181920212223242526272829303132333435public class AutoboxingTest &#123; public static void main(String args[]) &#123; // Example 1: == comparison pure primitive – no autoboxing int i1 = 1; int i2 = 1; System.out.println("i1==i2 : " + (i1 == i2)); // true // Example 2: equality operator mixing object and primitive Integer num1 = 1; // autoboxing int num2 = 1; System.out.println("num1 == num2 : " + (num1 == num2)); // true // Example 3: special case - arises due to autoboxing in Java Integer obj1 = 1; // autoboxing will call Integer.valueOf() Integer obj2 = 1; // same call to Integer.valueOf() will return same // cached Object cache range(-128~127) System.out.println("obj1 == obj2 : " + (obj1 == obj2)); // true // Example 4: equality operator - pure object comparison Integer one = new Integer(1); // no autoboxing Integer anotherOne = new Integer(1); System.out.println("one == anotherOne : " + (one == anotherOne)); // false &#125;&#125;Output:i1==i2 : truenum1 == num2 : trueobj1 == obj2 : trueone == anotherOne : false 缓存的对象在Java中, 基本数据类型的包装类型都会有缓存, 如 Integer 会对 value 在 -128 到 127 的对象进行缓存. 当创建新的Integer对象时(Integer.valueOf()) 如果符合这个这个范围. 并且已有存在的相同值的对象. 则返回这个对象 否则创建新的Integer对象. 因为自动拆箱, 自动装箱实质就是调用 Integer.valueOf() 和 Integer.intValue(), 所以会使用到缓存. 静态方法内不能调用非静态成员静态方法可以不通过对象进行调用. 因此在静态方法里. 不能调用其他非静态变量. 也不可以访问费静态变量成员. 在 Java 中定义一个空的无参构造方法的作用Java程序在执行子类的构造方法之前. 如果没有用super()来调用父类特定的构造方法. 就会调用父类的无参构造方法.因此. 如果父类中只定义了有参构造方法. 而子类的构造方法中没有用super()来调用父类中特定的构造方法. 则编译时将发生错误. 接口和抽象类的区别 接口 抽象类 方法默认是public. 所有方法都不能有实现且子类必须重写全部方法Java 8开始. 接口提供默认方法default. 子类不需重写 可以有非抽象方法 实例变量必须是public static final类型 可以不是final类型 多继承 单继承 不能用new实例化. 但可以声明. 但是必须引用一个实现该接口的对象接口是行为的抽象. 是一种行为的规范.适合对类的行为抽象 同样不能用new实例化. 但可以声明. 但是必须引用一个继承该类的对象抽象是对类的抽象. 是一种模板设计.适合对事物抽象 为什么接口属性必须是 public static final 接口是一种高度抽象的模板. 而接口中的属性也就是模板成员.就应当是所有实现模板的类的共有特性. 其次. 接口中如果可以定义非final变量的话. 而方法又都是abstract的. 这就会造成有可变成员变量. 但对应的方法却无法操作这些变量.接口是一种更高层面的抽象. 是一种规范. 功能定义的声明. 所有可变的东西都应归属到实现类中. 这样的接口才能起到标准化. 规范化的作用.所以接口中的属性必然是final的 最后, 接口只是对事物的属性和行为更高层次的抽象.对修改关闭. 对扩展开放.接口是对开闭原则的一种体现 接口的静态方法在 Java8 中. 接口也可以定义静态方法. 可以直接用接口名调用. 实现类和实现四不可以调用的. 如果同时实现多个接口. 接口中定义了一样的默认方法. 必须重写. 否则会报错. 成员变量和局部变量的区别 成员变量 局部变量 成员变量属于类. 可以用public private static等修饰符修饰 局部变量在方法中定义或是方法的参数. 不能被权限修饰符修饰. 除了 final 非静态时. 随对象存储于堆内存中 随方法存储于栈内存中 没有被赋初值. 则会自动以类型的默认值而赋值 不会被自动赋值 == 与 equals==: 判断两个对象的地址是不是相等. 即是不是同一个对象. 都是基本类型时比较的是值是否相等. 都是引用类型时比较的是内存地址是否相等.基本类型和它的封装类型时. 封装类型拆箱为基本类型然后比较值是否相等. equals: 通过方法中定义的方式进行比较.所有类都继承了Object类的equals()方法. 其默认是用==来进行比较两个对象. 如果调用方法的对象重写了此方法. 按重写的方法中的方式进行比较. equals 与 hashCode重写equals时必须重写hashCode方法 hashCode()hashCode()的作用是获取哈希码. 也称为散列码. 返回的是一个 int 整数. 这个哈希码的作用是确定该对象在哈希表中的索引位置. hashCode()定义在 JDK 的 Object.java中. 意味着任何内都包含有hashCode()函数. 散列表存储的是键值对(key-value). 特点是:能根据”键”快速检索出对应的”值”. 这里就会用到散列码. 用来快速定位要查找的对象存储位置. 为什么要有 hashCode当把对象加入 HashSet 时. HashSet 会先计算对象的 hashCode 值来判断对象加入的位置, 同时会与其他加入的对象的 hashCode 值作比较, 如果没有相符的 hashCode , HashSet 会假设对象没有重复出现, 如果发现有相同的 hashCode 值的对象, 这时就会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同. 如果两者相同, HashSet 就不会让其加入操作成功. 如果不同, 就会重新散列到其他位置. 这样就大大减少了 equals 的次数, 相应地就大大提高了执行速度. hashCode() 与 equals() 相关规定 如果两个对象相等, 则 hashCode 一定也是相同的 两个对象相等, 调用 equals() 方法是返回一定是 true 两个对象有相同的 hashCode 值, 它们不一定是相等的 因此, equals 方法被重写时, hashCode 也一定要重写覆盖 hashCode 的默认行为是对堆上的对象产生独特值, 如果没有重写 hashCode , 则该 class 的两个对象无论如何都不会相等(即使这两个对象指向相同的数据) 线程 进程 程序 线程 线程与进程类似, 但线程是一个比进程更小的执行单位. 一个进程在其执行过程中可以产生多个线程. 同类的多个线程共享同一块内存和一组系统资源, 所以系统在产生一个线程, 或者在各个线程之间切换工作时, 负担要比进程小得多, 因此, 线程也被称为轻量级进程 进程 进程是程序的一次执行过程,是系统运行程序的基本单位,因此进程是动态的. 系统运行一个程序即使一个进程从创建, 运行到消亡的过程. 一个正在执行的程序就是一个进程, 它在计算机中一个一个指令地执行者, 同时每个进程占有某些系统资源, 如CPU时间, 内存空间, 文件, 输入输出设备的使用权等. 当程序被执行时, 会被操作系统载入到内存中. 线程和进程的最大的不同在于基本上各进程是独立的, 而同一进程中的各线程极有可能会相互影响. 程序 程序是含有指令和数据文件, 被存储在硬盘或其他的数据存储设备中的静态代码 线程的基本状态 Java 线程状态变化如下图 线程在创建之后处于 NEW(新建) 状态, 调用 start() 方法后开始运行后, 线程处于 READY(可运行) 状.可运行状态状态的线程获得了 CPU 时间片(timeslice)后就处于 RUNNING(运行) 状态. 操作系统隐藏 Java 虚拟机(JVM)中的 RUNNABLE 和 RUNNING 状态, 它只能看到 RUNNABLE 状态. 所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态. 当线程执行 wait() 方法之后, 线程进入 WAITING(等待) 状态. 进入等待状态的进程需要依靠其他线程的通知才能返回到运行状态. 而 TIME_WATING(超时等待) 状态相当于在等待状态的基础上增加了超时限制. 比如通过 sleep(long millis) 方法或 wait(long millis) 方法可以将 Java 线程置于 TIME_WATING(超时等待) 状态. 当超时时间到达后, Java 线程会返回到 RUNNABLE 状态. 当线程调用同步(synchronized)方法时, 在没有获取到锁的情况下, 线程将会进入到 BLOCKED(阻塞) 状态. 线程在执行 Runnable 的 run() 方法之后将会进入到 TERMINATED(终止) 状态 final static this super 关键字finalfinal 关键字主要作用在三个地方: 变量 方法 类 对于一个 final 变量, 如果是基本类型的变量, 则其一旦在初始化之后便不能修改. 如果是引用类型的变量, 则在对其进行初始化之后便不能在让其指向另一个对象. 当用 final 修饰一个类时, 表明这个类不能被继承. final 类中的所有成员方法都会被隐式地指定为 final 方法. 使用 final 方法的原因是: 把方法锁定, 以防任何继承类修改它的含义. staticstatic 关键字主要有以下四种使用场景: 修饰成员变量和成员方法: 被 static 修饰的成员属于类, 不属于单个这个类的某个对象, 被该类的所有对象共享, 建议通过类名调用. 被 static 声明的成员变量属于静态变量. 静态变量存放在 Java 内存区域的方法区. 调用格式: 类名.静态变量名 类名.静态方法名() 方法区与 java 堆一样, 是各个线程共享的内存区域, 它用于存储已被虚拟机加载的类信息, 常量, 静态变量, 即时编译器编译后的代码等数据. 虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分, 但是它却有一个别名叫做 Non-Heap(非堆), 目的应该是与 Java 堆区分开来 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码之前执行(静态代码块 -&gt; 非静态代码块 -&gt; 构造方法). 该类不管创建多少对象, 静态代码只执行一次. 1234&gt; static&#123;&gt; 静态代码块;&gt; &#125;&gt; 一个类的静态代码块可以有多个, 位置任意. 它不处于任何方法体内, JVM 加载类时会执行这些静态的代码块, 如果静态代码块有多个, JVM 将按照他们在类中出现的先后顺序依次执行它们. 静态代码块对于定义在它之后的静态变量, 可以赋值, 但是不能访问. 静态内部类(static 修饰类的话只能修饰内部类): 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后隐含地保存着一个引用, 该引用是指向创建它的外围类(外围类名.this), 但是静态内部类却没有. 非静态内部类在外部类创建时, 会被隐式地初始化并绑定到外部类上, 而静态类则不会被初始化. 即, 静态内部类的创建是不需要依赖外围类的创建. 可以用于单例模式的延迟加载. 它不能够使用外围类的非 static 成员变量和方法. 静态内部类实现单例模式 123456789101112131415public class Singleton &#123; // 声明为 private 避免调用默认构造方法创建对象 private Singleton() &#123; &#125; // 声明为 private 表明静态内部该类只能在该 Singleton 类中被访问 private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getUniqueInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 当 Singleton 类加载时, 静态内部类 SingletonHolder 没有被加载进内存. 只有当调用 getUniqueInstance() 方法而触发 Singleton.INSTANCE 时 SingletonHolder 才会被加载, 此时初始化 INSTANCE 实例, 并且 JVM 能确保该实例只被实例化一次. 静态导包(用来导入类中的静态资源): 格式为: import static 这两个关键字连用可以导入某个类中的指定静态资源, 并且不需要使用类名调用类中的静态成员, 可以直接使用类中静态变量和成员方法. 123456789101112// Math. --- 将Math中的所有静态资源导入, 这时候可以直接使用里面的静态方法, 而不用通过类名进行调用// 如果只想导入单一某个静态方法，只需要将换成对应的方法名即可import static java.lang.Math.;// 换成import static java.lang.Math.max;具有一样的效果 public class Demo &#123; public static void main(String[] args) &#123; int max = max(1,2); System.out.println(max); &#125;&#125; static{} 静态代码块与 {} 非静态代码块(构造代码块)相同点: 都是在 JVM 加载类时且在构造方法执行之前执行, 在类中都可以定义多个, 定义多个时按定义的顺序执行, 一般在代码块中对一些 static 变量进行赋值. 不同点: 静态代码块在非静态代码块之前执行(静态代码块 -&gt; 非静态代码块 -&gt; 构造方法). 静态代码块只在第一次 new 执行一次, 之后不再执行. 而非静态代码块在每 new 一次就执行一次. 非静态代码块可以在普通方法中定义, 而静态代码块不行. 非静态代码块与构造函数的区别是: 非静态代码块是给所有对象进行统一初始化, 而构造函数是给对应的对象初始化, 因为构造函数可以是多个的, 运行哪个构造函数就会建立什么样的对象 而无论建立哪个对象, 都会先执行相同的构造代码块. 也就是说, 构造代码块中定义的是不同对象共性的初始化内容. thisthis 关键字用于引用类的当前实例, 如: 1234567891011class Manager &#123; Employees[] employees; void manageEmployees() &#123; int totalEmp = this.employees.length; System.out.println("Total employees: " + totalEmp); this.report(); &#125; void report() &#123; &#125;&#125; 在以上示例中, this 关键字用于两个地方: this.employees.length: 访问 Manager 类的当前实例的变量 this.report(): 调用 Manager 类的当前实例的方法 supersuper 关键字用于从子类访问父类的变量和方法. super.方法名(): 调用父类的非 private 方法 super(): 调用父类的构造方法 super.变量名: 操作父类的非 private 变量 使用 this 和 super 要注意的问题: super 调用父类中的其他构造方法时, 要放在构造方法的首行. this 调用本类中的其他构造方法时, 也要放在首行 this, super 不能用在 static 方法中 被 static 修饰的成员属于类, 不属于单个这个类的某个对象, 被类中所有对象共享. 而 this 代表对父类的引用, 指向父类对象. 所以, this 和 super 是属于对象范畴的东西, 而静态方法是属于类范畴的东西 Java 中的异常处理Java 异常类层次结构图 在 Java 中, 所有的异常都有一个共同的祖先 java.lang.Throwable类. Throwable有两个重要的子类: Exception(异常)和 Error(错误). 二者都是 Java 异常处理的重要子类, 各自都包含大量的子类. Error: 是程序无法处理的错误, 表示运行程序中较严重的问题. 大多数错误是 JVM 出现的问题. 如: Java虚拟机运行错误(VritualMachineError), 当 JVM 没有足够的内存资源执行操作时, 将出现 OutOfMemoryError . 这些异常发生时, JVM 一般会选择线程终止. 这些错误表示故障发生于虚拟机自身, 或者发生在虚拟机试图执行应用时. 这些错误是不可查的, 错误通过 Error 的子类描述. Exception: 是程序本身可以处理的异常. Exception 类有一个重要的子类: RuntimeException. 该类异常由虚拟机抛出, 线程不会被终止. NullPointerException: 访问的变量没有任何引用对象时, 抛出该异常. ArithmeticException: 算数运算异常, 整数除以 0 时, 抛出该异常. ArrayIndexOutOfBoundsException: 数组下标越界时, 抛出该异常. Throwable 类常用方法 public String getMessage(): 返回异常发生时的详细信息. public String toString(): 返回异常发生时的简要描述. public String getLocalizedMessage(): 返回异常对象的本地化信息. 使用 Throwable 的子类覆盖这个方法, 可以生成本地化信息. 如果子类没有覆盖该方法, 则该方法返回的信息与 getMessage() 返回的结果相同. public void printStackTrace(): 在控制台上打印 Throwable 对象封装的异常信息. 异常处理try catch try: 用于捕获异常. 其后可接任意个 catch 块. 如果没有 catch 块, 则必须跟一个 finally 块 catch: 用于处理 try 捕获到的异常. 越具体的类必须越先捕获处理. finally: 无论是否捕获或处理异常, 块中的语句都会被执行. 当在 try 块或 catch 块中遇到 return 或 throw 语句时, finally 语句块将在方法返回之前执行. 以下 4 种特殊情况下, finally块不会被执行: 在 finally 语句块中第一行发生了异常. 如果在其他行发生异常, 之前行的代码仍会执行. 在前面的代码中使用了 System.exit(int) 退出虚拟机. 程序所在的线程死亡 当所有的非守护线程中止时, 不论存不存在守护线程, 虚拟机都会kill掉守护线程从而中止程序. 所以, 如果守护线程中存在 finally 代码块, 那么当所有的非守护线程中止时, 守护线程被 kill 掉, 其 finally 代码块是不会执行的. 关闭 CPU 关于返回值: 如果 try 于中中有 return , 返回的是 try 语句块中的变量值. 详细过程如下: 如果有返回值, 就把返回值保存到局部变量中 执行 jsr 指令跳转到 finally 语句中执行 执行完 finally 语句后, 返回之前保存在局部变量中的值 如果 try , finally 块中均有 return , 则忽略 try 中的 return , 而使用 finally 中的 return throw使用 throw 语句抛出异常, 在方法上声明 , 交由调用方法的对象来处理. 序列化 Java 序列化技术是将对象编码成字节流. 反之, 将字节流重新构建成对象, 称之为反序列化. 实现对象的持久化. 实现借助 common-lang 工具类 12345678910111213import org.apache.commons.lang3.SerializationUtils;public class Test &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername("Java"); user.setAddress("China"); byte[] bytes = SerializationUtils.serialize(user); User u = SerializationUtils.deserialize(bytes); System.out.println(u); &#125;&#125; 注意事项: 序列化对象必须实现序列化接口. 序列化对象里面的属性是对象的话也要实现序列化接口. 类的对象序列化后, 类的序列化 ID 不能轻易修改, 不然反序列化会失败. 类的对象序列化后, 类的属性有增加或者删除不会影响序列化, 只是值会丢失. 如果父类序列化了, 子类会继承父类的序列化, 子类无需添加序列化接口. 如果父类没有序列化, 子类序列化了, 子类中的属性能正常序列化, 但父类的属性会丢失, 不能序列化. 用 Java 序列化的二进制字节数据只能由 Java 反序列化, 不能被其他语言反序列化。如果要进行前后端或者不同语言之间的交互一般需要将对象转变成 Json/Xml 通用格式的数据, 再恢复原来的对象. 如果某个字段不想序列化, 在该字段前加上 transient 关键字即可 反射 Java 反射机制在程序运行时, 对于任意一个类, 都能够知道这个类的所有属性和方法. 对于任意一个对象, 都鞥能够调用它的任意一个方法和属性. 这种动态调用对象的方法的功能, 称为 Java 的反射机制 反射机制很重要的一点就是”运行时”. 其使得我们可以在程序运行时加载, 探索以及使用编译期间完全未知的 .class文件. 也就是说, Java 程序可以在一个运行时才得知名称的 .class 文件, 然后获悉其完整构造, 并生成对象实体, 或对其变量 field 进行操作, 或调用其方法 method. 获取Class类的三种方法: 类名.class 对象名.getClass() Class.forName(“要加载的类名”) 主要方法 Class.getName(): 获取类的名称 Class.getFields(): 获取当前类及其所继承的父类的 public 变量 Class.getDeclaredFields(): 获取当前类的所有成员变量, 不论访问权限 Class.getMethods(): 获取当前类及其所继承的父类的 public 方法 Class.getDeclaredMethods(): 获取当前类的所有成员方法, 不论访问权限 Method.setAccessible(true): 获取当前方法的访问权限, 操作私有方法时必须设置, 否则会报异常 IllegalAccessException Field.setAccessible(true): 获取当前变量的访问权限, 操作私有方法时必须设置, 否则会报异常 IllegalAccessException Method.invoke(): 调用当前方法 Field.set(): 修改当前变量的值 修改常量的特殊情况使用 static final 修饰的常量值, 在 JVM 编译时, 会在常量被使用的地方将常量名直接替换为常量值来优化代码. 而有些数据类型不会被优化. 要想避免上面出现的特殊情况, 有两种方法来修改常量的值 方法一: 1234567891011public class TestClass &#123; //...... private final String FINAL_VALUE; //构造函数内为常量赋值 public TestClass()&#123; this.FINAL_VALUE = "FINAL"; &#125; //......&#125; 输出: 123Before Modify：FINAL_VALUE = FINALAfter Modify：FINAL_VALUE = ModifiedActually ：FINAL_VALUE = Modified 解释: 将赋值放在构造函数中, 构造函数只有在 new 对象时才会调用, 所以不会在编译阶段被直接优化为常量值, 而是指向常量名. 这样就可以在运行阶段来通过反射修改常量 方法二: 将声明常量的语句改为使用三目表达式赋值: 12private final String FINAL_VALUE = null == null ? "FINAL" : null; 因为 null == null ? &quot;FINAL&quot; : null 是在运行时刻计算的, 在编译时刻不会计算, 也就不会被优化. 判断是否能修改 大数值BigInteger 类实现了任意精度的整数运算, BigDecimal 类实现了任意精度的浮点数运算. 应用在需要精确运算结果的场合, 如: 涉及金钱汇率以及不同表达式计算出的结果的比较 类加载及初始化顺序 基类的静态代码块, 基类的静态成员字段. 并列优先级, 按代码中出现的先后顺序执行. (只在第一次加载类时执行, 即 &lt;clinit&gt;() 方法.) 派生类的静态代码块, 派生类的静态成员字段. 并列优先级, 按代码中出现的先后顺序执行. (只在第一次加载类时执行, 即 &lt;clinit&gt;() 方法.) 基类普通代码块, 基类普通成员字段. 并列优先级, 按代码中出现的先后顺序执行. 基类构造函数. 3, 4 在每次实例化对象时执行. 即在 &lt;init()&gt; 方法中. 派生类普通代码块, 派生类普通成员字段. 并列优先级, 按代码中出现的先后顺序执行. 派生类构造函数. 5, 6 在每次实例化对象时执行. 即在 &lt;init()&gt; 方法中. 说明: 因为派生类的构造函数中会如果没有显示调用 this(), 会隐式调用 super(). 所以 5,6 会在 3,4 之后. 如果在初始化一个派生类的实例时, 使用到了重写的方法, 那么基类和派生类都会使用派生类重写的此方法. 示例基类 123456789101112131415161718192021222324/** * Father */public class Father &#123; private static int j = setj(); static &#123; System.out.println("Father static block"); &#125; &#123; System.out.println("Father non-static block"); &#125; private int i = seti(); Father() &#123; System.out.println("Father constructor"); &#125; private int seti() &#123; System.out.println("Father non-static field "); return 0; &#125; private static int setj() &#123; System.out.println("Father static field"); return 0; &#125;&#125; 派生类 123456789101112131415161718192021222324/** * Son */public class Son extends Father &#123; private int i = seti(); private static int j = setj(); static &#123; System.out.println("Son static block"); &#125; &#123; System.out.println("Son non-static block"); &#125; Son() &#123; System.out.println("Son constructor"); &#125; private int seti() &#123; System.out.println("Son non-static field"); return 0; &#125; private static int setj() &#123; System.out.println("Son static field"); return 0; &#125;&#125; 测试代码及结果 12345678910/** * demo01 */public class demo01 &#123; public static void main(String[] args) &#123; Son s = new Son(); System.out.println("----------------"); Son s2 = new Son(); &#125;&#125; 1234567891011121314151617Father static fieldFather static blockSon static fieldSon static blockFather non-static blockFather non-static field Father constructorSon non-static fieldSon non-static blockSon constructor----------------Father non-static blockFather non-static field Father constructorSon non-static fieldSon non-static blockSon constructor 注意: 在程序运行时, JVM 首先会试图访问 main() 方法, 并会加载其所在的类. Java 线程 join() 方法join() 方法的源码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Waits at most &#123;@code millis&#125; milliseconds for this thread to * die. A timeout of &#123;@code 0&#125; means to wait forever. * * &lt;p&gt; This implementation uses a loop of &#123;@code this.wait&#125; calls * conditioned on &#123;@code this.isAlive&#125;. As a thread terminates the * &#123;@code this.notifyAll&#125; method is invoked. It is recommended that * applications not use &#123;@code wait&#125;, &#123;@code notify&#125;, or * &#123;@code notifyAll&#125; on &#123;@code Thread&#125; instances. * * @param millis * the time to wait in milliseconds * * @throws IllegalArgumentException * if the value of &#123;@code millis&#125; is negative * * @throws InterruptedException * if any thread has interrupted the current thread. The * &lt;i&gt;interrupted status&lt;/i&gt; of the current thread is * cleared when this exception is thrown. */public final synchronized void join(long millis)throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125;&#125; 简单使用示例: 12345public static void main(String[] args) throws InterruptedException&#123; Thread ta = new Thread(() -&gt; System.out.println("haha")); ta.start(); ta.join()&#125; 刚开始不理解为什么 a.join() 会使 main 线程等待. 这里对源码 isAlive() 和 wait(0) 做下说明: isAlive() 判断的是判断调用 join() 方法的线程是否存活. 完整的方法签名是: 1234/*** Tests if this thread is alive. A thread is alive if it has been started and has not yet died.*/public final native boolean isAlive(); 这个方法调用的是 Thread 类中的本地方法, 是普通的方法调用, 所以判断的是当前对象线程是否存活. wait() 方法是 Object 类中的方法, 调用该方法会使获取当前对象锁的线程等待. 在示例中, ta.join() 方法是在主线程中调用, join() 方法是同步方法, 所以当前线程(main)在调用 ta.join() 时, 获取了 join() 方法上的对象(ta)的锁. 所以调用 join() 执行到 wait() 方法时会使 main 线程等待, 直到 ta 线程执行结束后, main 线程才会继续执行. Java 对象大小计算https://www.cnblogs.com/xrq730/p/6928133.html 参考资料 JavaGuide: &lt;https://github.com/Snailclimb/JavaGuide Java 核心技术 第十版]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CORS跨域资源共享]]></title>
    <url>%2FJavaLearning%2FWeb%2FCORS%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB.html</url>
    <content type="text"><![CDATA[简介 跨域资源共享(CORS)标准新增了一组 HTTP 首部字段,允许服务器声明哪些源站有权限访问哪些资源.另外,规范要求,对那些可能对服务器数据产生副作用的HTTP 请求方法(特别是 GET 以外的 HTTP 请求,或者搭配某些 MIME 类型的 POST 请求),浏览器必须首先使用 OPTIONS 方法发起一个预检请求(preflight request),从而获知服务端是否允许该跨域请求.服务器确认允许之后,才发起实际的 HTTP 请求.在预检请求的返回中,服务器端也可以通知客户端,是否需要携带身份凭证(包括 Cookies 和 HTTP 认证相关数据). 两种请求浏览器将CORS请求分成两类:简单请求和非简单请求 只要满足一下两个条件,就属于简单请求: 请求方法是以下三种方法之一: HEAD GET POST HTTP的header信息不超出以下几种字段 Accept Accept-Language Content-Language Last-Event-ID Content-Type只限于:application/x-www-form-urlencoded,multipart/form-data,text/plain 不满足以上条件的就属于非简单请求 简单请求对于简单请求,浏览器直接发出CORS请求.在RequestHeader中添加Origin字段 Origin字段用来说明本次请求的请求源(协议+域名+端口).服务器根据这个值决定是否同意这次请求 如果Origin指定的请求源不在许可范围内,服务器会返回一个正常的HTTP回应.这个响应的header信息中没有包含Access-Control-Allow-Origin字段,浏览器发现后就知道出错.从而抛出一个,然后被XMLHttpRequest的onerror`回调函数捕获. 这种错误无法通过状态码识别,因为是一个正常的HTTP回应 如果Origin指定的域名在许可范围内,服务器返回的响应会多出几个字段. 1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 以Access-Control-的字段与CORS请求相关 Access-Control-Allow-Origin 该字段是必须的.它的值是请求时Origin字段的值,或者为*表示接受任意域名的请求. Access-Control-Allow-Credentials 该字段可选,它的值是一个布尔值,表示是否允许发送Cookies.默认情况下,Cookies不包括在CORS请求之中.设置为true则Cookie可以包含在请求中,一起发送给服务器.如果服务器不要浏览器发送 Cookie,删除该字段即可. Cookie中携带有Session Id相关信息,所以在请求一些需要得到Session进行身份验证的服务时,需要加上这个字段设置为true.比如:登录操作. 而客户端也需要在Ajax请求中打开withCredentials属性. 12345&gt; xhrFields: &#123;&gt; withCredentials: true&gt; &#125;,&gt; crossDomain: true&gt; 注意:如果要发送Cookie,Access-Control-Allow-Origin就不能为*.必须明确指定与网页一致的域名.Cookie也遵循同源策略,只有用服务器域名设置的Cookie才会上传,其他域名的Cookie不会上传.(防止了伪造Cookie的攻击手段) Access-Control-Expose-Headers 该字段可选.CORS请求时,XMLHTTPRequest对象的getResponseHeader()方法只能拿到6个基本字段:Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma.如果想要拿到其他的字段,就需要在此字段中指定.上面的例子中指定,getResponseHeader(&#39;FooBar&#39;)可以返回FooBar字段的值 非简单请求预检请求非简单请求是对服务器有特殊要求的请求,如请求方法为PUT或DELETE,或者Content-Type字段的类型是application/json 非简单请求的CORS请求,会在正式通信之前,增加一次HTTP查询请求,即预检请求. 浏览器查询服务器,当前网页所在的域名是否在服务器的许可名单之中,以及可以使用哪些HTTP动词和头信息字段.只有在得到肯定答复后,浏览器才会发出正式的XMLHttpRequest请求,否则就报错 12345var url = 'http://api.alice.com/cors';var xhr = new XMLHttpRequest();xhr.open('PUT', url, true);xhr.setRequestHeader('X-Custom-Header', 'value');xhr.send(); 上面构造了一个非简单请求.请求方法是PUT,并且发送一个自定义字段X-Custom-Header 浏览器就会自动发出一个预检请求.确认服务器是否接受这样的请求.预检请求的HTTP头信息如下 12345678OPTIONS /cors HTTP/1.1Origin: http://api.bob.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 预检请求的请求方法是OPTIONS,表示这个请求是用来查询的.头信息里,关键字段是Origin,表示请求源 Access-Control-Request-Method 该字段是必须的,用来列出浏览器的CORS请求会用到哪些HTTP方法,这里是PUT Access-Control-Request-Headers 该字段可以有多个值,用,分隔.指定浏览器CORS请求会额外发送的头信息字段,这里是X-Custom-Header 预检请求的回应服务器收到预检请求后,检查Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后,确认是否允许跨域请求,并作出回应 123456789101112131415HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Allow-Credentials: trueAccess-Control-Max-Age: 1728000Content-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 上面的HTTP回应中,关键的是Access-Control-Allow-Origin字段,表示http://api.bob.com可以请求数据.如果为*表示接受任意域名的跨域请求. 如果服务器否定了预检请求,会返回一个正常的HTTP回应,但是没有任何CORS相关的头信息字段.这是浏览器会认为服务器不同意预检请求,便会出发一个错误,被XMLHttpRequest对象的onerror回调函数捕获.控制台会打印出如下的报错信息:XMLHttpRequest cannot load http://api.alice.com.Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. Access-Control-Allow-Methods 该字段必须,可以有多个值,用,分隔.表明服务器支持的所有跨域请求的方法.返回所有是为了避免多次预检请求. Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段,则Access-Control-Allow-Headers字段是必须的.也可以由多个值,用,分隔.表明服务器至支持的所有头信息字段,不限于浏览器在预检请求中请求的字段. Access-Control-Allow-Credentials 同简单请求. Access-Control-Max-Age 该字段可选.用来指定本次预检请求的有效期,单位为秒.在有效期内,不用发出另一条预检请求. 服务器正常请求和回应一旦服务器通过了预检请求,以后每次浏览器正常的CORS请求就会和简单请求一样,会有一个Origin头信息字段,服务器的回应也会有一个Access-Control-Allow-Origin头信息字段.]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CROS</tag>
        <tag>跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux线上环境搭建]]></title>
    <url>%2FJavaLearning%2FOps%2FServerSetting.html</url>
    <content type="text"><![CDATA[服务器环境搭建 Java安装 到官网下载 jdk. 注意要使用带 Auth 的下载链接下载 解压到 tar -zxvf jdk_8u_201 -C /usr/java 编辑环境变量 vim /etc/profile, 使环境变量生效 source /etc/profile 1234export JAVA_HOME=/usr/java/jdk...export JRE_HOME=/usr/java/jdk.../jreexport CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport PATH=$PATH:$GIT_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$JRE_HOME/bin: 检查是否安装成功: java -version 运行打包好的 jar运行 java 程序, 进程不挂起: 1nohup java -jar &#123;path&#125; &amp; OpenJDK安装 OpenJDK 命令 123456789yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-develcat &gt; /etc/profile.d/java8.sh &lt;&lt;EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac)))))export PATH=\$PATH:\$JAVA_HOME/binexport CLASSPATH=.:\$JAVA_HOME/jre/lib:\$JAVA_HOME/lib:\$JAVA_HOME/lib/tools.jarEOFsource /etc/profile.d/java8.sh Mysql安装 到官网Yum 仓库下载 rpm 包. rpm -ivh &lt;文件名&gt;或者 yum localinstall &lt;文件名&gt;安装刚才下载好的包 此时便可以通过 yum安装最新的 mysql. 使用指令 yum install mysql-community-server安装 配置 启动 mysql: service mysqld start/systemctl start mysqld 关闭 mysql: service mysqld stop/systemctl stop mysqld 查看 mysql 提供的初始 root 密码: cat /var/log/mysqld.log | grep password 登录 mysql, 若出现提示 Access denied for user &#39;root&#39;@&#39;localhost&#39;, 可能是密码错了. 修改 root 密码: ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;vnaso943983&#39;; 如果不想使用强密码,需要先降低密码策略等级: SET GLOBAL validate_password.policy=0; 创建用户 CREATE USER &#39;vnaso&#39;@&#39;%&#39; IDENTIFIED BY &#39;vnaso222&#39;;, 并授权: GRANT ALL ON *.* TO &#39;vnaso&#39;@&#39;%&#39; WITH GRANT OPTION; 修改默认字符集. 打开 /etc/my.cnf, 添加如下配置: 1234567891011121314# 对本地的mysql客户端的配置[client]default-character-set = utf8mb4# 对其他远程连接的mysql客户端的配置[mysql]default-character-set = utf8mb4# 本地mysql服务的配置[mysqld]character-set-client-handshake = FALSEcharacter-set-server = utf8mb4collation-server = utf8mb4_unicode_cidefault-time-zone = UTC 查看字符集: SHOW VARIABLES WHERE Variable_name LIKE &#39;character\_set\_%&#39; OR Variable_name LIKE &#39;collation%&#39;; 查看时区: show variables like &#39;%zone%&#39;; 修改时区为 UTC: set time_zone = &#39;utc&#39;;flush privileges;修改时区并刷新. 如果报错 Unknown or incorrect time zone: &#39;UTC&#39;, 在shell执行: mysql_tzinfo_to_sql /usr/share/zoneinfo |mysql -u root mysql -p, 有warning提示 Unable to load xxx 是正常的. 如果修改了之后, 再次查询还是 System, 修改配置文件 /etc/my.cnf, 在 [mysqld] 下添加 default-time-zone = UTC Git安装 到官网获取下载链接. wget &lt;url&gt; 下载 使用 tar -zxvf &lt;文件名&gt; 解压缩. 安装 git 前置依赖: 1yum -y install zlib-devel openssl-devel cpio expat-devel gettext-devel curl-devel perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker 进入 git安装目录下, 进行安装: ./configure --prefix=/usr/local/git-&gt; make &amp;&amp; make install 如果报错 no such file or directory: ./configure. 使用: yum install autoconf然后在目录中键入 autoconf, 再次进行安装即可. 添加环境变量. vim /etc/profile, 使环境变量生效source /etc/profile 检查是否安装成功: git --version 配置 生成私钥: ssh-keygen -t rsa -C &quot;XXX@outlook.com&quot; 密钥名称可自定义 告知系统来管理生成的密钥: ssh-add ~/.ssh/id_rsa 如果报错 could not open a connection to your authentication agent 执行命令: eval `ssh-agent`(是 ~, 而不是单引号). 然后再执行:ssh-add ~/.ssh/id_rsa 在远程服务器中添加公钥: cat ~/.ssh/id_rsa.pub Nginx安装 到官网下载压缩包并解压. 安装 nginx编译文件及库文件 yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel pcre pcre-devel 进入解压后的目录进行安装: ./configure --prefix=/usr/local/nginx-&gt;make &amp;&amp; make install 测试: /usr/local/nginx/sbin/nginx -t 配置参考Nginx配置文件说明 Tomcat安装 到官网下载压缩包, 解压. 一般选择 core 就行 防火墙打开 8080 端口, 启动 tomcat: $CATALINA_HOME/bin/startup.sh 测试: 访问 &lt;服务器地址&gt;:8080 配置 修改默认编码. vim /opt/Develop/apache-tomcat-9.0.16/conf/server.xml. 搜索8080, 在 &lt;Connector&gt;节点中添加 URIEncoding=&quot;UTF-8&quot; 编辑环境变量.vim /etc/profile,添加export CATALINA_HOME = /opt/apache-tomcat-9.0.1.然后source /etc/profile使配置生效 Maven安装 到官网下载压缩包,解压. 添加环境变量:vim /etc/profile,添加export $MAVEN_HOME=....并将$MAVEN_HOME/bin添加到$PATH中 source /etc/profile使配置生效 测试:mvn -v 配置 将下载镜像更换为阿里云中央仓库,解决依赖从境外网站下载过慢的问题 打开maven安装根目录-&gt;conf-&gt;settings.xml 在&lt;settings&gt;标签下找到&lt;mirrors&gt;标签,添加如下代码 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 保存即可 Vstpd安装 下载安装:yum install vstpd 创建接收上传文件的文件夹:mkdir /ftpfile 创建只有上传权限不能登录的用户:useradd ftpuser -d /ftpfile/ -s /sbin/nologin 给创建的ftpuser此用户赋予/ftpfile的权限:chown -R ftpuser.ftpuser /ftpfile/.此时用户名和用户组都是ftpuser 给ftpuser设置密码:passwd ftpuser 编辑配置文件,让上传目录指向之前创建的目录:vim /etc/vsftpd/vsftpd.conf 搜索banner节点,此处配置ftp的欢迎信息.ftpd_banner=xxxxx 可以选择添加use_localtime=yes表示使用服务器时间 添加anonymous_enable=NO关闭匿名用户访问 配置FTP被动模式的端口 pasv_min_port=30000pasv_max_port=30000 ftp配置 12345678910111213141516# 修改为NO，关闭匿名用户访问anonymous_enable=NO# 将所有本地用户限制在自家目录中。chroot_local_user=YES # 设置系统用户FTP主目录local_root=/data# 开启charoot写权限allow_writeable_chroot=YES#配置可以登录ftp的用户目录userlist_deny=NOuserlist_file=/etc/vsftpd/user_list#配置ftp用户访问目录配置目录user_config_dir=/etc/vsftpd/userconfig# 配置FTP被动模式的端口pasv_min_port=30000pasv_max_port=30000 配置ftp用户登录后访问的目录 在/etc/vsftpd目录下新建userconfig目录 在目录下配置用户的登录目录,文件名即对应的用户名 vim /etc/vsftpd/userconfig/ftpuser 在创建的文件中添加local_root=/ftpfile/ftpuser /ftpfile表示对应用户登录ftp时的根目录 注意:路径前不能有空格,否则不识别!!!!会报错unrecognised variable in config file: local_root 重启vsftpd 启动:systemctl start vsftpd.service(service vsftpd start) 停止:systemctl stop vsftpd.service(service vsftpd stop) 重启:systemctl restart vsftpd.service(service vsftpd restart) 打开防火墙,添加21端口和30000端口. 添加端口:firewall-cmd --add-port=21/tcp --permanent,firewall-cmd --add-port=21/tcp --permanent 重启防火墙:firewall-cmd --reload 测试.浏览器打开 ftp://服务器地址 登录.如果只能下载不能上传,请查阅开启关闭SELinux相关信息 Redis 官网下载压缩包, 并解压. 进入解压后的 redis 根目录. 安装 redis: 编译: make 指定安装目录: make PREFIX=/usr/local/redis install 测试: 进入 /usr/local/redis/bin 运行 ./redis-server 另起一个窗口, 进入 /usr/local/redis/bin 运行 ./redis-cli zsh 使用 yum 安装: sudo yum update &amp;&amp; sudo yum -y install zsh 检测是否安装成功: zsh --version 将 shell 切换至 zsh: chsh -s $(which zsh) 重新登录, 检查 shell 是否切换: echo $SHELL 配置 从官网下载插件 12git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestionsgit clone git://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting 插件需要放在 ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/ 目录下, 否则每次重启都需要重新安装插件. 安装插件 进入到下载的插件的根目录下, 分别 source &lt;插件名.zsh&gt;. 进入 zsh 安装目录(默认为 /root/ 下) vim .zshrc 编辑 zsh 配置文件: 1234567891011ZSH_THEME=&quot;agnoster&quot;plugins=( git zsh-autosuggestions zsh-syntax-highlighting ...# 插件名, 需要先下载并安装插件)# 采纳提示组合键bindkey &apos;^ &apos; autosuggest-accept# autojump 支持[[ -s $(brew --prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew --prefix)/etc/profile.d/autojump.sh 为 autosuggestions 的自动提示绑定快捷采纳键, 默认为 →. 替换为 Ctrl + Space. bindkey &#39;^ &#39; autosuggest-accept. 可以通过 cat &gt; /dev/null 来查看组合键的转换序列. 在命令行中执行命令, 只在这次连接中生效. 要永久生效, 需要在 .zshrc 配置文件中添加以上命令. 安装 autojump: 下载完 autojump 后, 进入根目录, 执行 install.py. 然后根据提示, 将 [[ -s /root/.autojump/etc/profile.d/autojump.sh ]] &amp;&amp; source /root/.autojump/etc/profile.d/autojump.sh 添加到 .zshrc中. swap交换缓存创建 创建swap文件:sudo fallocate -l 4G /swapfile(在/下创建一个大小为4G的文件swapfile) 授权swap文件:chmod 600 /swapfile(该文件的读写只能root操作) 告知系统将文件用于swap:mkswap /swapfile 启用swap文件:swapon /swapfile 确认:free 至此已经在系统中启用了swap交换缓存,但是一旦系统重启后,服务器还不能自动启用该文件. 使swap永久生效 打开文件nano /etc/fstab,在文件末尾添加/swapfile swap swap sw 0 0 ^X表示Ctrl + X,按下选择yes保存退出. Docker安装 安装相关依赖. 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息 1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新并安装 Docker-CE 1sudo yum -y install docker-ce 开启 Docker. 1sudo service docker start 检测是否安装成功: docker version. 配置使用阿里云镜像加速 登录阿里云控制台. 选择容器镜像服务. 开启该服务后, 镜像中心 -&gt; 镜像加速器. 参照下方操作文档添加镜像加速. 安装 MySQL 拉取镜像. 1docker pull mysql 运行 Mysql. 1docker run --name mysqltest -e MYSQL_ROOT_PASSWORD=123456 -d -p 3306:3306 --restart=always -v ~/app/mysql-docker/:/data/mysql mysql --name mysqltest: 将容器命名为 mysql. -e: MYSQL_ROOT_PASSWORD=123456: 设置 root 账户密码. -d: 容器在后台运行, 并返回容器 ID. -p: 端口映射, 主机端口:容器端口. --restart=always: 当 docker 重启时, 该容器自动重启. -v ~/app/mysql-docker/:/data/mysql: 挂载数据卷, 主机绝对路径:容器路径. 建议挂载 空文件夹:空文件夹, 否则可能出现各种情况. 参考:https://segmentfault.com/a/1190000015684472, https://www.jianshu.com/p/e605de64e9f9 进入 MySQL 容器. 1docker exec -ti mysqltest bash 在挂载的目录下创建 custom.cnf, 添加相关 mysql 配置后, 在 MySQL 容器中对应的目录下将 custom.cnf 拷贝到 /etc/mysql/conf.d 下. RabbitMQ安装通过 Docker 安装 拉取镜像. 1docker pull rabbitmq:3.3.7-management 注意: 带有 management 的版本才有后台管理. 后台运行容器. 1docker run -d --name rabbitmq -p 5672:5672 -p 15672:15672 -v ~/app/rabbitmq/data:/var/lib/rabbitmq --hostname rabbitNo1 -e RABBITMQ_DEFAULT_VHOST=my_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin rabbitmq:3.7.7-management 5672 为运行 rabbitmq 运行端口, 15672 为后台管理界面端口. -e RABBITMQ_DEFAULT_VHOST: 改变默认的 vhost. -e RABBITMQ_DEFAULT_USER: 设置登录名. -e RABBITMQ_DEFAULT_PASS=admin: 设置登录密码. 访问 {IP}:15672 验证是否启动成功. 环境变量配置在腾讯云服务器中的配置 1234567export CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport PATH=$PATH:$GIT_HOME/bin:$MAVEN_HOME/bin:$JAVA_HOME/bin:$JRE_HOME/bin:export GIT_HOME=/usr/local/gitexport MAVEN_HOME=/opt/Develop/apache-maven-3.6.0export JAVA_HOME=/usr/java/jdk1.8.0_201export JRE_HOME=/usr/java/jdk1.8.0_201/jreexport CATALINA_HOME=/opt/Develop/apache-tomcat-9.0.16]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记]]></title>
    <url>%2FJavaLearning%2FOps%2FLinux.html</url>
    <content type="text"><![CDATA[Linux学习笔记系统命令tar 解压解压到指定目录:tar -zxvf 要解压的文件 -C 目的路径 tar 命令参数 -x或–extract或–get:从备份文件中还原文件 -Z或–compress或–uncompress:通过compress指令处理备份文件 -f&lt;备份文件&gt;或–file=&lt;备份文件&gt;:指定备份文件 -v:显示操作过程 -C &lt;目录&gt;:这个选项用在解压缩,若要在特定目录解压缩,可以使用这个选项 防火墙(使用firewalld)开启关闭重启:service firewalld start/stop/restart 查看运行状态:service firewalld status/sudo firewall-cmd --state 查看防火墙开放的端口:sudo firewall-cmd --list-ports 删除端口:firewall-cmd --zone=分区名 --remove-port=端口号/通讯协议 --permanent 为防火墙添加端口:sudo firewall-cmd --zone=分区名 --add-port=端口号/通讯协议 --permanent(永久生效) 防火墙分为多个区域,可用sudo firewall-cmd --get-zones查看,默认是public 软件安装查看软件是否安装: rom -qa | grep 软件名&gt; 下载安装软件: yum install 软件名1,2,3.../yum -y install 软件名1,2,3... 安装rpm软件包: rpm -ivh 软件包名 卸载软件: rpm -e 软件包名/yum remove 软件包名 rpm 命令参数 -i: 安装过程中显示正在安装的文件信息 -h: 安装过程中显示安装进度 服务进程查看服务进程: ps -ef | grep &lt;进程名&gt; 修改权限为文件添加可执行权限: chmod +x file.name,可以用通配符*.后缀名 查看文件查看文件最后n行: tail -n &lt;行数&gt; &lt;文件名&gt; 动态查看文件最后n行: tail -n &lt;行数&gt; -f &lt;文件名&gt;,使用ctrl+c来结束 创建文件创建指定大小文件: fallocate -l &lt;文件大小&gt; &lt;文件名&gt; 例:sudo fallocate -l 4G /swapfile(在/下创建一个大小为4G的文件swapfile) 查看系统状态查看内存使用情况: free free 命令参数 -m:以MB为单位显示.默认单位为KB.向下取整 用户操作创建用户: useradd 用户名或adduser 用户名.后者创建的用户会自动创建主目录,系统shell版本和创建时输入密码,而前者没有. 修改用户密码: passwd 用户名 删除用户: userdel 用户名 下载下载: wget 下载地址 下载并指定名称: wget -O 文件名称 下载地址 查找查找文件: find &lt;path&gt; -name &quot;&lt;文件名&gt;&quot;. 必须有双引号, 可以使用通配符来模糊查找. -iname - 忽略大小写. vim 搜索 在 normal 模式下按下 / 即可进入查找模式, 输入要查找的字符串并按下回车. Vim 会跳转到第一个匹配. 按下 n查找下一个, 按下 N查找上一个. 注意查找回车应当用 \n,替换回车应当用 \r vim默认采用大小写敏感.在查找模式中加入 \c 表示大小写不敏感 ,\C敏感. 问题解决Linux Terminal 提示符显示 bash-4.2用 root 用户登录, 发现提示符显示的不是 root@主机名+路径, 原因是家目录下配置文件丢失. .bash_profile 和 .bashrc 这两个文件是用户必须的配置文件. 解决方案: 从主默认文件重新拷贝一份配置信息 12cp /etc/skel/.bashrc /root/cp /etc/skel/.bash_profile /root]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记]]></title>
    <url>%2FJavaLearning%2FOps%2FNginx.html</url>
    <content type="text"><![CDATA[Nginx配置 在···/nginx/conf目录下的nginx.conf 123456789101112131415161718192021222324252627282930313233# 全局块... # events块events &#123; ...&#125;# http块http &#123; # http全局块 ... # 虚拟主机server块 server &#123; # server全局块 ... # location块 location [PATTERN] &#123; ... &#125; location [PATTERN] &#123; ... &#125; &#125; server &#123; ... &#125; # http全局块 ... &#125; 全局块 配置影响nginx全局的指令.一般有运行nginx服务器的用户组,nginx进程pid存放路径,日志存放路径,配置文件引入,允许生成worker process数等. 12345########### 每个指令必须有分号结束。##################user administrator administrators; #配置用户或者组，默认为nobody nobody。#worker_processes 2; #允许生成的进程数，默认为1#pid /nginx/pid/nginx.pid; #指定nginx进程运行文件存放地址error_log log/error.log debug; #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg events块 配置影响nginx服务器或与用户的网络连接.有每个进程的最大连接数,选取哪种事件驱动模型处理连接请求,是否允许同时接受多个网路连接,开启多个网络连接序列化等. 123456events &#123; accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off #use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #每个worker process最大连接数&#125; http块 可以嵌套多个server,配置代理,缓存,日志定义等绝大多数功能和第三方模块的配置.如文件引入,mime-type定义,日志自定义,是否使用sendfile传输文件,连接超时时间,单连接请求数等. 1234567891011121314151617http &#123; include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型，默认为text/plain #access_log off; #取消服务日志 log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式 access_log log/access.log myFormat; #combined为日志格式的默认值 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 keepalive_timeout 65; #连接超时时间，默认为75s，可以在http，server，location块。 # 定义常量 # 负载均衡,按权重或规则将请求转发到某个服务器 upstream mysvr &#123; server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 &#125; error_page 404 https://www.baidu.com; #请求失败时跳转的错误页 server块 配置虚拟主机的相关参数,一个http中可以有多个server. 并且可以过滤有人恶意将某些域名指向自己的主机服务器. 123456#定义某个负载均衡服务器 server &#123; keepalive_requests 120; #单连接请求上限次数。 listen 4545; #监听端口 server_name 127.0.0.1; #监听地址 &#125; server_name server_name是监听你的HTTP请求头中的host. 默认情况下，Nginx 允许直接以 IP 的方式就能直接访问到网站，或者通过未设置的域名访问(比如有人把他自己的域名指向了你的服务器 IP),可通过如下设置进行防护 例如: 1234567891011121314151617181920&gt; server &#123;&gt; listen 80 default_server;&gt; server_name _;&gt; return 444; # 过滤其他域名的请求，返回444状态码&gt; &#125;&gt; server &#123;&gt; listen 80;&gt; server_name www.aaa.com; # www.aaa.com域名&gt; location / &#123;&gt; proxy_pass http://localhost:8080; # 对应端口号8080&gt; &#125;&gt; &#125;&gt; server &#123;&gt; listen 80;&gt; server_name www.bbb.com; # www.bbb.com域名&gt; location / &#123;&gt; proxy_pass http://localhost:8081; # 对应端口号8081&gt; &#125;&gt; &#125;&gt; server_name的值为www.aaa.com.在浏览器中输入www.aaa.com,那么匹配到了对应server_name,请求就会被转发到http://localhost:8080.这里www.aaa.com和www.bbb.com都绑定到了服务器 而对于未绑定的域名指向服务器时,匹配不到配置的虚拟主机域名,就会使用默认的虚拟主机,然会返回444. listen 80 default_server:指定该server配置段为80端口的默认主机，即对于未绑定的域名指向你的服务器时,匹配不到你配置的虚拟主机域名后,会默认使用这个虚拟主机 server_name _:此处的_可以换成任意其他无效字符或无效的域名,表示该server配置不会被正常访问到 access_log:日志 格式:access_log logs/aaa.access.log main-日志类型 日志存放路径 日志格式 location块详细参考-知否 详细参考-本地 nginx跨域代理的一些设置 配置请求的路由,以及各种页面的处理情况 1234567location ~*^.+$ &#123; #请求的url过滤,正则匹配 #root path; #根目录 #index vv.txt; #设置默认页 proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip &#125; 通配符 =表示精确匹配.只有请求的url路径与后面的字符串完全相等时,才会命中 ~表示该规则是使用正则定义的,区分大小写 ~*表示该规则是使用正则定义的,不区分大小写 ^~表示如果该符号后面的字符是最佳匹配,采用该规则,不再进行后续的查找 属性解释 root表示url匹配上了此location定义的正则后,就将这个url映射到定义的根目录. 例如 location /user/img,root /usr/local/resource;. 那么如果请求为http://www.aaa.com/user/img/aaa.jpg,/user/img对应/usr/local/resource.该请求得到的结果就是/usr/local/resource下的aaa.jpg index就是设置默认的欢迎页面.页面需要存在于定义的root目录下. 接上例index index.html;. 那么如果请求为http://www.aaa.com/user/img/,请求的结果就是/usr/local/resource/user/img下的index.html,有点类似于相对路径.而如果要让index.html为/usr/local/resource/路径下的index.html的话,需要把root换为alias,具体参照root和alias的区别. 注意unknown directive所有关键词后都必须加一个空格,否则会报错unknown directive xxx Linux下命令 在···/nginx/sbin/目录下 启动nginx:nginx -s start 停止nginx:nginx -s stop 重启nginx:nginx -s reload 测试配置:nginx -t Nginx日志目录 在···/nginx/logs/目录下 目录下通常存放有access.log,error.log以及nginx.pid. nginx.pid中保存着nginx的进程号,可以通过查看此文件获取进程号来kill nginx. root和alias的区别root 示例11234location / &#123; root /data/www/; index index.html;&#125; 请求http://example.com这个地址,那么在服务器中真实对应的地址为/data/www/index.html 请求 真实地址 http://example.com /data/www/index.html http://example.com/a.png /data/www/a.png root 示例21234location /aaa/ &#123; root /data/www/; index index.html;&#125; 请求 真实地址 http://example.com/aaa/ /data/www/aaa/indexhtml http://example.com/aaa/a.gif /data/www/aaa/a.gif alias 示例11234location /&#123; alias /data/www/; index index.html;&#125; 请求 真实地址 http://example.com /data/www/index.html http://example.com/b.jpg /data/www/a.jpg alias 示例21234location /bbb/&#123; alias /data/www/; index index.html;&#125; 请求 真实地址 http://example.com/bbb/ /data/www/index.html http://example.com/bbb/b.jpg /data/www/b.jpg 对比上面4个示例可以发现,使用root的真实地址是root+location+(文件名),而使用alias的真实地址是alias+(文件名). 注意 alias只能作用在location中,而root可以存在server,http,location. alias后面必须要用/结束. Nginx 配置静态资源目录12345678910111213server &#123; listen 8082; server_name localhost; location / &#123; # 设置资源根目录 root /root/app/project/upload/files; # 设置默认欢迎页 index index.html; # 目录结构 autoindex on; autoindex_localtime on; &#125;&#125; 如果访问资源显示 403, 则在 nginx.conf 文件中找到被注释的 #user nobody, 添加一行 user root 即可. Nginx实现解决前后端分离的跨域问题参考 在前端使用ajax向从后端获取数据发送跨域请求,由于浏览器的同源策略限制,会报403错误. 同源策略 同源是指”协议+域名+端口”三者相同,即便两个不同的域名指向同一个ip地址,也非同源 同源策略限制行为 Cookie,LocalStorage和IndexDB无法读取 DOM和JS对象无法获得 Ajax请求不能发送 常见的跨域场景 URL 说明 是否允许通信 http://www.domain.com/a.jshttp://www.domain.com/b.jshttp://www.domain.com/lab/c.js 同一域名,不同文件或路径 允许 http://www.domain.com:8000/a.js http://www.domain.com/b.js 同一域名,不同端口 不允许 http://www.domain.com/a.jshttps://www.domain.com/b.js 同一域名,不同协议 不允许 http://www.domain.com/a.jshttp://192.168.4.12/b.js 域名和域名对应相同ip 不允许 http://www.domain.com/a.jshttp://x.domain.com/b.jshttp://domain.com/c.js 主域相同,子域不同 不允许(cookie这种情况下也不允许访问) http://www.domain1.com/a.jshttp://www.domain2.com/b.js 不同域名 不允许 跨域解决方案Nginx代理跨域 一种比较简单的解决方案,无需动后端代码 Nginx配置123456789101112131415161718192021222324252627server &#123; listen 80; server_name vnaso.live; location / &#123; if ($http_origin ~* (http://vnaso\.live$) ) &#123; add_header Access-Control-Allow-Origin '$http_origin'; add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; add_header 'Access-Control-Allow-Credentials' 'true'; &#125; if ($http_origin ~* (http://127.0.0.1:8080$) ) &#123; add_header Access-Control-Allow-Origin '$http_origin'; add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; add_header 'Access-Control-Allow-Credentials' 'true'; &#125; proxy_pass http://127.0.0.1:8080/; #proxy_cookie_path / /; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; root /opt/Develop/apache-tomcat-9.0.16/webapps/ROOT/; # index index.html; &#125;&#125; if语句来决定匹配跨域请求,并为这些请求添加RequestHeader,上面配置的add_header Access-Control-Allow-Origin:服务器默认是不允许跨域的,这里可以添加接受跨域的请求源(origin). Access-Control-Allow-Methods:设置允许接收哪些方法的跨域请求 Access-Control-Allow-Headers:不添加此属性会报错:Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response.原因是当前的Content-Type不被支持.详情看下方的预检请求. Access-Control-Allow-Credentials:表示是否允许发送 Cookie, 默认情况下, Cookis 不包括在 CORS 请求之中. 不设置为 true, 客户端不会收到服务器发送的 Cookie, 可能会客户端获取不到 session. proxy_pass:表示将匹配的请求转发到该url下 proxy_set_header:为转发的请求添加RequestHeader 相关知识CORS]]></content>
      <categories>
        <category>Tool</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logback学习笔记]]></title>
    <url>%2FJavaLearning%2FJava%2FFramework%2Flogback.html</url>
    <content type="text"><![CDATA[Logback使用标签属性介绍根标签 - configuration属性 scan:布尔值.表示是否自动扫描logback.xml的文本变化. scanPeriod:字符串.表示每间隔多长时间对logback.xml进行扫描.格式:数字 + 时间单位(如:seconds,minutes) debug:布尔值.表示是否打印logback内部的日志信息. 常用配置为:&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt;&lt;/configuration&gt; 表示自动每60秒扫描一次logback.xml的配置有无变化,如果有就更新,且不打印logback的内部日志.这样可以热更新logback配置文件 子标签 &lt;property&gt;:定义参数常量 &lt;appender: &lt;root&gt;: configuration - property属性 name:变量名 value:变量值 常用来设置一些配置文件中需要使用的常量值,如默认日志等级,最大保存天数,日志存储位置等. configuration - appender属性 name:变量名 class:具体的实现类的全限定类名 用来定义日志的输出源的配置 class的取值一般有两个:ch.qos.logback.core.ConsoleAppender和ch.qos.logback.core.rolling.RollingFileAppender 前者的功能是在输出到控制台,后者功能可以按时间分卷输出到文件 子标签 &lt;encoder&gt;:把日志转为字符串并将其输出到文件中 &lt;encoding&gt;:编码方式 &lt;filter&gt;:过滤器 &lt;file&gt;:日志文件输出储存位置 &lt;rollingPolicy&gt;:分卷模式 configuration - appender - encoder子标签 &lt;pattern&gt;:日志格式 configuration - appender - rollingPolicy属性 class:分卷模式的全限定类名 &lt;append&gt;:布尔值.日志被追加到文件结尾.如果是false,清空现存文件,默认是true 用来定义日志的分卷模式 class常用的类是ch.qos.logback.core.rolling.TimeBasedRollingPolicy 可以按照时间进行分卷 子标签 &lt;filenamePattern&gt;:分卷日志文件名格式 &lt;MaxHistory&gt;:日志最大保存天数 configuration - appender - filter属性 class:实现过滤规则的类的全限定类名 子标签 &lt;level&gt;:要进行过滤的级别 &lt;onMatch&gt;:等于level属性值时的操作.有NEUTRAL(有序列表里的下个过滤器过接着处理日志),ACCEPT和DENY可选 &lt;onMismatch&gt;:不等于时的操作,类上 用来定义输出源要过滤的日志等级 示例: 123456&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt;&gt; &lt;level&gt;ERROR&lt;/level&gt;&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;&gt; &lt;/filter&gt;&gt; 以上配置表示只保留Error等级的日志信息 ch.qos.logback.classic.filter.ThresholdFilter: 临界值过滤器, 过滤掉低于指定临界值的日志. 当日志级别高于或等于临界值时, 过滤器返回 NEUTRAL, 否则拒绝. 1234&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;&gt; &lt;level&gt;INFO&lt;/level&gt;&gt; &lt;/filter&gt;&gt; configuration - logger属性 name:指定为哪个包或类添加appender level:指定为哪个级别及以上记录日志,如果不指定,则默认继承&lt;root&gt;下的level additivity:是否向上级传递打印信息.默认值为true 子标签 &lt;appender-ref&gt;:为name表示的包或类指定appender.用属性ref来指定 用来定义包或类的日志打印级别及指定appender 示例: 123456&gt; &lt;logger name="com.vnaso" additivity="false" level="INFO"&gt;&gt; &lt;!-- 设置日志输出 --&gt;&gt; &lt;appender-ref ref="vnasoRss"/&gt;&gt; &lt;appender-ref ref="console"/&gt;&gt; &lt;/logger&gt;&gt; 表示为com.vnaso这个包及下面所有类指定name为vnasoRss和console的appender来记录INFO及以上的日志,且不向上继承. Appender是绑定在logger上的,而logger又有继承关系,因此一个logger打印信息时的目的地Appender需要参考它的父亲和祖先.在logback中,默认情况下,如果一个logger打印一条信息,那么这条信息首先会打印至它自己的Appender,然后打印至它的父亲和父亲以上的祖先的Appender,但如果它的父亲设置了 additivity = false,那么这个logger除了打印至它自己的Appender外,只会打印至其父亲的Appender,因为它的父亲的additivity 属性置为了false,开始变得忘祖忘宗了,所以这个logger只认它父亲的Appender;此外,对于这个logger的父亲来说,如果父亲的logger打印一条信息,那么它只会打印至自己的Appender中(如果有的话),因为父亲已经忘记了爷爷及爷爷以上的那些父辈了. configuration - root属性 level(only):指定为哪个级别及以上记录日志 子标签 &lt;appender-ref&gt;:为name表示的包或类指定appender.用属性ref来指定 &lt;root&gt;标签是一种特殊的logger,但是它不能特别指定包或类.也就是说,它只能够接收继承了它且additivity值为true的logger传来的打印信息,具体查看继承关系 继承关系 继承关系是通过logger的name属性来实现的. 示例 &lt;root&gt;&gt;com.vnaso&gt;com.vnaso.controller&gt;com.vnaso.controller.UserController.java 以上对应继承关系:祖&gt;爷&gt;父&gt;子 level 继承关系 appender 继承关系 相关知识如果程序运行在 Tomcat 服务器上, 可以利用 ${catalina.base} 来设置 log 文件持久化的根目录, 如: &lt;property name=&quot;log.filePath&quot; value=&quot;${catalina.base}/...&quot;/&gt;. Tomcat 有 ${catalina.base} 和 ${catalina.home} 两个变量, 容易混淆, 这里说一下区别. ${catalina.home} home 一般指的是 Tomcat 的安装目录, 即存放 bin 和 lib 这些包含创建 Tomcat 实例所必要的文件的目录. ${catalina.base} base 一般指的是 Tomcat 的工作目录, 即存放 conf, logs, temp, webapps, work 这些保存和运行程序有关的文件的目录. 一般来说, 在普通单实例单应用的情况下, 这两者是没有区别的. 但如果启动多个实例时, home 相同, 但是 base 是根据实例而不同的. 因此, 可以通过设置多个 base 目录, 包含必要的那些文件夹, 修改监听的端口, 就可以启动多个 Tomcat. 详见: https://www.cnblogs.com/mafly/p/tomcat.html. Tomcat 安装目录下有: bin: 存放一些脚本文件, 比如: startup.sh 和 shutdown.sh. conf: 存放配置文件, 比如: server.xml 和 web.xml. lib: 存放 Tomcat 依赖的包. logs: 存放运行时产生的日志文件. temp: 存放运行时产生的临时文件. webapps: 部署 Web 应用程序的默认目录, 也就是 war 包所在默认目录. work: 存放由 JSP 文件生成的 servlet. 整合 Spring Boot引入依赖在 pom.xml 中添加: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;&lt;/dependency&gt; 在 Spring Boot 中有一些 starter 已经依赖了 logging, 则不需显式添加依赖. 如 spring-boot-starter-web, spring-boot-starter-aop. 配置 logback-spring.xml 文件名末尾添加 -spring 可以使用 &lt;springProfile&gt; 标签来配置不同环境日志策略. 示例配置: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- 每 60 seconds 进行一次 scan, 不开启 logback 的内部 debug --&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;!-- 引入 Spring Boot 默认的一些设置 --&gt; &lt;include resource="org/springframework/boot/logging/logback/defaults.xml"/&gt; &lt;!-- 引入 Spring Boot 的参数 --&gt; &lt;!-- FIXME 日志目录 --&gt; &lt;springProperty scope="context" name="logback.path" source="logging.path" defaultValue="logs"/&gt; &lt;!-- 定义参数常量 --&gt; &lt;!-- 日志最大保存天数 --&gt; &lt;property name="log.maxHistory" value="30"/&gt; &lt;property name="log.filePath" value="$&#123;logback.path&#125;"/&gt; &lt;!-- 日志格式 --&gt; &lt;property name="log.pattern" value="===[%d&#123;HH:mm:ss.SSS&#125;][%p][%c&#123;40&#125;][%t]=== - %m%n"/&gt; &lt;!-- 单日志最大大小 --&gt; &lt;property name="log.maxFileSize" value="10MB" /&gt; &lt;!-- 异步存储文件阻塞队列最大处理 event 数量 --&gt; &lt;property name="log.queueSize" value="512" /&gt; &lt;!-- 控制台输出配置 DEBUG --&gt; &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;log.pattern&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 项目输出配置 DEBUG --&gt; &lt;appender name="DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- FIXME 日志名 --&gt; &lt;!-- 正在记录的日志文件名 --&gt; &lt;File&gt;$&#123;log.filePath&#125;/debug.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!-- 日志归档, 索引 i 从 0 开始 --&gt; &lt;fileNamePattern&gt;$&#123;log.filePath&#125;/debug-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;!-- 控制单日志最大大小 --&gt; &lt;maxFileSize&gt;$&#123;log.maxFileSize&#125;&lt;/maxFileSize&gt; &lt;maxHistory&gt;$&#123;log.maxHistory&#125;&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;log.pattern&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 指定日志界别过滤器, 不匹配的直接拒绝 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;debug&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 项目输出配置 INFO --&gt; &lt;appender name="INFO_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- FIXME 日志名 --&gt; &lt;!-- 正在记录的日志文件名 --&gt; &lt;File&gt;$&#123;log.filePath&#125;/info.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!-- 日志归档, 索引 i 从 0 开始 --&gt; &lt;fileNamePattern&gt;$&#123;log.filePath&#125;/info-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;!-- 控制单日志最大大小 --&gt; &lt;maxFileSize&gt;$&#123;log.maxFileSize&#125;&lt;/maxFileSize&gt; &lt;maxHistory&gt;$&#123;log.maxHistory&#125;&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;log.pattern&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 指定日志界别过滤器, 不匹配的直接拒绝 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;info&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 项目日志输出配置 ERROR --&gt; &lt;appender name="FILE_ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- FIXME 日志名--&gt; &lt;!-- 正在记录的日志文件名 --&gt; &lt;File&gt;$&#123;log.filePath&#125;/error.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;log.filePath&#125;/error.%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;$&#123;log.maxHistory&#125;&lt;/maxHistory&gt; &lt;maxFileSize&gt;$&#123;log.maxFileSize&#125;&lt;/maxFileSize&gt; &lt;/rollingPolicy&gt; &lt;append&gt;true&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;log.pattern&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 开发环境 profile 配置 --&gt; &lt;springProfile name="dev"&gt; &lt;!-- FIXME 数据库日志打印 --&gt; &lt;!--&lt;logger name="" level="DEBUG"/&gt;--&gt; &lt;logger name="cn.cdutacm.onlinejudge.mapper" level="DEBUG"/&gt; &lt;logger name="cn.cdutacm.onlinejudge" level="INFO"/&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="CONSOLE"/&gt; &lt;appender-ref ref="INFO_FILE"/&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;!-- 测试环境 profile 配置 --&gt; &lt;springProfile name="beta"&gt; &lt;!-- FIXME 数据库日志打印 --&gt; &lt;!--&lt;logger name="" level="DEBUG"/&gt;--&gt; &lt;logger name="cn.cdutacm.onlinejudge.mapper" level="DEBUG"/&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="CONSOLE"/&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;!-- 生产环境 profile 配置 --&gt; &lt;springProfile name="prod"&gt; &lt;!-- FIXME 其他需要记录日志的 logger --&gt; &lt;!--&lt;logger name="" level="INFO" additivity="false"&gt;--&gt; &lt;!-- &lt;appender-ref ref="FILE_INFO"/&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;root level="info"&gt; &lt;appender-ref ref="FILE_ERROR"/&gt; &lt;/root&gt; &lt;/springProfile&gt; &lt;/configuration&gt; 如果需要引用 application.yml 文件中的属性, 可以使用: 1&lt;springProperty scope="context" name="&#123;PROPERTY_NAME&#125;" source="&#123;PROPERTY_KEY&#125;" defaultValue="&#123;DEFAULT_VALUE&#125;"/&gt; 设置日志打印颜色: 格式: %color(日志内容), 可以识别的颜色有: black, red, green, yellow, blue, magenta, cyan, white, gray, bold*, highlight. Grouping by parentheses as explained above allows coloring of sub-patterns. As of version 1.0.5, PatternLayout recognizes “%black”, “%red”, “%green”,”%yellow”,”%blue”, “%magenta”,”%cyan”, “%white”, “%gray”, “%boldRed”,”%boldGreen”, “%boldYellow”, “%boldBlue”, “%boldMagenta””%boldCyan”, “%boldWhite” and “%highlight” as conversion words. These conversion words are intended to contain a sub-pattern. Any sub-pattern enclosed by a coloring word will be output in the specified color.]]></content>
      <categories>
        <category>Java</category>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Logback</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven学习笔记]]></title>
    <url>%2FJavaLearning%2FOps%2FMaven.html</url>
    <content type="text"><![CDATA[Maven使用Maven安装与配置下载与安装Maven官网下载地址 在如下位置点击下载最新版本Maven的压缩包 解压文件得到apache-maven-xxx文件夹.xxx为版本号 新建环境变量MAVEN_HOME,变量值为maven根目录地址 打开环境变量设置的步骤为: 右击此电脑-&gt;属性-&gt;高级系统设置-&gt;高级-&gt;环境变量 Win+R-&gt;sysdm.cpl-&gt;高级-&gt;环境变量 编辑环境变量path,添加%MAVEN_HOME%\bin\ 至此,maven安装完毕.打开命令提示符窗口(Win+R-&gt;cmd),输入mvn -v查询maven版本,检查maven是否成功安装.成功如图所示 配置创建maven仓库 如果不自行创建,maven将默认使用Default: ${user.home}/.m2/repository作为本地仓库 在想要作为maven仓库的地方创建文件夹maven-repository作为maven的本地仓库 打开maven安装根目录-&gt;conf-&gt;settings.xml 在&lt;settings&gt;标签下,找到&lt;localRepository&gt;标签.如果没有则自行添加. 修改值为本地仓库的地址,如&lt;localRepository&gt;C:\Maven\apache-maven-3.6.0\maven-repository&lt;/localRepository&gt; 修改下载镜像地址 将下载镜像更换为阿里云中央仓库,解决依赖从境外网站下载过慢的问题 打开maven安装根目录-&gt;conf-&gt;settings.xml 在&lt;settings&gt;标签下找到&lt;mirrors&gt;标签,添加如下代码 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 保存即可 针对Maven添加依赖时没有自动提示的解决方案 到Maven仓库官网搜索,点击需要添加的依赖.复制&lt;dependency&gt;标签即可. Maven仓库官网 在IDEA中配置Maven 打开IDEA,settings-&gt;Build,Execution,Deployment-&gt;Build Tools-&gt;Maven 修改Maven home directory为maven安装根目录 勾选User settings file后的Override,然后修改为安装根目录-&gt;conf-&gt;settings.xml 勾选Local repository后的Override,然后修改为本地的repository地址 Maven项目结构 项目根目录/ src/ main/ 项目主体根目录 java 源代码目录 resources 所需资源目录 filters 资源过滤文件目录 assembly 组件的描述配置(如何打包) config 配置文件 wepapp web应用的目录.WEB-INF,css,js等 test/ 项目测试目录根 java 单元测试java源代码文件 resources 测试需要用的资源库 filters c测试资源过滤库 site Site一些文档 target/ 存放项目构建后的文件和目录.jar,war,编译的class文件等 pom.xml MMaven的pom文件 LICENSE.TXT 项目的LISENCE README.TXT 项目的README Maven生命周期 Maven生命周期执行顺序从上至下 每执行当前指令之前,会把之前(上方)的指令都执行一次 每执行当前周期之前会执行之前的生命周期 clean生命周期 清理项目 clean 描述 pre-clean 执行清理前需要完成的工作execute processes needed prior to the actual project cleaning clean 清理上一次构建生成的文件remove all files generated by the previous build post-clean 执行清理后需要完成的工作execute processes needed to finalize the project cleaning default生命周期 构建项目 default 描述 validate 验证工程是否正确,所需要的资源是否可用validate the project is correct and all necessary information is available compile 编译项目的源代码compile the source code of the project test 使用已编译的测试代码,测试已编译的源代码test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed package 采用编译的代码,并以其可分配格式(如JAR)进行打包take the compiled code and package it in its distributable format, such as a JAR. verify 运行所有检查,验证包是否有效且达到质量标准run any checks on results of integration tests to ensure quality criteria are met install 把包安装在本地的repository中,可以被其他工程作为依赖来使用install the package into the local repository, for use as a dependency in other projects locally deploy 在整合或者发布环境下执行,将最终版本的包拷贝到远程的repository,使得其他的开发者或者工程可以共享done in the build environment, copies the final package to the remote repository for sharing with other developers and projects. site生命周期 建立和发布项目站点 site 描述 pre-site 生成项目站点之前需要完成的工作execute processes needed prior to the actual project site generation site 生成项目站点文档generate the project’s site documentation post-site 生成项目站点之后需要完成的工作execute processes needed to finalize the site generation, and to prepare for site deployment site-deploy 将项目站点发布到服务器deploy the generated site documentation to the specified web server Maven默认生命周期 Default Lifecycle 生命周期阶段 描述 validate 验证 确保当前配置和 POM 的内容是有效的。这包含对 pom.xml 文件树的验证。 initialize 初始化 在执行构建生命周期的主任务之前可以进行初始化。 generate-sources 生成源码 代码生成器可以开始生成在以后阶段中处理或编译的源代码。 process-sources 处理源码 提供解析、修改和转换源码。常规源码和生成的源码都可以在这里处理。 generate-resources 生成资源 可以生成非源码资源。通常包括元数据文件和配置文件。 process-resources 处理资源 处理非源码资源。修改、转换和重定位资源都能在这阶段发生。 compile 编译 编译源码。编译过的类被放到目标目录树中。 process-classes 处理类 处理类文件转换和增强步骤。字节码交织器和常用工具常在这一阶段操作。 generate-test-sources 生成测试源码 mojo 可以生成要操作的单元测试代码。 process-test-sources 处理测试源码 在编译前对测试源码执行任何必要的处理。在这一阶段，可以修改、转换或复制源代码。 generate-test-resources 生成测试资源 允许生成与测试相关的（非源码）资源。 process-test-resources 处理测试资源 可以处理、转换和重新定位与测试相关的资源。 test-compile 测试编译 编译单元测试的源码。 process-test-classes 对测试编译生成的文件做后期处理(需Maven2.0.5及以上) test 测试 运行编译过的单元测试并累计结果。 prepare-package 执行打包前的所有操作(需Maven2.1及以上) package 打包 将可执行的二进制文件打包到一个分布式归档文件中，如 JAR 或 WAR。 pre-integration-test 前集成测试 准备集成测试。这种情况下的集成测试是指在一个受到一定控制的模拟的真 实部署环境中测试代码。这一步能将归档文件部署到一个服务器上执行。 integration-test 集成测试 执行真正的集成测试。 post-integration-test 后集成测试 解除集成测试准备。这一步涉及测试环境重置或重新初始化。 verify 检验 检验可部署归档的有效性和完整性。过了这个阶段，将安装该归档。 install 安装 将该归档添加到本地 Maven 目录。这一步让其他可能依赖该归档的模块可以使用它。 deploy 部署 将该归档添加到远程 Maven 目录。这一步让这个工件能为更多的人所用。 Maven环境隔离配置 在pom.xml中的&lt;build&gt;节点下插入&lt;resources&gt;节点,在其中添加&lt;resource&gt;节点来声明要进行环境分离的resource目录.然后在pom.xml中&lt;project&gt;节点下(与&lt;build&gt;同级)增加&lt;profiles&gt;节点,通过添加&lt;profile&gt;节点来配置不同环境. 在&lt;build&gt;节点下插入&lt;resources&gt;节点 1234567891011121314&lt;resources&gt; &lt;!-- 设置resources资源文件夹目录 --&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources.$&#123;deploy.type&#125;&lt;/directory&gt; &lt;!-- 排除指定文件 --&gt; &lt;excludes&gt; &lt;exclude&gt;*.jsp&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;!-- 设置resources资源文件夹目录 --&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt;&lt;/resources&gt; 在&lt;project&gt;节点下插入&lt;profiles&gt;节点,并配置各个环境的环境标识及其他属性 123456789101112131415161718192021222324252627282930&lt;profiles&gt; &lt;!-- dev环境 --&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;!-- 设置默认使用该环境 --&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;!-- 设置环境标识 --&gt; &lt;properties&gt; &lt;deploy.type&gt;dev&lt;/deploy.type&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;!-- beta环境 --&gt; &lt;profile&gt; &lt;id&gt;beta&lt;/id&gt; &lt;!-- 设置环境标识 --&gt; &lt;properties&gt; &lt;deploy.type&gt;beta&lt;/deploy.type&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;!-- product环境 --&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;!-- 设置环境标识 --&gt; &lt;properties&gt; &lt;deploy.type&gt;prod&lt;/deploy.type&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 创建resources.&lt;环境标识&gt;目录,这样便可结合&lt;resource&gt;中定义的${deploy.type}和&lt;profile&gt;中定义的&lt;properties&gt;属性值来选择使用哪个环境 Maven命令使用 -Dmaven.test.skip=true:跳过测试 -P${deploy.type}:使用指定的环境.例如:mvn package -Pdev,使用dev环境 mvn help:effective-settings:查看生效的配置.这里的内容是settings.xml中生效的配置,可以用来查看repository地址和mirror的配置]]></content>
      <categories>
        <category>Tool</category>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
</search>
